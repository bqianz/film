{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acc43dc-19d6-4aaa-bf16-1f899c9499ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from pyproj import Transformer\n",
    "\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from itertools import cycle\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30eca0ef-65f3-4c60-8240-c82ee6e5db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "\n",
    "component_selected = 0\n",
    "\n",
    "chip_size = 16\n",
    "\n",
    "data_root = \"saga_data\"\n",
    "n_channels = len(os.listdir(data_root)) // 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "\n",
    "L2_param = 1e-5\n",
    "\n",
    "transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:3413\")\n",
    "\n",
    "models_dir = 'res_comp/'\n",
    "Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_max_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05bd1bc4-61b9-4e41-8f9e-866dc8be75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('components_analysis/mix_n{}.npy'.format(n_components), 'rb') as f:\n",
    "    mix = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5400e0a1-33a3-4a0a-923f-6f127b053c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('components_analysis/df_unique.csv', header=[0])\n",
    "df['proj_x'], df['proj_y'] = transformer.transform(df.latitude,df.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4d4d60-b2ef-47a8-a243-5f63d2b67ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.7847711764706"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['latitude'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc99a9fe-920f-4cd0-87d4-d95f0be643aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-133.49410438502673"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.longitude.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdae3152-779b-4a9d-aa61-3e0364d1acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(mix.shape[1]):\n",
    "    df['component{}'.format(i)] = mix[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4335684-c909-4d2d-8aa7-71687974c0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>visible_ice_code</th>\n",
       "      <th>proj_x</th>\n",
       "      <th>proj_y</th>\n",
       "      <th>component0</th>\n",
       "      <th>component1</th>\n",
       "      <th>component2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pure ice</td>\n",
       "      <td>ICE</td>\n",
       "      <td>Ice</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.280305e+06</td>\n",
       "      <td>-76170.639998</td>\n",
       "      <td>3.020992</td>\n",
       "      <td>0.351328</td>\n",
       "      <td>-1.557754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.16105</td>\n",
       "      <td>-133.08880</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GP-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.280371e+06</td>\n",
       "      <td>-76093.964428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.15849</td>\n",
       "      <td>-133.08865</td>\n",
       "      <td>2012-03-20T00:00:00Z</td>\n",
       "      <td>0170-1-17</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>SM-SC</td>\n",
       "      <td>Sand</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.280657e+06</td>\n",
       "      <td>-76109.484296</td>\n",
       "      <td>-5.746523</td>\n",
       "      <td>-0.052054</td>\n",
       "      <td>-0.600771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.15908</td>\n",
       "      <td>-133.08968</td>\n",
       "      <td>2012-03-20T00:00:00Z</td>\n",
       "      <td>0170-1-18</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>ML</td>\n",
       "      <td>Silt</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.280593e+06</td>\n",
       "      <td>-76066.285386</td>\n",
       "      <td>-5.746523</td>\n",
       "      <td>-0.052054</td>\n",
       "      <td>-0.600771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.15886</td>\n",
       "      <td>-133.09090</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-19</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GP</td>\n",
       "      <td>Gravel</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.280619e+06</td>\n",
       "      <td>-76018.544396</td>\n",
       "      <td>-5.850486</td>\n",
       "      <td>-0.905609</td>\n",
       "      <td>-0.506412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude                  time   borehole  depth  frozen  \\\n",
       "0  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.85       1   \n",
       "1  69.16105 -133.08880  2012-03-21T00:00:00Z  0170-1-12   1.20       1   \n",
       "2  69.15849 -133.08865  2012-03-20T00:00:00Z  0170-1-17   1.45       1   \n",
       "3  69.15908 -133.08968  2012-03-20T00:00:00Z  0170-1-18   1.20       1   \n",
       "4  69.15886 -133.09090  2012-03-21T00:00:00Z  0170-1-19   1.20       1   \n",
       "\n",
       "  cryostructures     visible_ice ASTM_2488    materials  ...  \\\n",
       "0            NaN        Pure ice       ICE          Ice  ...   \n",
       "1             Nf  No visible ice     GP-GM  Coarse till  ...   \n",
       "2            NaN  No visible ice     SM-SC         Sand  ...   \n",
       "3            NaN  No visible ice        ML         Silt  ...   \n",
       "4             Nf  No visible ice        GP       Gravel  ...   \n",
       "\n",
       "   bottom_of_interval  month  year  month_cyclic  visible_ice_code  \\\n",
       "0                 1.4      3  2012             3                 4   \n",
       "1                 2.4      3  2012             3                 0   \n",
       "2                 2.3      3  2012             3                 0   \n",
       "3                 2.3      3  2012             3                 0   \n",
       "4                 2.4      3  2012             3                 0   \n",
       "\n",
       "         proj_x        proj_y  component0  component1  component2  \n",
       "0 -2.280305e+06 -76170.639998    3.020992    0.351328   -1.557754  \n",
       "1 -2.280371e+06 -76093.964428    0.000000    0.000000    0.000000  \n",
       "2 -2.280657e+06 -76109.484296   -5.746523   -0.052054   -0.600771  \n",
       "3 -2.280593e+06 -76066.285386   -5.746523   -0.052054   -0.600771  \n",
       "4 -2.280619e+06 -76018.544396   -5.850486   -0.905609   -0.506412  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee0e1f4-5fa1-43c9-8975-489abf9dbdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c2a10-1db2-4d9d-88fd-952062f475aa",
   "metadata": {},
   "source": [
    "## Cluster boreholes into groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f06827e-aabf-4416-953c-430596f27025",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[['latitude','longitude', ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf864728-8ca6-45fd-9967-e742efaebe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of estimated clusters : 52\n"
     ]
    }
   ],
   "source": [
    "# The following bandwidth can be automatically detected using\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.02)\n",
    "# bandwidth = estimate_bandwidth(X, quantile=0.0)\n",
    "\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "#labels looks like [0, 0, 0, 1, 1, 1, 1, 2, 2,....]\n",
    "\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters = len(labels_unique)\n",
    "\n",
    "print(\"number of estimated clusters : %d\" % n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdac14-5e9b-4d63-8a3b-6624f1d0425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cdb651-541b-4f4d-a373-246149508a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_proj = transformer.transform(cluster_centers[:,0], cluster_centers[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90457aa-f27f-46a9-a4fb-6a09b83285a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_proj[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb54f0-b471-4533-803e-70be98068100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmplot as gp\n",
    "\n",
    "with open('components_analysis/apikey.txt') as f:\n",
    "    apikey = f.readlines()[0]\n",
    "\n",
    "gmap = gp.GoogleMapPlotter(df.latitude.mean(), df.longitude.mean(), 8, apikey = apikey)\n",
    "gmap.scatter(cluster_centers[:, 0], cluster_centers[:, 1], size = 100, marker=False) \n",
    "gmap.draw('components_analysis/cluster_centers.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a9edb-169e-4ceb-a879-dfac94a7be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = np.load('components_analysis/horizontal_sources_n{}.npy'.format(n_components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87753af0-3db7-49be-b0eb-62c0924b4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_clustered = np.zeros([n_clusters, n_components])\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    ind = (labels == i)\n",
    "    sources_clustered[i, :] = np.mean(sources[ind,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caba5ba-2471-41fe-b1ef-7d97f0542767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGA10mDataset(Dataset):\n",
    "    def __init__(self, data_root, df, sources, chip_size=32):\n",
    "        \n",
    "        self.df = df\n",
    "        self.mix = mix\n",
    "        \n",
    "        # 1 pixel = 10|\n",
    "        self.n_pixels = 10000\n",
    "        self.base_y = -9995\n",
    "        self.base_x = -239995\n",
    "        \n",
    "        self.chip_size = chip_size\n",
    "        \n",
    "        self.trans = transforms.ToTensor()\n",
    "        \n",
    "        self.n_channels = len(os.listdir(data_root))//2\n",
    "        \n",
    "        print(\"Dataset contains {} channels\".format(self.n_channels))\n",
    "        \n",
    "        self.preloaded = torch.zeros(self.n_channels, self.n_pixels, self.n_pixels*2)\n",
    "        \n",
    "        file_list = os.listdir(data_root)\n",
    "        \n",
    "        for i in range(self.n_channels):\n",
    "            # name = file.split('_')[0]\n",
    "            # print(name)\n",
    "            self.preloaded[i, :, 0:10000] = self.trans(Image.open(data_root + os.path.sep + file_list[i*2]))\n",
    "            self.preloaded[i,:, 10000:20000] = self.trans(Image.open(data_root + os.path.sep + file_list[i*2 + 1]))\n",
    "        \n",
    "        self.preloaded[self.preloaded < -9000] = 0\n",
    "        \n",
    "        print('Dataset initialized')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return n_clusters\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        lat = cluster_centers[idx][0]\n",
    "        lng = cluster_centers[idx][1]\n",
    "        \n",
    "        x = cluster_centers_proj[0][idx]\n",
    "        y = cluster_centers_proj[1][idx]\n",
    "        \n",
    "        \n",
    "        pixel_len = 10\n",
    "        \n",
    "        \n",
    "        \n",
    "        x_start = max(0 , np.round((x - self.base_x) / pixel_len - self.chip_size/2).astype(int))\n",
    "        x_end = min(20000, x_start + self.chip_size)\n",
    "        \n",
    "        y_start = max(0, np.round((- y) / pixel_len - self.chip_size/2).astype(int))\n",
    "        y_end = min(20000, y_start + self.chip_size)\n",
    "        \n",
    "\n",
    "        image= self.preloaded[:, y_start:y_end, x_start:x_end]\n",
    "        \n",
    "        sources_tensor = torch.tensor(sources[idx]).float()\n",
    "        \n",
    "        return {'image': image, 'label': sources_tensor, \\\n",
    "                'latitude': lat, 'longitude': lng}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd4b44-e9ad-4717-ba3b-6c167cee766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = SAGA10mDataset(data_root, df, sources_clustered, chip_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec6abc-199d-4df7-92e8-55e6ed9ef86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check max min values\n",
    "\n",
    "minval = 1000\n",
    "maxval = 0\n",
    "\n",
    "for i in range(len(full_dataset)):\n",
    "    image = full_dataset[i]['image'].flatten()\n",
    "    \n",
    "    minval = min(minval, min(image))\n",
    "    maxval = max(maxval, max(image))\n",
    "\n",
    "print(minval)\n",
    "print(maxval)\n",
    "\n",
    "# normalize image values\n",
    "magnitude_max = max(abs(minval), maxval)\n",
    "\n",
    "for i in range(len(full_dataset)):\n",
    "    full_dataset[i]['image'] = full_dataset[i]['image'] / magnitude_max\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0c1f4-dbd6-419c-8a67-faaa0895e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, testloader, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    \n",
    "    model= models.resnet18()\n",
    "    model.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model.fc = nn.Linear(in_features=512, out_features=n_components)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay = L2_param)\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            image, labels = data['image'].to(device), data['label'].to(device)\n",
    "\n",
    "            predicted = model(image)\n",
    "            \n",
    "            \n",
    "#             print(predicted.squeeze().get_device())\n",
    "#             print('\\n')\n",
    "#             print(labels.get_device())\n",
    "            \n",
    "            \n",
    "            # squeeze: return tensor with all dimensions of size 1 removed\n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            image, labels = data['image'].to(device), data['label'].to(device)\n",
    "\n",
    "            predicted = model(image)\n",
    "            \n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(models_dir, 'epoch-{}.pt'.format(epoch+1)))\n",
    "    \n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "        \n",
    "    return epoch_loss\n",
    "        \n",
    "def test(epoch_loss, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    \n",
    "    model= models.resnet18()\n",
    "    model.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model.fc = nn.Linear(in_features=512, out_features=n_components)\n",
    "    \n",
    "    model.load_state_dict(torch.load('{}epoch-{}.pt'.format(models_dir, ind+1)))\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_results = []\n",
    "        \n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            image, labels = data['image'].to(device), data['label'].to(device)\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            output = model(image)\n",
    "            \n",
    "            test_results.extend(output)\n",
    "            \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55f2b6-601c-4166-8f3b-f2a26076a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# batchsize can cause error when last leftover batchsize is 1, batchnorm cannot function on 1 sample data\n",
    "batchsize = 32\n",
    "while(train_size % batchsize == 1):\n",
    "    batchsize+=1\n",
    "print(batchsize)\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(full_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76219a7-9510-4123-b2a7-eac77ae04905",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe395ee-ba55-462a-b773-0a2abe2f4f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b9e75-bd2c-4b98-bc95-55a74cb2f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(1):\n",
    "    start = time.time()\n",
    "    \n",
    "    epoch_loss = train(trainloader,testloader, print_epochs = True, loss_fn = nn.HuberLoss())\n",
    "    test_results= test(epoch_loss, print_model_epoch = True)\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print('iteration {} elapsed time: {}'.format(it+1, end-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd82ef-af5e-4123-8be0-2476a282e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = torch.vstack(test_results)\n",
    "\n",
    "test_results = test_results.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a63cb-6fea-4c83-a1ab-4a44617da098",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.zeros([len(test_data), n_components])\n",
    "for i in range(len(test_data)):\n",
    "    test_labels[i, :] = test_data[i]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c3341-a3d9-4d80-9c34-2bc64e42c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = ((test_results - test_labels).flatten())\n",
    "np.mean(err * err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9e4b7-1e49-494f-a1d1-c1422dcec5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs(err), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066fde8b-d1e4-4a89-9b7e-d666d9425153",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_array = np.hstack((test_labels, test_results))\n",
    "\n",
    "cols_a = ['ICA_{}'.format(i) for i in range(n_components)]\n",
    "cols_b = ['Resnet_{}'.format(i) for i in range(n_components)]\n",
    "\n",
    "cols = cols_a + cols_b\n",
    "\n",
    "stacked_df = pd.DataFrame(stacked_array, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69aebe-3a3a-4af6-94a1-9a3f33255b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def component_coherence(i): \n",
    "    sb.lmplot(x='ICA_{}'.format(i), y='Resnet_{}'.format(i), data=stacked_df);\n",
    "    plt.axis('equal')\n",
    "# #     plt.scatter(test_labels[:, i], test_results[:, i])\n",
    "    plt.savefig('components_analysis/horizontal_coherence_n{}_channel{}.png'.format(n_components, i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7889cc-69a2-4d7d-902d-31108d36f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "component_coherence(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b15e4e-6763-4eca-a6fa-e0ea1a6246de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cd884-583d-42bc-8f04-f5c84e205d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for channel in range(3):\n",
    "    \n",
    "#     base = \"components_analysis/components_strengths_{}_n{}_channel{}.html\"\n",
    "#     name_res = base.format(\"resnet\", n_components, channel)\n",
    "#     name_nonres = base.format(\"nonres\", n_components, channel)\n",
    "    \n",
    "#     plot_component_strength_map(test_results[:, channel], channel, name_res, \\\n",
    "#                                 latitudes_map, longitudes_map)\n",
    "    \n",
    "#     plot_component_strength_map(test_labels[:, channel], channel, name_nonres, \\\n",
    "#                                 latitudes_map, longitudes_map)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dabc2a0-e412-4bb4-94a6-2434c3d48fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_max_bh = 50\n",
    "\n",
    "# plt.figure()\n",
    "# plt.subplot(311)\n",
    "# plt.imshow(bh_obs[:, 0:n_max_bh], interpolation = 'none', extent=[0,n_max_bh-1, 5, 0])\n",
    "# ax = plt.gca()\n",
    "# ax.set_aspect(2)\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Observed Visible Ice\")\n",
    "# plt.xlabel(\"Boreholes\")\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "# plt.subplot(312)\n",
    "# plt.imshow(ica_approximation[:, 0:n_max_bh], interpolation = 'none', extent=[0,n_max_bh-1, 5, 0])\n",
    "# ax = plt.gca()\n",
    "# ax.set_aspect(2)\n",
    "# plt.colorbar()\n",
    "# plt.title(\"ICA Approximation\")\n",
    "# plt.xlabel(\"Boreholes\")\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "# plt.subplot(313)\n",
    "# plt.imshow(res_approximation[:, 0:n_max_bh], interpolation = 'none', extent=[0,n_max_bh-1, 5, 0])\n",
    "# ax = plt.gca()\n",
    "# ax.set_aspect(2)\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Resnet\")\n",
    "# plt.xlabel(\"Boreholes\")\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "# # plt.savefig('components_analysis/comparison_n{}.png'.format(n_components))\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
