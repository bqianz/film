{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c6a9f38-9ff3-4628-97f9-39fa73883c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import time\n",
    "\n",
    "hidden_width = 32\n",
    "hidden_nblocks = 4\n",
    "train_max_epoch = 15\n",
    "\n",
    "data_root = \"geomorph_data\"\n",
    "n_channels = len(os.listdir(data_root))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "L2_param = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c13a7-4234-4dfe-84b6-1cee1ea446e7",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2981fd6-efad-4c24-88eb-83e428132c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dAuto(nn.Conv2d):\n",
    "    # same as Conv2d but with padding\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) # super() : inherit from baseclass\n",
    "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    "        \n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels =  in_channels, out_channels\n",
    "        self.blocks = nn.Identity()\n",
    "        self.shortcut = nn.Identity()   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut:\n",
    "            residual = self.shortcut(x)\n",
    "        # print(residual.shape)\n",
    "        x = self.blocks(x)\n",
    "        # print(x.shape)\n",
    "        x += residual # shapes of x and residual don't match when expansion > 1 and self.blocks = nn.Identity()\n",
    "        return x\n",
    "    \n",
    "    @property # use method like attribute with \".\" syntax\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "    \n",
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
    "        self.shortcut = nn.Sequential(OrderedDict(\n",
    "        {\n",
    "            # Conv2d expects the input to be of shape [batch_size, input_channels, input_height, input_width]\n",
    "            'conv' : nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
    "                      stride=self.downsampling, bias=False),\n",
    "            \n",
    "            'bn' : nn.BatchNorm2d(self.expanded_channels)\n",
    "        })) if self.should_apply_shortcut else None\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels\n",
    "\n",
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(OrderedDict({'conv': conv(in_channels, out_channels, *args, **kwargs), \n",
    "                                      'bn': nn.BatchNorm2d(out_channels)\n",
    "                                     }))\n",
    "\n",
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, activation=nn.ReLU, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            # self.conv = conv3x3\n",
    "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
    "            activation(),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
    "        )\n",
    "\n",
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, activation=nn.ReLU, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
    "             activation(),\n",
    "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
    "             activation(),\n",
    "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
    "        )\n",
    "        \n",
    "\n",
    "class ResNetLayer(nn.Module):\n",
    "    # def __init__(self, in_channels, out_channels, gamma, beta, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
    "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
    "        downsampling = 2 if in_channels != out_channels else 1\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
    "            *[block(out_channels * block.expansion, \n",
    "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
    "        )\n",
    "    \n",
    "    # def forward(self, x, gamma, beta)\n",
    "    def forward(self, x): #gamma beta\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet encoder composed by increasing different layers with increasing features.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], \n",
    "                 activation=nn.ReLU, block=ResNetBasicBlock, *args,**kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks_sizes = blocks_sizes\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
    "            activation(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
    "        self.blocks = nn.ModuleList([ \n",
    "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n",
    "                        block=block,  *args, **kwargs),\n",
    "            *[ResNetLayer(in_channels * block.expansion, \n",
    "                          out_channels, n=n, activation=activation, \n",
    "                          block=block, *args, **kwargs) \n",
    "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
    "        ])\n",
    "        \n",
    "        # gammas[:, num_block, block_dim]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gate(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class ResnetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
    "    correct class by using a fully connected layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
    "        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n",
    "        # self.module_dim = \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "def resnet18(in_channels, n_classes):\n",
    "    return ResNet(in_channels, n_classes, block=ResNetBasicBlock, deepths=[2, 2, 2, 2])\n",
    "\n",
    "def resnet34(in_channels, n_classes):\n",
    "    return ResNet(in_channels, n_classes, block=ResNetBasicBlock, deepths=[3, 4, 6, 3])\n",
    "\n",
    "def resnet50(in_channels, n_classes):\n",
    "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 4, 6, 3])\n",
    "\n",
    "def resnet101(in_channels, n_classes):\n",
    "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 4, 23, 3])\n",
    "\n",
    "def resnet152(in_channels, n_classes):\n",
    "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "629fd05b-9fa8-4502-b8cb-e0892a432591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "        def __init__(self, input_size, output_size = 1, hidden_width = 20, hidden_nblocks = 2):\n",
    "            super(mlp, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.hidden_width = hidden_width\n",
    "            self.hidden_nblocks = hidden_nblocks\n",
    "            \n",
    "            self.fc1 = nn.Linear(self.input_size, self.hidden_width)\n",
    "            self.fc2 = nn.Linear(self.hidden_width,self.hidden_width)\n",
    "            self.fc3 = nn.Linear(self.hidden_width, self.output_size)\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.sigmoid = torch.nn.Sigmoid() ## sigmoid for multi-label, softmax for multi-class (mutually exclusive)\n",
    "            \n",
    "        def forward(self, x, film_params):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            \n",
    "            \n",
    "            for i in range(self.hidden_nblocks):\n",
    "                out = self.fc2(out)\n",
    "                \n",
    "                # ------- film layer -----------\n",
    "                start = i * hidden_width * 2\n",
    "                mid = start + hidden_width\n",
    "                end = mid + hidden_width\n",
    "                \n",
    "                gamma = film_params[:, start : mid]\n",
    "                beta = film_params[:, mid : end]\n",
    "                \n",
    "#                 print(out.shape)\n",
    "#                 print(gamma.shape)\n",
    "#                 print(beta.shape)\n",
    "                \n",
    "                out = out * gamma\n",
    "                out += beta\n",
    "                # ------- film layer -----------\n",
    "                \n",
    "                out = self.relu(out)\n",
    "            \n",
    "            out = self.fc3(out)\n",
    "            out = self.sigmoid(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905bba3-9233-41cb-8ed6-ffa9087f4e8f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93742035-18c5-42bc-b9e8-86134e61c23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2837\n",
      "566\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TOPSOIL</td>\n",
       "      <td>Organics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>-1.024010</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pure ice</td>\n",
       "      <td>ICE</td>\n",
       "      <td>Ice</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>-0.835753</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>SW-SM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>-0.553369</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GW-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>0.387914</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.16105</td>\n",
       "      <td>-133.08880</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GP-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.437542</td>\n",
       "      <td>1.842296</td>\n",
       "      <td>-0.741625</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude                  time   borehole  depth  frozen  \\\n",
       "0  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.15       0   \n",
       "1  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.85       1   \n",
       "2  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   1.90       1   \n",
       "3  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   5.40       1   \n",
       "4  69.16105 -133.08880  2012-03-21T00:00:00Z  0170-1-12   1.20       1   \n",
       "\n",
       "  cryostructures     visible_ice ASTM_2488    materials  ...  top_of_interval  \\\n",
       "0            NaN             NaN   TOPSOIL     Organics  ...              0.0   \n",
       "1            NaN        Pure ice       ICE          Ice  ...              0.3   \n",
       "2             Nf  No visible ice     SW-SM  Coarse till  ...              1.4   \n",
       "3             Nf  No visible ice     GW-GM  Coarse till  ...              2.4   \n",
       "4             Nf  No visible ice     GP-GM  Coarse till  ...              0.0   \n",
       "\n",
       "   bottom_of_interval  month  year  month_cyclic  lat_norm  lng_norm  \\\n",
       "0                 0.3      3  2012             3  1.439692  1.851129   \n",
       "1                 1.4      3  2012             3  1.439692  1.851129   \n",
       "2                 2.4      3  2012             3  1.439692  1.851129   \n",
       "3                 8.4      3  2012             3  1.439692  1.851129   \n",
       "4                 2.4      3  2012             3  1.437542  1.842296   \n",
       "\n",
       "   depth_norm  year_norm  month_cyclic_norm  \n",
       "0   -1.024010  -1.164786          -1.225079  \n",
       "1   -0.835753  -1.164786          -1.225079  \n",
       "2   -0.553369  -1.164786          -1.225079  \n",
       "3    0.387914  -1.164786          -1.225079  \n",
       "4   -0.741625  -1.164786          -1.225079  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "def normalize(values):\n",
    "    # zero mean, unit variance\n",
    "    return (values-values.mean())/values.std()\n",
    "\n",
    "def normalize_maxmin(values):\n",
    "    # range from 0 to 1\n",
    "    (values-values.min())/(values.max()-values.min())\n",
    "\n",
    "def preprocess_df(df):\n",
    "    # convert timecodes to year and month columns\n",
    "    datetimes = pd.to_datetime(df['time'])\n",
    "    df['month'] = datetimes.dt.month\n",
    "    df['year'] = datetimes.dt.year\n",
    "\n",
    "    df['month_cyclic'] = 7 - abs(df['month'] - 7)\n",
    "\n",
    "    df['lat_norm'] = normalize(df['latitude'])\n",
    "    df['lng_norm'] = normalize(df['longitude'])\n",
    "    df['depth_norm'] = normalize(df['depth'])\n",
    "    df['year_norm'] = normalize(df['year'])\n",
    "    df['month_cyclic_norm'] = normalize(df['month_cyclic'])\n",
    "\n",
    "df = pd.read_csv('data_stephen_fix_header.csv', header=[0])\n",
    "preprocess_df(df)\n",
    "    \n",
    "print(df.shape[0])\n",
    "print(df['borehole'].nunique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32725fd1-d4ef-41e8-8fc9-6cb0af2751ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mouju\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\series.py:4509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>68.38262</td>\n",
       "      <td>-133.71211</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH15</td>\n",
       "      <td>5.45</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf/Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.498903</td>\n",
       "      <td>-0.938394</td>\n",
       "      <td>0.401360</td>\n",
       "      <td>-0.318997</td>\n",
       "      <td>0.736292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>68.38262</td>\n",
       "      <td>-133.71211</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH15</td>\n",
       "      <td>8.65</td>\n",
       "      <td>1</td>\n",
       "      <td>Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>9.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.498903</td>\n",
       "      <td>-0.938394</td>\n",
       "      <td>1.261961</td>\n",
       "      <td>-0.318997</td>\n",
       "      <td>0.736292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>ORGANICS</td>\n",
       "      <td>Organics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494226</td>\n",
       "      <td>-0.927509</td>\n",
       "      <td>-1.050903</td>\n",
       "      <td>-0.318997</td>\n",
       "      <td>0.736292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494226</td>\n",
       "      <td>-0.927509</td>\n",
       "      <td>-0.970222</td>\n",
       "      <td>-0.318997</td>\n",
       "      <td>0.736292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1</td>\n",
       "      <td>Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494226</td>\n",
       "      <td>-0.927509</td>\n",
       "      <td>0.239998</td>\n",
       "      <td>-0.318997</td>\n",
       "      <td>0.736292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude  longitude                  time           borehole  depth  \\\n",
       "2832  68.38262 -133.71211  2013-04-27T00:00:00Z  W14103137-S6-BH15   5.45   \n",
       "2833  68.38262 -133.71211  2013-04-27T00:00:00Z  W14103137-S6-BH15   8.65   \n",
       "2834  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   0.05   \n",
       "2835  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   0.35   \n",
       "2836  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   4.85   \n",
       "\n",
       "      frozen cryostructures     visible_ice ASTM_2488 materials  ...  \\\n",
       "2832       1         Nf/Nbn  No visible ice       NaN      Till  ...   \n",
       "2833       1            Nbn  No visible ice       NaN      Till  ...   \n",
       "2834       0            NaN  No visible ice  ORGANICS  Organics  ...   \n",
       "2835       0            NaN  No visible ice       NaN      Till  ...   \n",
       "2836       1            Nbn  No visible ice       NaN      Till  ...   \n",
       "\n",
       "      top_of_interval  bottom_of_interval  month  year  month_cyclic  \\\n",
       "2832              2.7                 8.2      4  2013             4   \n",
       "2833              8.2                 9.1      4  2013             4   \n",
       "2834              0.0                 0.1      4  2013             4   \n",
       "2835              0.1                 0.6      4  2013             4   \n",
       "2836              0.6                 9.1      4  2013             4   \n",
       "\n",
       "      lat_norm  lng_norm  depth_norm  year_norm  month_cyclic_norm  \n",
       "2832 -1.498903 -0.938394    0.401360  -0.318997           0.736292  \n",
       "2833 -1.498903 -0.938394    1.261961  -0.318997           0.736292  \n",
       "2834 -1.494226 -0.927509   -1.050903  -0.318997           0.736292  \n",
       "2835 -1.494226 -0.927509   -0.970222  -0.318997           0.736292  \n",
       "2836 -1.494226 -0.927509    0.239998  -0.318997           0.736292  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.dropna(subset=['visible_ice'])\n",
    "df2['visible_ice'].replace(['None'], 'No visible ice', regex=True, inplace=True)\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0223c99-9d82-4efb-92d3-f4281a9897c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check None values have been replaced\n",
    "len(df2[df2['visible_ice'] == 'None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e75fb7-eae7-46ae-b952-38c585925846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1796\n",
       "0     956\n",
       "Name: No visible ice, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visible_ice = pd.get_dummies(df2.visible_ice)\n",
    "bin_visible_ice = (~visible_ice['No visible ice'].astype('bool')).astype('int')\n",
    "bin_visible_ice.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c029bb47-2a0f-4d54-be1a-e00630ece3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-697191a84568>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['visible_ice'] = bin_visible_ice\n"
     ]
    }
   ],
   "source": [
    "df2['visible_ice'] = bin_visible_ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830c0d00-3256-4107-a9d6-0b3026a7f95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ICE</td>\n",
       "      <td>Ice</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>-0.835753</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>0</td>\n",
       "      <td>SW-SM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>-0.553369</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>0</td>\n",
       "      <td>GW-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>0.387914</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.16105</td>\n",
       "      <td>-133.08880</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>0</td>\n",
       "      <td>GP-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.437542</td>\n",
       "      <td>1.842296</td>\n",
       "      <td>-0.741625</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69.16105</td>\n",
       "      <td>-133.08880</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-12</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>0</td>\n",
       "      <td>SM</td>\n",
       "      <td>Sand</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.437542</td>\n",
       "      <td>1.842296</td>\n",
       "      <td>-0.002046</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude                  time   borehole  depth  frozen  \\\n",
       "1  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.85       1   \n",
       "2  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   1.90       1   \n",
       "3  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   5.40       1   \n",
       "4  69.16105 -133.08880  2012-03-21T00:00:00Z  0170-1-12   1.20       1   \n",
       "5  69.16105 -133.08880  2012-03-21T00:00:00Z  0170-1-12   3.95       1   \n",
       "\n",
       "  cryostructures  visible_ice ASTM_2488    materials  ...  top_of_interval  \\\n",
       "1            NaN            1       ICE          Ice  ...              0.3   \n",
       "2             Nf            0     SW-SM  Coarse till  ...              1.4   \n",
       "3             Nf            0     GW-GM  Coarse till  ...              2.4   \n",
       "4             Nf            0     GP-GM  Coarse till  ...              0.0   \n",
       "5             Nf            0        SM         Sand  ...              2.4   \n",
       "\n",
       "   bottom_of_interval  month  year  month_cyclic  lat_norm  lng_norm  \\\n",
       "1                 1.4      3  2012             3  1.439692  1.851129   \n",
       "2                 2.4      3  2012             3  1.439692  1.851129   \n",
       "3                 8.4      3  2012             3  1.439692  1.851129   \n",
       "4                 2.4      3  2012             3  1.437542  1.842296   \n",
       "5                 5.5      3  2012             3  1.437542  1.842296   \n",
       "\n",
       "   depth_norm  year_norm  month_cyclic_norm  \n",
       "1   -0.835753  -1.164786          -1.225079  \n",
       "2   -0.553369  -1.164786          -1.225079  \n",
       "3    0.387914  -1.164786          -1.225079  \n",
       "4   -0.741625  -1.164786          -1.225079  \n",
       "5   -0.002046  -1.164786          -1.225079  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c3c456-b700-46cd-9a6d-52872ba14aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Geo90Dataset(Dataset):\n",
    "    def __init__(self, data_root, df, base_lat, base_lng, chip_size=32, label_name = 'frozen'):\n",
    "        \n",
    "        self.base_lat = base_lat\n",
    "        self.base_lng = base_lng\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        self.chip_size = chip_size\n",
    "        self.label_name = label_name\n",
    "        \n",
    "        self.trans = transforms.ToTensor()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.preloaded = torch.zeros(self.n_channels, 6000, 6000)\n",
    "        \n",
    "        for i, file in enumerate(os.listdir(data_root)):\n",
    "            # name = file.split('_')[0]\n",
    "            # print(name)\n",
    "            self.preloaded[i] = self.trans(Image.open(data_root + os.path.sep + file))\n",
    "        \n",
    "        print('Dataset initialized')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        bh_id = row.at['borehole']\n",
    "        lat = row.at['latitude']\n",
    "        lng = row.at['longitude']\n",
    "        \n",
    "\n",
    "        pixel_len = 5/6000\n",
    "        \n",
    "\n",
    "        lat_index_start = np.round((self.base_lat - lat) / pixel_len - self.chip_size/2).astype(int)\n",
    "        lat_index_end = lat_index_start + self.chip_size\n",
    "        \n",
    "        lng_index_start = np.round((lng - self.base_lng) / pixel_len - self.chip_size/2).astype(int)\n",
    "        lng_index_end = lng_index_start + self.chip_size\n",
    "        \n",
    "        image = self.preloaded[:, lat_index_start:lat_index_end,lng_index_start:lng_index_end]\n",
    "        \n",
    "        \n",
    "        # surface = torch.tensor(row.filter(['depth'])).float()\n",
    "        surface = torch.tensor(row.filter(['depth_norm', 'month_cyclic_norm', 'lat_norm', 'lng_norm', 'year_norm'])).float()\n",
    "        \n",
    "        frozen = torch.tensor(row.at['frozen']).float()\n",
    "        \n",
    "        visible_ice = torch.tensor(row.at['visible_ice']).float()\n",
    "        \n",
    "        return {'image': image, 'surface_data': surface, 'frozen': frozen, 'visible_ice': visible_ice}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b0df9-d51d-4bb3-9658-7a49e95bc6a6",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f46b0-7537-4ceb-8f96-263c007e97fd",
   "metadata": {},
   "source": [
    "## FiLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "214c4911-bcfb-43a2-b41b-b01949690b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_film_params = hidden_width * hidden_nblocks * 2\n",
    "generator = models.resnet18()\n",
    "generator.fc = nn.Linear(512, n_film_params)\n",
    "generator.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# generator = resnet18(n_channels, n_film_params)\n",
    "\n",
    "def train_model(trainloader, testloader, label_name, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    # loss: binary cross entropy\n",
    "\n",
    "    \n",
    "    gen_model = generator\n",
    "\n",
    "    # print(gen_model)\n",
    "\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    net_model = mlp(input_size[0],1, hidden_width, hidden_nblocks).to(device)\n",
    "    \n",
    "    gen_optimizer = torch.optim.Adam(gen_model.parameters(), weight_decay = L2_param)\n",
    "    net_optimizer = torch.optim.Adam(net_model.parameters(), weight_decay = L2_param)\n",
    "    \n",
    "    gen_model.to(device)\n",
    "    net_model.to(device)\n",
    "\n",
    "    # --------- check back propagation ----------- -\n",
    "    # net_model.fc1.weight.register_hook(lambda x: print('grad accumulated in mlp fc1'))\n",
    "    # gen_first_layer = gen_model.encoder.blocks[0].blocks[0].blocks[0].conv\n",
    "    # gen_first_layer.weight.register_hook(lambda x: print('grad accumulated in resnet first layer'))\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        # ------------ train -----------------\n",
    "        gen_model.train()\n",
    "        net_model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            \n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "\n",
    "            gen_optimizer.zero_grad()\n",
    "            net_optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            gen_optimizer.step()\n",
    "            net_optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        gen_model.eval()\n",
    "        net_model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # TODO: exammine film_params gradients / readup pytorch\n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "\n",
    "        torch.save(gen_model.state_dict(), os.path.join('mlp-resnet-models/', 'gen-epoch-{}.pt'.format(epoch+1)))\n",
    "        torch.save(net_model.state_dict(), os.path.join('mlp-resnet-models/', 'net-epoch-{}.pt'.format(epoch+1)))\n",
    "\n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "## Test model\n",
    "\n",
    "def test_model(epoch_loss, label_name, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    \n",
    "    gen_model = generator\n",
    "\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    net_model = mlp(input_size[0],1, hidden_width, hidden_nblocks)\n",
    "\n",
    "    gen_model.load_state_dict(torch.load('mlp-resnet-models/gen-epoch-{}.pt'.format(ind+1)))\n",
    "    net_model.load_state_dict(torch.load('mlp-resnet-models/net-epoch-{}.pt'.format(ind+1)))\n",
    "    \n",
    "    gen_model.to(device)\n",
    "    net_model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    gen_model.eval()\n",
    "    net_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "\n",
    "            predicted = torch.round(predicted)\n",
    "            # print(predicted.shape)\n",
    "            lb = labels.tolist()\n",
    "            pr = predicted.tolist()\n",
    "            y_test.extend(lb)\n",
    "            y_pred.extend(pr)\n",
    "    \n",
    "    arr_accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "    return arr_accuracy, scores\n",
    "\n",
    "\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "#     print(classification_report(y_test,y_pred))\n",
    "#     print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "## Pure MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8b5b0-1039-4c76-93e4-0f90b60d87be",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c02928-f9c5-4b2a-8ef7-d8a2de4a4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_pure(nn.Module):\n",
    "        def __init__(self, input_size, output_size):\n",
    "            super(mlp_pure, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.hidden_size = hidden_width\n",
    "            self.hidden_nblocks = hidden_nblocks\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size,self.hidden_size)\n",
    "            self.fc3 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.sigmoid = torch.nn.Sigmoid() ## sigmoid for multi-label, softmax for multi-class (mutually exclusive)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            # print(out.shape)\n",
    "            \n",
    "            for i in range(self.hidden_nblocks):\n",
    "                out = self.fc2(out)\n",
    "                out = self.relu(out)\n",
    "            \n",
    "            out = self.fc3(out)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "def train_mlp(trainloader, testloader, label_name, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    surface_model = mlp_pure(input_size[0],1)\n",
    "    \n",
    "    surface_model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(surface_model.parameters(), weight_decay = L2_param)\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        surface_model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            predicted = surface_model(surface_data)\n",
    "\n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        surface_model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            predicted = surface_model(surface_data)\n",
    "\n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "\n",
    "        torch.save(surface_model.state_dict(), os.path.join('mlp-models/', 'epoch-{}.pt'.format(epoch+1)))\n",
    "    \n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "        \n",
    "    return epoch_loss\n",
    "        \n",
    "def test_mlp(epoch_loss, label_name, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    \n",
    "    surface_model = mlp_pure(input_size[0],1)\n",
    "    surface_model.load_state_dict(torch.load('mlp-models/epoch-{}.pt'.format(ind+1)))\n",
    "    \n",
    "    surface_model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    surface_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            output = surface_model(surface_data)\n",
    "            predicted = torch.round(output)\n",
    "            # print(predicted.shape)\n",
    "            lb = labels.tolist()\n",
    "            pr = predicted.tolist()\n",
    "            y_test.extend(lb)\n",
    "            y_pred.extend(pr)\n",
    "    \n",
    "    arr_accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "    return arr_accuracy, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cfb6b-c5ca-4fdd-81e9-571685a86522",
   "metadata": {},
   "source": [
    "## Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51bacf58-7320-49da-8f08-aa180ed67446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized\n"
     ]
    }
   ],
   "source": [
    "base_lat = 70\n",
    "base_lng = -135\n",
    "\n",
    "full_dataset = Geo90Dataset(data_root, df2, base_lat, base_lng, chip_size = 32)\n",
    "\n",
    "# image = full_dataset[0]['image']\n",
    "# n_channels = list(image.shape)[0]\n",
    "\n",
    "# for data in full_dataset:\n",
    "#     image = data['image']\n",
    "#     for i in range(n_channels):\n",
    "#         channel = image[i]\n",
    "#         ind = (channel == -9999)\n",
    "# #         mean_val = torch.mean(channel[~ind])\n",
    "#         channel[ind] = 0\n",
    "#         data['image'][i] = channel\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba299154-57ac-4e30-bf2e-74e41e3fc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# batchsize can cause error when last leftover batchsize is 1, batchnorm cannot function on 1 sample data\n",
    "batchsize = 20\n",
    "while(train_size % batchsize == 1):\n",
    "    batchsize+=1\n",
    "print(batchsize)\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(full_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de8608a7-1f33-4c4a-89ee-c0a758bd0570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[ 1.8050e-01,  1.2650e-01,  1.1382e-01,  ...,  9.3215e-01,\n",
       "            9.2837e-01,  9.2911e-01],\n",
       "          [ 8.1855e-02,  7.1024e-02,  7.1678e-02,  ...,  6.9787e-01,\n",
       "            7.6498e-01,  7.9781e-01],\n",
       "          [ 2.5863e-02,  4.7906e-02,  5.6162e-02,  ...,  1.9884e-01,\n",
       "            1.6252e-01,  3.0827e-02],\n",
       "          ...,\n",
       "          [ 4.5016e-01,  2.5719e-01,  1.4909e-01,  ...,  5.1338e-01,\n",
       "            5.4945e-01,  5.9167e-01],\n",
       "          [ 9.0233e-01,  7.2969e-01,  6.0320e-01,  ...,  3.9848e-01,\n",
       "            4.1345e-01,  4.7782e-01],\n",
       "          [ 8.2499e-01,  8.9178e-01,  8.7839e-01,  ...,  2.7827e-01,\n",
       "            3.0778e-01,  3.3983e-01]],\n",
       " \n",
       "         [[-9.7966e-01, -9.8791e-01, -8.2319e-01,  ..., -3.3211e-01,\n",
       "           -2.1186e-01, -9.6948e-03],\n",
       "          [-9.9664e-01, -9.9702e-01, -9.9650e-01,  ..., -6.6177e-01,\n",
       "           -6.0003e-01, -4.3316e-01],\n",
       "          [-9.9793e-01, -9.9751e-01, -9.9703e-01,  ..., -9.6500e-01,\n",
       "           -8.4183e-01, -7.0997e-01],\n",
       "          ...,\n",
       "          [-7.4442e-01, -9.2049e-01, -9.8081e-01,  ..., -8.4783e-01,\n",
       "           -8.2263e-01, -7.9902e-01],\n",
       "          [-4.2238e-02, -2.7489e-01, -5.3946e-01,  ..., -9.1537e-01,\n",
       "           -9.1013e-01, -8.6837e-01],\n",
       "          [-5.8865e-02,  2.6800e-01,  1.8647e-01,  ..., -9.5851e-01,\n",
       "           -9.4793e-01, -9.3759e-01]],\n",
       " \n",
       "         [[ 2.8044e+02,  2.8044e+02,  2.7106e+02,  ...,  3.4509e+02,\n",
       "            3.3229e+02,  2.0435e+01],\n",
       "          [ 2.7471e+02,  2.7471e+02,  2.7106e+02,  ...,  3.3229e+02,\n",
       "            3.3229e+02,  3.1928e+02],\n",
       "          [ 2.6775e+02,  2.7449e+02,  2.7449e+02,  ...,  2.8273e+02,\n",
       "            2.8273e+02,  3.1928e+02],\n",
       "          ...,\n",
       "          [ 2.8259e+02,  2.8259e+02,  2.8259e+02,  ...,  2.9552e+02,\n",
       "            2.9552e+02,  3.0980e+02],\n",
       "          [ 3.5585e+02,  3.5585e+02,  3.0536e+02,  ...,  2.9552e+02,\n",
       "            2.9425e+02,  2.9425e+02],\n",
       "          [ 2.2997e+01,  2.2997e+01,  2.2997e+01,  ...,  2.8622e+02,\n",
       "            2.8622e+02,  2.9115e+02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 7.7812e-01,  1.2277e-01,  1.5812e-01,  ...,  8.4978e-01,\n",
       "            1.2270e+00,  1.6359e+00],\n",
       "          [ 1.0840e+00,  6.5022e-01,  9.9800e-02,  ...,  7.6597e-01,\n",
       "            1.6849e+00,  2.1347e+00],\n",
       "          [ 1.5839e+00,  1.3867e+00,  1.2251e+00,  ...,  2.7252e-01,\n",
       "            1.5703e+00,  2.7459e+00],\n",
       "          ...,\n",
       "          [-1.0707e+00, -1.2340e+00, -1.0994e+00,  ...,  6.7302e-01,\n",
       "            1.0322e+00,  1.4411e+00],\n",
       "          [-3.8663e-02, -5.5663e-01, -1.2370e+00,  ..., -1.3944e-03,\n",
       "           -1.2538e-01,  1.0475e-01],\n",
       "          [ 1.1023e+00,  9.8027e-01,  3.0728e-01,  ..., -5.6980e-01,\n",
       "           -2.8413e-01,  2.8195e-01]],\n",
       " \n",
       "         [[ 3.6793e+00,  3.3132e+00,  3.2066e+00,  ...,  2.9689e+00,\n",
       "            2.8358e+00,  2.7372e+00],\n",
       "          [ 3.5359e+00,  3.3827e+00,  3.2293e+00,  ...,  2.5527e+00,\n",
       "            2.7685e+00,  2.8073e+00],\n",
       "          [ 5.1233e+00,  4.3728e+00,  3.6980e+00,  ...,  3.2832e+00,\n",
       "            3.5811e+00,  3.8604e+00],\n",
       "          ...,\n",
       "          [ 2.6013e+00,  2.6938e+00,  2.8671e+00,  ...,  5.6242e+00,\n",
       "            5.2432e+00,  4.8848e+00],\n",
       "          [ 1.6939e+00,  1.9701e+00,  2.2496e+00,  ...,  5.8755e+00,\n",
       "            6.1876e+00,  6.0220e+00],\n",
       "          [ 1.6651e+00,  1.5438e+00,  1.7932e+00,  ...,  5.9614e+00,\n",
       "            6.4256e+00,  6.8871e+00]],\n",
       " \n",
       "         [[ 2.9173e-04,  3.4273e-04,  5.6947e-04,  ...,  3.3125e-04,\n",
       "            2.5740e-04,  1.8658e-04],\n",
       "          [ 1.9478e-04,  2.2328e-04,  2.3940e-04,  ...,  2.2368e-04,\n",
       "            2.5029e-04,  2.6256e-04],\n",
       "          [ 3.7294e-04,  3.1346e-04,  2.8436e-04,  ...,  3.5587e-04,\n",
       "            4.5562e-04,  5.5235e-04],\n",
       "          ...,\n",
       "          [ 1.5133e-04,  1.6951e-04,  1.9234e-04,  ...,  1.5327e-04,\n",
       "            2.2186e-04,  2.9711e-04],\n",
       "          [ 1.1523e-04,  1.4583e-04,  1.8788e-04,  ...,  1.1343e-04,\n",
       "            1.2966e-04,  2.0903e-04],\n",
       "          [ 7.9588e-05,  7.3222e-05,  1.0847e-04,  ...,  1.0548e-04,\n",
       "            1.3219e-04,  1.6541e-04]]]),\n",
       " 'surface_data': tensor([-0.7954,  0.7363, -1.5172, -0.8907, -0.3190]),\n",
       " 'frozen': tensor(1.),\n",
       " 'visible_ice': tensor(0.)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb6971-4f9e-409e-b8ee-9ebe27a3cfae",
   "metadata": {},
   "source": [
    "## Scale of image chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e37318c4-ff02-48e0-aa8c-c5a708eb0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = full_dataset[0]['image']\n",
    "# n_samples = len(train_data)\n",
    "# n_channels = list(image.shape)[0]\n",
    "\n",
    "# scalers = []\n",
    "# for i in range(n_channels):\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X = torch.empty((n_samples, full_dataset.chip_size, full_dataset.chip_size))\n",
    "    \n",
    "#     for j, data in enumerate(train_data):\n",
    "#         #print(data['image'][i].shape)\n",
    "#         # print(X[j].shape)\n",
    "#         X[j] = data['image'][i]\n",
    "#     X = torch.reshape(X, (-1,1))\n",
    "#     scaler.fit(X)\n",
    "#     scalers.append(scaler)\n",
    "    \n",
    "#     def scale_data(subset):\n",
    "#         for data in subset:\n",
    "#             X = data['image'][i]\n",
    "#             X_flat = torch.reshape(X, (-1,1))\n",
    "#             X_trans = scaler.transform(X_flat)\n",
    "#             data['image'][i] = torch.reshape(torch.Tensor(X_trans), (full_dataset.chip_size, full_dataset.chip_size))\n",
    "    \n",
    "#     scale_data(train_data)\n",
    "#     scale_data(test_data)\n",
    "#     print(\"Channel {} scaled.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f7a6d03-a8b0-4c0f-b562-415f30deb0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: running loss: 0.66633, validation loss: 0.64669\n",
      "epoch  2: running loss: 0.64050, validation loss: 0.64856\n",
      "epoch  3: running loss: 0.63696, validation loss: 0.63990\n",
      "epoch  4: running loss: 0.63131, validation loss: 0.63876\n",
      "epoch  5: running loss: 0.62241, validation loss: 0.62802\n",
      "epoch  6: running loss: 0.61092, validation loss: 0.62984\n",
      "epoch  7: running loss: 0.60045, validation loss: 0.63016\n",
      "epoch  8: running loss: 0.59030, validation loss: 0.62487\n",
      "epoch  9: running loss: 0.58224, validation loss: 0.61713\n",
      "epoch 10: running loss: 0.57418, validation loss: 0.61972\n",
      "epoch 11: running loss: 0.56777, validation loss: 0.60518\n",
      "epoch 12: running loss: 0.56641, validation loss: 0.60686\n",
      "epoch 13: running loss: 0.56040, validation loss: 0.60311\n",
      "epoch 14: running loss: 0.56255, validation loss: 0.60694\n",
      "epoch 15: running loss: 0.56187, validation loss: 0.60474\n",
      "Finished Training\n",
      "epoch 13 model selected\n",
      "iteration 1 elapsed time: 22.900066375732422, accuracy : 0.6805807622504537\n",
      "epoch  1: running loss: 0.66327, validation loss: 0.65810\n",
      "epoch  2: running loss: 0.63906, validation loss: 0.64661\n",
      "epoch  3: running loss: 0.63102, validation loss: 0.63630\n",
      "epoch  4: running loss: 0.62614, validation loss: 0.64556\n",
      "epoch  5: running loss: 0.61317, validation loss: 0.62889\n",
      "epoch  6: running loss: 0.59504, validation loss: 0.62929\n",
      "epoch  7: running loss: 0.58110, validation loss: 0.62869\n",
      "epoch  8: running loss: 0.57257, validation loss: 0.60809\n",
      "epoch  9: running loss: 0.56735, validation loss: 0.62850\n",
      "epoch 10: running loss: 0.56778, validation loss: 0.60426\n",
      "epoch 11: running loss: 0.56178, validation loss: 0.64597\n",
      "epoch 12: running loss: 0.56041, validation loss: 0.61770\n",
      "epoch 13: running loss: 0.55568, validation loss: 0.60102\n",
      "epoch 14: running loss: 0.55631, validation loss: 0.61783\n",
      "epoch 15: running loss: 0.55234, validation loss: 0.61804\n",
      "Finished Training\n",
      "epoch 13 model selected\n",
      "iteration 2 elapsed time: 22.84930992126465, accuracy : 0.6950998185117967\n",
      "epoch  1: running loss: 0.65507, validation loss: 0.65305\n",
      "epoch  2: running loss: 0.63523, validation loss: 0.63609\n",
      "epoch  3: running loss: 0.62220, validation loss: 0.64681\n",
      "epoch  4: running loss: 0.61038, validation loss: 0.62969\n",
      "epoch  5: running loss: 0.59920, validation loss: 0.62562\n",
      "epoch  6: running loss: 0.58730, validation loss: 0.62495\n",
      "epoch  7: running loss: 0.57758, validation loss: 0.60999\n",
      "epoch  8: running loss: 0.56942, validation loss: 0.60657\n",
      "epoch  9: running loss: 0.56719, validation loss: 0.60235\n",
      "epoch 10: running loss: 0.56060, validation loss: 0.60430\n",
      "epoch 11: running loss: 0.55906, validation loss: 0.59980\n",
      "epoch 12: running loss: 0.55621, validation loss: 0.59921\n",
      "epoch 13: running loss: 0.55080, validation loss: 0.60188\n",
      "epoch 14: running loss: 0.55294, validation loss: 0.59929\n",
      "epoch 15: running loss: 0.54776, validation loss: 0.58852\n",
      "Finished Training\n",
      "epoch 15 model selected\n",
      "iteration 3 elapsed time: 22.943289279937744, accuracy : 0.6987295825771325\n",
      "epoch  1: running loss: 0.65758, validation loss: 0.64537\n",
      "epoch  2: running loss: 0.63694, validation loss: 0.64470\n",
      "epoch  3: running loss: 0.63027, validation loss: 0.63526\n",
      "epoch  4: running loss: 0.62235, validation loss: 0.63131\n",
      "epoch  5: running loss: 0.61412, validation loss: 0.62909\n",
      "epoch  6: running loss: 0.60332, validation loss: 0.61765\n",
      "epoch  7: running loss: 0.59248, validation loss: 0.61213\n",
      "epoch  8: running loss: 0.58684, validation loss: 0.62384\n",
      "epoch  9: running loss: 0.57930, validation loss: 0.62769\n",
      "epoch 10: running loss: 0.57295, validation loss: 0.60933\n",
      "epoch 11: running loss: 0.57002, validation loss: 0.62673\n",
      "epoch 12: running loss: 0.56807, validation loss: 0.61490\n",
      "epoch 13: running loss: 0.56343, validation loss: 0.60552\n",
      "epoch 14: running loss: 0.55739, validation loss: 0.62795\n",
      "epoch 15: running loss: 0.56645, validation loss: 0.61391\n",
      "Finished Training\n",
      "epoch 13 model selected\n",
      "iteration 4 elapsed time: 22.86184573173523, accuracy : 0.6842105263157895\n",
      "epoch  1: running loss: 0.66320, validation loss: 0.65371\n",
      "epoch  2: running loss: 0.63686, validation loss: 0.63587\n",
      "epoch  3: running loss: 0.63054, validation loss: 0.62985\n",
      "epoch  4: running loss: 0.62030, validation loss: 0.62181\n",
      "epoch  5: running loss: 0.60616, validation loss: 0.62340\n",
      "epoch  6: running loss: 0.59072, validation loss: 0.61127\n",
      "epoch  7: running loss: 0.58398, validation loss: 0.63274\n",
      "epoch  8: running loss: 0.57550, validation loss: 0.61223\n",
      "epoch  9: running loss: 0.57110, validation loss: 0.61486\n",
      "epoch 10: running loss: 0.56524, validation loss: 0.60659\n",
      "epoch 11: running loss: 0.56270, validation loss: 0.60747\n",
      "epoch 12: running loss: 0.56122, validation loss: 0.61709\n",
      "epoch 13: running loss: 0.55572, validation loss: 0.59495\n",
      "epoch 14: running loss: 0.55461, validation loss: 0.60613\n",
      "epoch 15: running loss: 0.55545, validation loss: 0.59118\n",
      "Finished Training\n",
      "epoch 15 model selected\n",
      "iteration 5 elapsed time: 23.11802387237549, accuracy : 0.7005444646098004\n",
      "epoch  1: running loss: 0.65267, validation loss: 0.64901\n",
      "epoch  2: running loss: 0.62811, validation loss: 0.62843\n",
      "epoch  3: running loss: 0.60724, validation loss: 0.62893\n",
      "epoch  4: running loss: 0.59019, validation loss: 0.60373\n",
      "epoch  5: running loss: 0.57785, validation loss: 0.60548\n",
      "epoch  6: running loss: 0.57118, validation loss: 0.63323\n",
      "epoch  7: running loss: 0.56919, validation loss: 0.60581\n",
      "epoch  8: running loss: 0.56955, validation loss: 0.61157\n",
      "epoch  9: running loss: 0.56529, validation loss: 0.62184\n",
      "epoch 10: running loss: 0.56850, validation loss: 0.59415\n",
      "epoch 11: running loss: 0.55972, validation loss: 0.59702\n",
      "epoch 12: running loss: 0.56010, validation loss: 0.58796\n",
      "epoch 13: running loss: 0.55642, validation loss: 0.59618\n",
      "epoch 14: running loss: 0.56108, validation loss: 0.59645\n",
      "epoch 15: running loss: 0.55964, validation loss: 0.62252\n",
      "Finished Training\n",
      "epoch 12 model selected\n",
      "iteration 6 elapsed time: 22.969562768936157, accuracy : 0.705989110707804\n",
      "epoch  1: running loss: 0.64975, validation loss: 0.65242\n",
      "epoch  2: running loss: 0.64014, validation loss: 0.64590\n",
      "epoch  3: running loss: 0.63302, validation loss: 0.63391\n",
      "epoch  4: running loss: 0.62001, validation loss: 0.62537\n",
      "epoch  5: running loss: 0.60763, validation loss: 0.62036\n",
      "epoch  6: running loss: 0.59570, validation loss: 0.61736\n",
      "epoch  7: running loss: 0.58364, validation loss: 0.62901\n",
      "epoch  8: running loss: 0.58274, validation loss: 0.62930\n",
      "epoch  9: running loss: 0.57353, validation loss: 0.61576\n",
      "epoch 10: running loss: 0.56811, validation loss: 0.63653\n",
      "epoch 11: running loss: 0.56753, validation loss: 0.61841\n",
      "epoch 12: running loss: 0.56293, validation loss: 0.60610\n",
      "epoch 13: running loss: 0.55894, validation loss: 0.61306\n",
      "epoch 14: running loss: 0.55693, validation loss: 0.62817\n",
      "epoch 15: running loss: 0.55699, validation loss: 0.63490\n",
      "Finished Training\n",
      "epoch 12 model selected\n",
      "iteration 7 elapsed time: 22.93145513534546, accuracy : 0.6878402903811253\n",
      "epoch  1: running loss: 0.65112, validation loss: 0.65441\n",
      "epoch  2: running loss: 0.63611, validation loss: 0.64223\n",
      "epoch  3: running loss: 0.62442, validation loss: 0.63027\n",
      "epoch  4: running loss: 0.60632, validation loss: 0.63167\n",
      "epoch  5: running loss: 0.59053, validation loss: 0.61202\n",
      "epoch  6: running loss: 0.57816, validation loss: 0.60280\n",
      "epoch  7: running loss: 0.57029, validation loss: 0.63417\n",
      "epoch  8: running loss: 0.56657, validation loss: 0.61511\n",
      "epoch  9: running loss: 0.55960, validation loss: 0.62711\n",
      "epoch 10: running loss: 0.55901, validation loss: 0.60821\n",
      "epoch 11: running loss: 0.55923, validation loss: 0.61561\n",
      "epoch 12: running loss: 0.55705, validation loss: 0.61906\n",
      "epoch 13: running loss: 0.55509, validation loss: 0.61430\n",
      "epoch 14: running loss: 0.55425, validation loss: 0.60743\n",
      "epoch 15: running loss: 0.54847, validation loss: 0.62433\n",
      "Finished Training\n",
      "epoch 6 model selected\n",
      "iteration 8 elapsed time: 22.82368016242981, accuracy : 0.691470054446461\n",
      "epoch  1: running loss: 0.66888, validation loss: 0.64052\n",
      "epoch  2: running loss: 0.62943, validation loss: 0.63338\n",
      "epoch  3: running loss: 0.61638, validation loss: 0.62296\n",
      "epoch  4: running loss: 0.59856, validation loss: 0.62395\n",
      "epoch  5: running loss: 0.58343, validation loss: 0.60780\n",
      "epoch  6: running loss: 0.57567, validation loss: 0.61348\n",
      "epoch  7: running loss: 0.57046, validation loss: 0.60516\n",
      "epoch  8: running loss: 0.56622, validation loss: 0.59708\n",
      "epoch  9: running loss: 0.56065, validation loss: 0.60515\n",
      "epoch 10: running loss: 0.55617, validation loss: 0.60040\n",
      "epoch 11: running loss: 0.55824, validation loss: 0.61417\n",
      "epoch 12: running loss: 0.55805, validation loss: 0.60174\n",
      "epoch 13: running loss: 0.55532, validation loss: 0.59305\n",
      "epoch 14: running loss: 0.55135, validation loss: 0.60559\n",
      "epoch 15: running loss: 0.55309, validation loss: 0.59272\n",
      "Finished Training\n",
      "epoch 15 model selected\n",
      "iteration 9 elapsed time: 22.854058742523193, accuracy : 0.6932849364791288\n",
      "epoch  1: running loss: 0.66096, validation loss: 0.65402\n",
      "epoch  2: running loss: 0.64001, validation loss: 0.64540\n",
      "epoch  3: running loss: 0.62481, validation loss: 0.62631\n",
      "epoch  4: running loss: 0.60638, validation loss: 0.61971\n",
      "epoch  5: running loss: 0.58761, validation loss: 0.62233\n",
      "epoch  6: running loss: 0.57378, validation loss: 0.66462\n",
      "epoch  7: running loss: 0.56885, validation loss: 0.61361\n",
      "epoch  8: running loss: 0.56703, validation loss: 0.61260\n",
      "epoch  9: running loss: 0.55669, validation loss: 0.62936\n",
      "epoch 10: running loss: 0.55699, validation loss: 0.62001\n",
      "epoch 11: running loss: 0.55680, validation loss: 0.61089\n",
      "epoch 12: running loss: 0.55075, validation loss: 0.62192\n",
      "epoch 13: running loss: 0.54684, validation loss: 0.61078\n",
      "epoch 14: running loss: 0.55106, validation loss: 0.60344\n",
      "epoch 15: running loss: 0.54566, validation loss: 0.61238\n",
      "Finished Training\n",
      "epoch 14 model selected\n",
      "iteration 10 elapsed time: 22.79901146888733, accuracy : 0.6805807622504537\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 10\n",
    "results = np.zeros([max_iterations, 9])\n",
    "# trainloader, testloader = prepare_dataloader(full_dataset)\n",
    "\n",
    "labelName = 'visible_ice'\n",
    "\n",
    "for it in range(max_iterations):\n",
    "    start = time.time()\n",
    "    \n",
    "    # mlp\n",
    "    epoch_loss_mlp = train_mlp(trainloader,testloader, labelName, print_epochs = True)\n",
    "    acc, scores = test_mlp(epoch_loss_mlp, labelName, print_model_epoch = True)\n",
    "    \n",
    "    #     # ------- mlp-resnet film \n",
    "#     epoch_loss = train_model(trainloader, testloader, labelName, print_epochs=True)\n",
    "#     acc, scores = test_model(epoch_loss, labelName, print_model_epoch = True)\n",
    "    \n",
    "    results[it, 0:2] = scores[0]\n",
    "    results[it, 2:4] = scores[1]\n",
    "    results[it, 4:6] = scores[2]\n",
    "    results[it, 6:8] = scores[3]\n",
    "    results[it, 8] = acc \n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print('iteration {} elapsed time: {}, accuracy : {}'.format(it+1, end-start, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c9a2433-e48a-459f-bc98-a1d16f330f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.696232</td>\n",
       "      <td>0.271859</td>\n",
       "      <td>0.387784</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.693130</td>\n",
       "      <td>0.929261</td>\n",
       "      <td>0.793778</td>\n",
       "      <td>352.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall        f1  support\n",
       "0   0.696232  0.271859  0.387784    199.0\n",
       "1   0.693130  0.929261  0.793778    352.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061029</td>\n",
       "      <td>0.034337</td>\n",
       "      <td>0.027304</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.027761</td>\n",
       "      <td>0.008786</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall        f1  support\n",
       "0   0.061029  0.034337  0.027304      0.0\n",
       "1   0.004978  0.027761  0.008786      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy mean: 0.6918330308529945, std: 0.008148800116386076\n"
     ]
    }
   ],
   "source": [
    "def display_table(scores):\n",
    "    df = np.reshape(scores, [2,4], order ='F')\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    # df.style.set_table_attributes(\"style='display:inline'\").set_caption(mode)\n",
    "    \n",
    "    df.columns = ['precision', 'recall', 'f1', 'support']\n",
    "    # df.index = ['unfrozen', 'frozen']\n",
    "    # df.index = ['Visible ice', 'No visible ice']\n",
    "    \n",
    "    display(df)\n",
    "def display_results(results):\n",
    "    mean = np.mean(results, axis=0)\n",
    "    std = np.std(results, axis=0)\n",
    "    \n",
    "    print(\"mean\")\n",
    "    display_table(mean[0:8])\n",
    "    \n",
    "    print(\"std\")\n",
    "    display_table(std[0:8])\n",
    "    \n",
    "    print(\"Accuracy mean: {}, std: {}\".format(mean[8], std[8]))\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c067723-4947-49a5-915e-c4ac4611cfba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
