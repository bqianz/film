{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f98096-bf83-41be-a7d5-72f7794cb779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2675d831-08ab-43f7-b625-cfab6c6128f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_width = 32\n",
    "hidden_nblocks = 4\n",
    "train_max_epoch = 50\n",
    "max_iterations = 10\n",
    "\n",
    "chip_size = 32\n",
    "data_root = \"geomorph_data\"\n",
    "# data_root = \"geomorph_data_test\"\n",
    "n_channels = len(os.listdir(data_root))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "L2_param = 1e-5\n",
    "\n",
    "label_name = \"visible_ice\"\n",
    "num_classes = 5\n",
    "output_size = num_classes\n",
    "\n",
    "sm = nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddd8266e-da2e-4871-a8f4-7d37702f30bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6248, -0.2995,  0.3713, -1.1229,  0.0562],\n",
      "        [ 0.5751, -0.4645, -0.2360,  0.7078, -0.1608],\n",
      "        [-2.0590, -0.0140, -0.3996, -0.9387, -0.4033]], requires_grad=True)\n",
      "tensor([2, 1, 4])\n",
      "tensor(1.6800, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "print(input)\n",
    "print(target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b59b6ad-d7be-476f-b4bd-c5d6ce57e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8474,  0.9359, -0.7293],\n",
      "        [-2.8762, -2.0847, -0.2615]])\n",
      "tensor([[0.1238, 0.7368, 0.1394],\n",
      "        [0.0593, 0.1308, 0.8099]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=-1)\n",
    "# m = nn.Sigmoid()\n",
    "input = torch.randn(2,3)\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "629fd05b-9fa8-4502-b8cb-e0892a432591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "        def __init__(self, input_size, output_size = 1, hidden_width = 20, hidden_nblocks = 2):\n",
    "            super(mlp, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.hidden_width = hidden_width\n",
    "            self.hidden_nblocks = hidden_nblocks\n",
    "            \n",
    "            self.fc1 = nn.Linear(self.input_size, self.hidden_width)\n",
    "            self.fc2 = nn.Linear(self.hidden_width,self.hidden_width)\n",
    "            self.fc3 = nn.Linear(self.hidden_width, self.output_size)\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.end= torch.nn.Softmax(dim = -1) ## sigmoid for multi-label, softmax for multi-class (mutually exclusive)\n",
    "            \n",
    "            self.dropout = nn.Dropout(0.25)\n",
    "            \n",
    "        def forward(self, x, film_params):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            \n",
    "            \n",
    "            for i in range(self.hidden_nblocks):\n",
    "                out = self.fc2(out)\n",
    "                \n",
    "                # ------- film layer -----------\n",
    "                start = i * hidden_width * 2\n",
    "                mid = start + hidden_width\n",
    "                end = mid + hidden_width\n",
    "                \n",
    "                gamma = film_params[:, start : mid]\n",
    "                beta = film_params[:, mid : end]\n",
    "                \n",
    "#                 print(out.shape)\n",
    "#                 print(gamma.shape)\n",
    "#                 print(beta.shape)\n",
    "                \n",
    "                out = out * gamma\n",
    "                out += beta\n",
    "                # ------- film layer -----------\n",
    "                # out = self.dropout(out)\n",
    "                out = self.relu(out)\n",
    "            \n",
    "            out = self.fc3(out)\n",
    "            # out = self.end(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905bba3-9233-41cb-8ed6-ffa9087f4e8f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93742035-18c5-42bc-b9e8-86134e61c23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2837\n",
      "566\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TOPSOIL</td>\n",
       "      <td>Organics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>-1.024190</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pure ice</td>\n",
       "      <td>ICE</td>\n",
       "      <td>Ice</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>-0.835900</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>SW-SM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>-0.553466</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GW-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>0.387982</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.16105</td>\n",
       "      <td>-133.08880</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GP-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.437795</td>\n",
       "      <td>1.842620</td>\n",
       "      <td>-0.741756</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude                  time   borehole  depth  frozen  \\\n",
       "0  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.15       0   \n",
       "1  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.85       1   \n",
       "2  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   1.90       1   \n",
       "3  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   5.40       1   \n",
       "4  69.16105 -133.08880  2012-03-21T00:00:00Z  0170-1-12   1.20       1   \n",
       "\n",
       "  cryostructures     visible_ice ASTM_2488    materials  ...  top_of_interval  \\\n",
       "0            NaN             NaN   TOPSOIL     Organics  ...              0.0   \n",
       "1            NaN        Pure ice       ICE          Ice  ...              0.3   \n",
       "2             Nf  No visible ice     SW-SM  Coarse till  ...              1.4   \n",
       "3             Nf  No visible ice     GW-GM  Coarse till  ...              2.4   \n",
       "4             Nf  No visible ice     GP-GM  Coarse till  ...              0.0   \n",
       "\n",
       "   bottom_of_interval  month  year  month_cyclic  lat_norm  lng_norm  \\\n",
       "0                 0.3      3  2012             3  1.439946  1.851455   \n",
       "1                 1.4      3  2012             3  1.439946  1.851455   \n",
       "2                 2.4      3  2012             3  1.439946  1.851455   \n",
       "3                 8.4      3  2012             3  1.439946  1.851455   \n",
       "4                 2.4      3  2012             3  1.437795  1.842620   \n",
       "\n",
       "   depth_norm  year_norm  month_cyclic_norm  \n",
       "0   -1.024190  -1.164992          -1.225295  \n",
       "1   -0.835900  -1.164992          -1.225295  \n",
       "2   -0.553466  -1.164992          -1.225295  \n",
       "3    0.387982  -1.164992          -1.225295  \n",
       "4   -0.741756  -1.164992          -1.225295  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "def normalize(values):\n",
    "    # zero mean, unit variance\n",
    "    value_mean = values.mean()\n",
    "    value_std = values.std()\n",
    "    return (values-values_mean)/values_std\n",
    "\n",
    "def normalize_maxmin(values):\n",
    "    # range from 0 to 1\n",
    "    (values-values.min())/(values.max()-values.min())\n",
    "\n",
    "\n",
    "def get_scaler(data):\n",
    "    scaler = StandardScaler()\n",
    "    print(data)\n",
    "    scaler.fit(data)\n",
    "    return scaler\n",
    "    \n",
    "def preprocess_df(df):\n",
    "    # convert timecodes to year and month columns\n",
    "    datetimes = pd.to_datetime(df['time'])\n",
    "    df['month'] = datetimes.dt.month\n",
    "    df['year'] = datetimes.dt.year\n",
    "\n",
    "    df['month_cyclic'] = 7 - abs(df['month'] - 7)\n",
    "    \n",
    "    data = df[['latitude', 'longitude', 'depth', 'year', 'month_cyclic']]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    df[['lat_norm', 'lng_norm', 'depth_norm', 'year_norm', 'month_cyclic_norm']] = scaler.transform(df[['latitude', 'longitude', 'depth', 'year', 'month_cyclic']])\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "\n",
    "#     df['lat_norm'] = normalize(df['latitude'])\n",
    "#     df['lng_norm'] = normalize(df['longitude'])\n",
    "#     df['depth_norm'],  = normalize(df['depth'])\n",
    "#     df['year_norm'] = normalize(df['year'])\n",
    "#     df['month_cyclic_norm'] = normalize(df['month_cyclic'])\n",
    "\n",
    "df = pd.read_csv('data_stephen_fix_header.csv', header=[0])\n",
    "scaler = preprocess_df(df)\n",
    "    \n",
    "print(df.shape[0])\n",
    "print(df['borehole'].nunique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf25c9ad-dfd3-4503-ba42-c3ad78c5370a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.44755\n",
      "68.36933\n",
      "-132.89346\n",
      "-133.82843\n"
     ]
    }
   ],
   "source": [
    "print(df.latitude.max())\n",
    "print(df.latitude.min())\n",
    "print(df.longitude.max())\n",
    "print(df.longitude.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10b03030-dd00-4063-948c-24493aafdd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>68.38262</td>\n",
       "      <td>-133.71211</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH15</td>\n",
       "      <td>5.45</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf/Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.499167</td>\n",
       "      <td>-0.938559</td>\n",
       "      <td>0.401431</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>68.38262</td>\n",
       "      <td>-133.71211</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH15</td>\n",
       "      <td>8.65</td>\n",
       "      <td>1</td>\n",
       "      <td>Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>9.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.499167</td>\n",
       "      <td>-0.938559</td>\n",
       "      <td>1.262184</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>ORGANICS</td>\n",
       "      <td>Organics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494489</td>\n",
       "      <td>-0.927672</td>\n",
       "      <td>-1.051089</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494489</td>\n",
       "      <td>-0.927672</td>\n",
       "      <td>-0.970393</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1</td>\n",
       "      <td>Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494489</td>\n",
       "      <td>-0.927672</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude  longitude                  time           borehole  depth  \\\n",
       "2832  68.38262 -133.71211  2013-04-27T00:00:00Z  W14103137-S6-BH15   5.45   \n",
       "2833  68.38262 -133.71211  2013-04-27T00:00:00Z  W14103137-S6-BH15   8.65   \n",
       "2834  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   0.05   \n",
       "2835  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   0.35   \n",
       "2836  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   4.85   \n",
       "\n",
       "      frozen cryostructures     visible_ice ASTM_2488 materials  ...  \\\n",
       "2832       1         Nf/Nbn  No visible ice       NaN      Till  ...   \n",
       "2833       1            Nbn  No visible ice       NaN      Till  ...   \n",
       "2834       0            NaN  No visible ice  ORGANICS  Organics  ...   \n",
       "2835       0            NaN  No visible ice       NaN      Till  ...   \n",
       "2836       1            Nbn  No visible ice       NaN      Till  ...   \n",
       "\n",
       "      top_of_interval  bottom_of_interval  month  year  month_cyclic  \\\n",
       "2832              2.7                 8.2      4  2013             4   \n",
       "2833              8.2                 9.1      4  2013             4   \n",
       "2834              0.0                 0.1      4  2013             4   \n",
       "2835              0.1                 0.6      4  2013             4   \n",
       "2836              0.6                 9.1      4  2013             4   \n",
       "\n",
       "      lat_norm  lng_norm  depth_norm  year_norm  month_cyclic_norm  \n",
       "2832 -1.499167 -0.938559    0.401431  -0.319053           0.736422  \n",
       "2833 -1.499167 -0.938559    1.262184  -0.319053           0.736422  \n",
       "2834 -1.494489 -0.927672   -1.051089  -0.319053           0.736422  \n",
       "2835 -1.494489 -0.927672   -0.970393  -0.319053           0.736422  \n",
       "2836 -1.494489 -0.927672    0.240040  -0.319053           0.736422  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['visible_ice'].replace(['None'], 'No visible ice', regex=True, inplace=True)\n",
    "\n",
    "ordered_ice = ['No visible ice', 'Low', \"Medium to high\", 'High', 'Pure ice']\n",
    "df['visible_ice'] = pd.Series(pd.Categorical(df['visible_ice'], categories=ordered_ice, ordered=True))\n",
    "\n",
    "df2 = df.dropna(subset=['visible_ice'])\n",
    "\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5e66d4b-f80d-4295-8f97-38f6603c4ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check None values have been replaced\n",
    "len(df2[df2['visible_ice'] == 'None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00107137-4ac0-49a9-bae8-7e5aa1883ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pure ice', 'No visible ice', 'High', 'Medium to high', 'Low']\n",
      "Categories (5, object): ['No visible ice' < 'Low' < 'Medium to high' < 'High' < 'Pure ice']\n",
      "[4 0 3 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-b83c4bbc5a30>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['visible_ice_code'] =  df2['visible_ice'].cat.codes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1             Pure ice\n",
       "2       No visible ice\n",
       "3       No visible ice\n",
       "4       No visible ice\n",
       "5       No visible ice\n",
       "             ...      \n",
       "2832    No visible ice\n",
       "2833    No visible ice\n",
       "2834    No visible ice\n",
       "2835    No visible ice\n",
       "2836    No visible ice\n",
       "Name: visible_ice, Length: 2752, dtype: category\n",
       "Categories (5, object): ['No visible ice' < 'Low' < 'Medium to high' < 'High' < 'Pure ice']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['visible_ice_code'] =  df2['visible_ice'].cat.codes\n",
    "print(df2['visible_ice'].unique())\n",
    "print(df2['visible_ice_code'].unique())\n",
    "df2['visible_ice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21c3d6e0-5ed8-4d93-ad53-e6a84b8a007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visible_ice = pd.get_dummies(df2.visible_ice)\n",
    "# bin_visible_ice = (~visible_ice['No visible ice'].astype('bool')).astype('int')\n",
    "# bin_visible_ice.value_counts()\n",
    "\n",
    "# df2['visible_ice'] = bin_visible_ice\n",
    "\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1837f5df-ee84-4d0d-9942-14b948a37934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.dropna(subset=['materials'])\n",
    "df3['materials'].replace(['ICE'], 'Ice', regex=True, inplace=True)\n",
    "df3['materials'].replace(['ice'], 'Ice', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dba8d5fb-1a64-4fea-a69a-20b744e097a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_materials = pd.get_dummies(df3.materials)\n",
    "df3['material_ice'] = dm_materials['Ice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32ecd3dc-6e04-48d5-84b0-6b3ce4251644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = pd.read_csv(r\"C:\\Users\\mouju\\Desktop\\film\\components_analysis\\df_unique.csv\", header=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75c3c456-b700-46cd-9a6d-52872ba14aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Geo90Dataset(Dataset):\n",
    "    def __init__(self, data_root, df, base_lat, base_lng, chip_size=32):\n",
    "        \n",
    "        self.base_lat = base_lat\n",
    "        self.base_lng = base_lng\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        self.chip_size = chip_size\n",
    "        \n",
    "        self.trans = transforms.ToTensor()\n",
    "        \n",
    "        self.n_channels = len(os.listdir(data_root))\n",
    "        self.preloaded = torch.ones(self.n_channels, 6000, 6000)\n",
    "        \n",
    "        for i, file in enumerate(os.listdir(data_root)):\n",
    "            # name = file.split('_')[0]\n",
    "            # print(name)\n",
    "            \n",
    "            I = np.array(Image.open(data_root + os.path.sep + file))\n",
    "            print(I.shape)\n",
    "            # I = plt.imread(data_root + os.path.sep + file)\n",
    "            \n",
    "#             print(I.max())\n",
    "#             print(I.min())\n",
    "            \n",
    "#             # normalize\n",
    "#             I = (((I - I.min()) / (I.max() - I.min())) * 255.9).astype(np.uint8)\n",
    "#             print(I.max())\n",
    "#             print(I.min())\n",
    "            \n",
    "            self.preloaded[i] = self.trans(I)\n",
    "            \n",
    "        \n",
    "        \n",
    "        print('Dataset initialized')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        bh_id = row.at['borehole']\n",
    "        lat = row.at['latitude']\n",
    "        lng = row.at['longitude']\n",
    "        \n",
    "\n",
    "        pixel_len = 5/6000\n",
    "        \n",
    "\n",
    "        lat_index_start = np.round((self.base_lat - lat) / pixel_len - self.chip_size/2).astype(int)\n",
    "        lat_index_end = lat_index_start + self.chip_size\n",
    "        \n",
    "        lng_index_start = np.round((lng - self.base_lng) / pixel_len - self.chip_size/2).astype(int)\n",
    "        lng_index_end = lng_index_start + self.chip_size\n",
    "        \n",
    "        image = self.preloaded[:, lat_index_start:lat_index_end,lng_index_start:lng_index_end]\n",
    "        \n",
    "        \n",
    "        # surface = torch.tensor(row.filter(['depth'])).float()\n",
    "        surface = torch.tensor(row.filter(['depth_norm', 'month_cyclic_norm', 'lat_norm', 'lng_norm', 'year_norm'])).float()\n",
    "        \n",
    "        frozen = torch.tensor(row.at['frozen']).float()\n",
    "        \n",
    "        # visible_ice = torch.tensor(row.at['visible_ice']).float()\n",
    "        visible_ice = torch.tensor(row.at['visible_ice_code']).long()\n",
    "        \n",
    "        # material_ice = torch.tensor(row.at['material_ice']).float()\n",
    "        \n",
    "        return {'image': image, 'surface_data': surface, 'frozen': frozen,  'visible_ice': visible_ice} #'material_ice': material_ice}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d26ba2b-3c00-436f-a175-933f8b97eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "(6000, 6000)\n",
      "Dataset initialized\n"
     ]
    }
   ],
   "source": [
    "base_lat = 70\n",
    "base_lng = -135\n",
    "\n",
    "loaded_dataset = Geo90Dataset(data_root, df2, base_lat, base_lng, chip_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "153cc6da-674e-4a77-b3ca-1324682c7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_dataset[0]['image'].shape)\n",
    "\n",
    "# chips_root = \"geomorph_data_test_chips\"\n",
    "\n",
    "# for i, data in enumerate(test_dataset):\n",
    "    \n",
    "#     save_image(data['image'], os.path.join(\"geomorph_data_test_chips\", f'{i:04d}.png'))\n",
    "#     # np.save(os.path.join(chips_root, f'{i:04d}.npy'), data['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b0df9-d51d-4bb3-9658-7a49e95bc6a6",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f46b0-7537-4ceb-8f96-263c007e97fd",
   "metadata": {},
   "source": [
    "## FiLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "214c4911-bcfb-43a2-b41b-b01949690b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_film_params = hidden_width * hidden_nblocks * 2\n",
    "\n",
    "# generator = resnet18(n_channels, n_film_params)\n",
    "\n",
    "def train_model(trainloader, testloader, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    # loss: binary cross entropy\n",
    "\n",
    "    generator = models.resnet18()\n",
    "    generator.fc = nn.Linear(512, n_film_params)\n",
    "    generator.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    gen_model = generator\n",
    "\n",
    "    # print(gen_model)\n",
    "\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    net_model = mlp(input_size[0],output_size, hidden_width, hidden_nblocks).to(device)\n",
    "    \n",
    "    gen_optimizer = torch.optim.Adam(gen_model.parameters(), weight_decay = L2_param)\n",
    "    net_optimizer = torch.optim.Adam(net_model.parameters(), weight_decay = L2_param)\n",
    "    \n",
    "    gen_model.to(device)\n",
    "    net_model.to(device)\n",
    "\n",
    "    # --------- check back propagation ----------- -\n",
    "    # net_model.fc1.weight.register_hook(lambda x: print('grad accumulated in mlp fc1'))\n",
    "    # gen_first_layer = gen_model.encoder.blocks[0].blocks[0].blocks[0].conv\n",
    "    # gen_first_layer.weight.register_hook(lambda x: print('grad accumulated in resnet first layer'))\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        # ------------ train -----------------\n",
    "        gen_model.train()\n",
    "        net_model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            \n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "\n",
    "            gen_optimizer.zero_grad()\n",
    "            net_optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            gen_optimizer.step()\n",
    "            net_optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        gen_model.eval()\n",
    "        net_model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # TODO: exammine film_params gradients / readup pytorch\n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "\n",
    "        torch.save(gen_model.state_dict(), os.path.join('mlp-resnet-models/', 'gen-epoch-{}.pt'.format(epoch+1)))\n",
    "        torch.save(net_model.state_dict(), os.path.join('mlp-resnet-models/', 'net-epoch-{}.pt'.format(epoch+1)))\n",
    "\n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "## Test model\n",
    "\n",
    "def test_model(epoch_loss, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    generator = models.resnet18()\n",
    "    generator.fc = nn.Linear(512, n_film_params)\n",
    "    generator.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    gen_model = generator\n",
    "\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    net_model = mlp(input_size[0], output_size, hidden_width, hidden_nblocks)\n",
    "\n",
    "    gen_model.load_state_dict(torch.load('mlp-resnet-models/gen-epoch-{}.pt'.format(ind+1)))\n",
    "    net_model.load_state_dict(torch.load('mlp-resnet-models/net-epoch-{}.pt'.format(ind+1)))\n",
    "    \n",
    "    gen_model.to(device)\n",
    "    net_model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    gen_model.eval()\n",
    "    net_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            \n",
    "            output = sm(predicted)\n",
    "            # print(output)\n",
    "            \n",
    "            \n",
    "            max_results = torch.max(output, dim= -1)\n",
    "            predicted = max_results.indices\n",
    "            #predicted = torch.round(output)\n",
    "            # print(predicted.shape)\n",
    "            \n",
    "            y_test.extend(labels.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            \n",
    "#             predicted = torch.squeeze(predicted)\n",
    "\n",
    "#             predicted = torch.round(predicted)\n",
    "#             # print(predicted.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "#             lb = labels.tolist()\n",
    "#             pr = predicted.tolist()\n",
    "#             y_test.extend(lb)\n",
    "#             y_pred.extend(pr)\n",
    "    \n",
    "    arr_accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "    return arr_accuracy, scores, gen_model, net_model\n",
    "\n",
    "\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "#     print(classification_report(y_test,y_pred))\n",
    "#     print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "## Pure MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8b5b0-1039-4c76-93e4-0f90b60d87be",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43c02928-f9c5-4b2a-8ef7-d8a2de4a4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_pure(nn.Module):\n",
    "        def __init__(self, input_size, output_size):\n",
    "            super(mlp_pure, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.hidden_size = hidden_width\n",
    "            self.hidden_nblocks = hidden_nblocks\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size,self.hidden_size)\n",
    "            self.fc3 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.end = torch.nn.Softmax(dim=-1) ## sigmoid for multi-label, softmax for multi-class (mutually exclusive)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            # print(out.shape)\n",
    "            \n",
    "            for i in range(self.hidden_nblocks):\n",
    "                out = self.fc2(out)\n",
    "                out = self.relu(out)\n",
    "            \n",
    "            out = self.fc3(out)\n",
    "            # out = self.end(out)\n",
    "            return out\n",
    "\n",
    "def train_mlp(trainloader, testloader, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    surface_model = mlp_pure(input_size[0],output_size)\n",
    "    \n",
    "    surface_model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(surface_model.parameters(), weight_decay = L2_param)\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        surface_model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            predicted = surface_model(surface_data)\n",
    "            \n",
    "#             print(predicted.squeeze().shape)\n",
    "#             print(labels.shape)\n",
    "            \n",
    "            \n",
    "            # squeeze: return tensor with all dimensions of size 1 removed\n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        surface_model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            predicted = surface_model(surface_data)\n",
    "            \n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "\n",
    "        torch.save(surface_model.state_dict(), os.path.join('mlp-models/', 'epoch-{}.pt'.format(epoch+1)))\n",
    "    \n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "        \n",
    "    return epoch_loss\n",
    "        \n",
    "def test_mlp(epoch_loss, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    \n",
    "    surface_model = mlp_pure(input_size[0],output_size)\n",
    "    surface_model.load_state_dict(torch.load('mlp-models/epoch-{}.pt'.format(ind+1)))\n",
    "    \n",
    "    surface_model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    surface_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        y_cert = []\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            output = surface_model(surface_data)\n",
    "            \n",
    "            output = sm(output)\n",
    "            # print(output)\n",
    "            \n",
    "            \n",
    "            max_results = torch.max(output, dim= -1)\n",
    "            predicted = max_results.indices\n",
    "            certainty = max_results.values\n",
    "            #predicted = torch.round(output)\n",
    "            # print(predicted.shape)\n",
    "            \n",
    "            y_test.extend(labels.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            y_cert.extend(certainty.tolist())\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "#     print(y_test)\n",
    "#     print(y_pred)\n",
    "    with open(\"mlp-certainty/iteration_{}.txt\".format(it), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(y_cert, fp)\n",
    "    #with open(\"test.txt\", \"rb\") as fp:   # Unpickling\n",
    "        #b = pickle.load(fp)\n",
    "    arr_accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "    return arr_accuracy, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cfb6b-c5ca-4fdd-81e9-571685a86522",
   "metadata": {},
   "source": [
    "## Multiple Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44629f3-5888-4eb0-8f54-cfd63f455c54",
   "metadata": {},
   "source": [
    "### Discard samples with invalid values in image chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc861435-97ab-4095-9760-4b797477daa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752\n",
      "2717\n"
     ]
    }
   ],
   "source": [
    "valid_ind = []\n",
    "for i, data in enumerate(loaded_dataset):\n",
    "    image = data['image']\n",
    "    ind  = (image == -9999)\n",
    "    if ~torch.any(ind):\n",
    "        valid_ind.append(i)\n",
    "\n",
    "full_dataset = torch.utils.data.Subset(loaded_dataset, valid_ind)\n",
    "\n",
    "\n",
    "print(len(loaded_dataset))\n",
    "print(len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba299154-57ac-4e30-bf2e-74e41e3fc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# batchsize can cause error when last leftover batchsize is 1, batchnorm cannot function on 1 sample data\n",
    "batchsize = 20\n",
    "while(train_size % batchsize == 1):\n",
    "    batchsize+=1\n",
    "print(batchsize)\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(full_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb6971-4f9e-409e-b8ee-9ebe27a3cfae",
   "metadata": {},
   "source": [
    "## Scale of image chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e37318c4-ff02-48e0-aa8c-c5a708eb0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = full_dataset[0]['image']\n",
    "# n_samples = len(train_data)\n",
    "# n_channels = list(image.shape)[0]\n",
    "\n",
    "# scalers = []\n",
    "# for i in range(n_channels):\n",
    "#     print(i)\n",
    "#     scaler = StandardScaler()\n",
    "#     X = torch.empty((n_samples, chip_size, chip_size))\n",
    "    \n",
    "#     for j, data in enumerate(train_data):\n",
    "#         #print(data['image'][i].shape)\n",
    "#         # print(X[j].shape)\n",
    "#         X[j] = data['image'][i]\n",
    "#     X = torch.reshape(X, (-1,1))\n",
    "#     #print(X)\n",
    "#     # break;\n",
    "#     scaler.fit(X)\n",
    "#     scalers.append(scaler)\n",
    "    \n",
    "#     def scale_data(subset):\n",
    "#         for data in subset:\n",
    "#             X = data['image'][i]\n",
    "#             X_flat = torch.reshape(X, (-1,1))\n",
    "            \n",
    "#             X_trans = scaler.transform(X_flat)\n",
    "#             data['image'][i] = torch.reshape(torch.Tensor(X_trans), (chip_size, chip_size))\n",
    "    \n",
    "#     scale_data(train_data)\n",
    "#     scale_data(test_data)\n",
    "#     print(\"Channel {} scaled.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f7a6d03-a8b0-4c0f-b562-415f30deb0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mouju\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: running loss: 1.43530, validation loss: 1.33109\n",
      "epoch  2: running loss: 1.37604, validation loss: 1.36154\n",
      "epoch  3: running loss: 1.37325, validation loss: 1.35365\n",
      "epoch  4: running loss: 1.37700, validation loss: 1.38505\n",
      "epoch  5: running loss: 1.36856, validation loss: 1.37667\n",
      "epoch  6: running loss: 1.34291, validation loss: 1.31264\n",
      "epoch  7: running loss: 1.33766, validation loss: 1.31906\n",
      "epoch  8: running loss: 1.33471, validation loss: 1.35965\n",
      "epoch  9: running loss: 1.33132, validation loss: 1.38011\n",
      "epoch 10: running loss: 1.32185, validation loss: 1.34067\n",
      "epoch 11: running loss: 1.31475, validation loss: 1.33611\n",
      "epoch 12: running loss: 1.30107, validation loss: 1.31841\n",
      "epoch 13: running loss: 1.27763, validation loss: 1.33137\n",
      "epoch 14: running loss: 1.28226, validation loss: 1.35673\n",
      "epoch 15: running loss: 1.28026, validation loss: 1.34811\n",
      "epoch 16: running loss: 1.26578, validation loss: 1.38884\n",
      "epoch 17: running loss: 1.26099, validation loss: 1.29648\n",
      "epoch 18: running loss: 1.24935, validation loss: 1.31676\n",
      "epoch 19: running loss: 1.23851, validation loss: 1.36596\n",
      "epoch 20: running loss: 1.23780, validation loss: 1.36493\n",
      "epoch 21: running loss: 1.22716, validation loss: 1.34349\n",
      "epoch 22: running loss: 1.21104, validation loss: 1.35576\n",
      "epoch 23: running loss: 1.27420, validation loss: 1.44001\n",
      "epoch 24: running loss: 1.29637, validation loss: 1.32553\n",
      "epoch 25: running loss: 1.22707, validation loss: 1.34129\n",
      "epoch 26: running loss: 1.20657, validation loss: 1.32789\n",
      "epoch 27: running loss: 1.20959, validation loss: 1.30937\n",
      "epoch 28: running loss: 1.17721, validation loss: 1.34903\n",
      "epoch 29: running loss: 1.17157, validation loss: 1.34542\n",
      "epoch 30: running loss: 1.16942, validation loss: 1.34086\n",
      "epoch 31: running loss: 1.15302, validation loss: 1.35596\n",
      "epoch 32: running loss: 1.15248, validation loss: 1.33642\n",
      "epoch 33: running loss: 1.13609, validation loss: 1.32936\n",
      "epoch 34: running loss: 1.12833, validation loss: 1.38832\n",
      "epoch 35: running loss: 1.13051, validation loss: 1.36616\n",
      "epoch 36: running loss: 1.11933, validation loss: 1.40782\n",
      "epoch 37: running loss: 1.12274, validation loss: 1.37776\n",
      "epoch 38: running loss: 1.09793, validation loss: 1.39419\n",
      "epoch 39: running loss: 1.11434, validation loss: 1.34773\n",
      "epoch 40: running loss: 1.09706, validation loss: 1.35094\n",
      "epoch 41: running loss: 1.08226, validation loss: 1.39960\n",
      "epoch 42: running loss: 1.07433, validation loss: 1.39503\n",
      "epoch 43: running loss: 1.07616, validation loss: 1.39097\n",
      "epoch 44: running loss: 1.05592, validation loss: 1.43391\n",
      "epoch 45: running loss: 1.07259, validation loss: 1.49391\n",
      "epoch 46: running loss: 1.06812, validation loss: 1.43909\n",
      "epoch 47: running loss: 1.04331, validation loss: 1.43604\n",
      "epoch 48: running loss: 1.04663, validation loss: 1.43400\n",
      "epoch 49: running loss: 1.03742, validation loss: 1.48227\n",
      "epoch 50: running loss: 1.03579, validation loss: 1.42952\n",
      "Finished Training\n",
      "epoch 17 model selected\n",
      "iteration 1 elapsed time: 198.92166018486023, accuracy : 0.4172794117647059\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 1\n",
    "results = np.zeros([max_iterations, num_classes*4 + 1])\n",
    "# trainloader, testloader = prepare_dataloader(full_dataset)\n",
    "\n",
    "for it in range(max_iterations):\n",
    "    start = time.time()\n",
    "    \n",
    "    # mlp\n",
    "#     epoch_loss_mlp = train_mlp(trainloader,testloader, print_epochs = True, loss_fn = nn.CrossEntropyLoss())\n",
    "#     acc, scores = test_mlp(epoch_loss_mlp, print_model_epoch = True)\n",
    "    \n",
    "    #     # ------- mlp-resnet film \n",
    "    epoch_loss = train_model(trainloader, testloader, print_epochs=True, loss_fn = nn.CrossEntropyLoss())\n",
    "    acc, scores, gen_model, net_model = test_model(epoch_loss, print_model_epoch = True)\n",
    "    \n",
    "    # scores = precision, recall, fscore, support\n",
    "    results[it, 0] = acc\n",
    "    \n",
    "    for j, score in enumerate(scores):\n",
    "        start_ind = 1 + j*num_classes\n",
    "        results[it, start_ind: start_ind + num_classes] = score\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print('iteration {} elapsed time: {}, accuracy : {}'.format(it+1, end-start, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c5555c3-2ed2-4e85-9c9b-0c3985f51881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 model selected\n"
     ]
    }
   ],
   "source": [
    "acc, scores, gen_model, net_model = test_model(epoch_loss, print_model_epoch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c9a2433-e48a-459f-bc98-a1d16f330f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.425676</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.505010</td>\n",
       "      <td>203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.177632</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.377551</td>\n",
       "      <td>0.524823</td>\n",
       "      <td>0.439169</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall        f1  support\n",
       "0   0.425676  0.620690  0.505010    203.0\n",
       "1   0.519231  0.177632  0.264706    152.0\n",
       "2   0.377551  0.524823  0.439169    141.0\n",
       "3   0.000000  0.000000  0.000000     15.0\n",
       "4   0.000000  0.000000  0.000000     33.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision  recall   f1  support\n",
       "0        0.0     0.0  0.0      0.0\n",
       "1        0.0     0.0  0.0      0.0\n",
       "2        0.0     0.0  0.0      0.0\n",
       "3        0.0     0.0  0.0      0.0\n",
       "4        0.0     0.0  0.0      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy mean: 0.4172794117647059, std: 0.0\n"
     ]
    }
   ],
   "source": [
    "def display_table(scores):\n",
    "    df = np.reshape(scores, [num_classes,4], order ='F')\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    # df.style.set_table_attributes(\"style='display:inline'\").set_caption(mode)\n",
    "    \n",
    "    df.columns = ['precision', 'recall', 'f1', 'support']\n",
    "    # df.index = ['unfrozen', 'frozen']\n",
    "    # df.index = ['Visible ice', 'No visible ice']\n",
    "    \n",
    "    display(df)\n",
    "    \n",
    "def display_results(results):\n",
    "    mean = np.mean(results, axis=0)\n",
    "    std = np.std(results, axis=0)\n",
    "    \n",
    "    print(\"mean\")\n",
    "    display_table(mean[1:])\n",
    "    \n",
    "    print(\"std\")\n",
    "    display_table(std[1:])\n",
    "    \n",
    "    print(\"Accuracy mean: {}, std: {}\".format(mean[0], std[0]))\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360fa50-304f-4e3b-b33a-51b9f820a580",
   "metadata": {},
   "source": [
    "## Display Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b51afa27-1fb7-4bf4-9da2-82b90a263657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.1\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(df.depth.max())\n",
    "print(df.depth.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b8c1174-a942-4f78-8a9f-4d1bb0f8b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_max = 69.5\n",
    "latitude_min = 68.3\n",
    "longitude_max = -132.8\n",
    "longitude_min = -133.9\n",
    "\n",
    "n_lat = 40\n",
    "n_lng = 20\n",
    "\n",
    "\n",
    "lng_range = np.linspace(longitude_min, longitude_max, n_lng)\n",
    "lat_range = np.linspace(latitude_max, latitude_min, n_lat)\n",
    "depth_range = [1, 3, 5, 7, 9]\n",
    "n_depth = len(depth_range)\n",
    "\n",
    "grid_lng, grid_lat, grid_depth = np.meshgrid(lng_range, lat_range, depth_range)\n",
    "\n",
    "\n",
    "\n",
    "# e.g \n",
    "# grid_lng[0, :, 0] = [-133.9, ... -132.8]\n",
    "# grid_lat[:,0,0] = [69.5, ..., 68.3]\n",
    "\n",
    "# rows: latitude, columns: longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "760cb363-06ed-4756-a8a3-defd0f81da4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69.5       , 69.46923077, 69.43846154, 69.40769231, 69.37692308,\n",
       "       69.34615385, 69.31538462, 69.28461538, 69.25384615, 69.22307692,\n",
       "       69.19230769, 69.16153846, 69.13076923, 69.1       , 69.06923077,\n",
       "       69.03846154, 69.00769231, 68.97692308, 68.94615385, 68.91538462,\n",
       "       68.88461538, 68.85384615, 68.82307692, 68.79230769, 68.76153846,\n",
       "       68.73076923, 68.7       , 68.66923077, 68.63846154, 68.60769231,\n",
       "       68.57692308, 68.54615385, 68.51538462, 68.48461538, 68.45384615,\n",
       "       68.42307692, 68.39230769, 68.36153846, 68.33076923, 68.3       ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lat[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "946bd4b9-c52f-4c0c-aec0-db5dd6176c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn = pd.DataFrame(columns=['latitude', 'longitude', 'depth', 'year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9d4dce3-206c-4862-b6b9-18bf06583869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn['latitude'] = grid_lat.flatten()\n",
    "df_syn['longitude'] = grid_lng.flatten()\n",
    "df_syn['depth'] = grid_depth.flatten()\n",
    "df_syn['year'] = 2013\n",
    "df_syn['month'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5aa90869-47cc-481f-b33e-e28f1bcb17ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>68.3</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>68.3</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>68.3</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>68.3</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>7</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>68.3</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>9</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude  longitude  depth  year  month\n",
       "3995      68.3     -132.8      1  2013      3\n",
       "3996      68.3     -132.8      3  2013      3\n",
       "3997      68.3     -132.8      5  2013      3\n",
       "3998      68.3     -132.8      7  2013      3\n",
       "3999      68.3     -132.8      9  2013      3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_syn.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca2e1789-890d-45cd-86a4-9cc1b2124194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  68.77996824, -133.50176278,    3.95760839, 2013.37715897,\n",
       "          3.62460345])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ceb5256c-18c4-429c-b4d5-947a845e9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn = scaler.transform(df_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "213f1a29-7853-43c9-af0b-b3fccfe40351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71663018, -1.77691568, -0.79555272, -0.31905326, -1.22529482],\n",
       "       [ 2.71663018, -1.77691568, -0.25758243, -0.31905326, -1.22529482],\n",
       "       [ 2.71663018, -1.77691568,  0.28038786, -0.31905326, -1.22529482],\n",
       "       ...,\n",
       "       [-1.81088706,  3.13123246,  0.28038786, -0.31905326, -1.22529482],\n",
       "       [-1.81088706,  3.13123246,  0.81835815, -0.31905326, -1.22529482],\n",
       "       [-1.81088706,  3.13123246,  1.35632844, -0.31905326, -1.22529482]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "272aaf1a-afbb-4a68-b560-b80f30a2d280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8359, -1.2253,  1.4399,  1.8515, -1.1650])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0]['surface_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56c8a0d6-6de8-4fce-b995-099c3d73f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 model selected\n"
     ]
    }
   ],
   "source": [
    "# ------ select model ---------\n",
    "ind = 40\n",
    "\n",
    "input_size = list(full_dataset[0]['surface_data'].size())\n",
    "\n",
    "surface_model = mlp_pure(input_size[0],output_size)\n",
    "\n",
    "surface_model.load_state_dict(torch.load('mlp-models/epoch-{}.pt'.format(ind+1)))\n",
    "\n",
    "surface_model.to(device)\n",
    "\n",
    "# surface_model = surface_model.float()\n",
    "\n",
    "print(\"epoch {} model selected\".format(ind+1))\n",
    "\n",
    "# evaluate model on synthetic set\n",
    "surface_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_cert = []\n",
    "    \n",
    "    surface_data= torch.from_numpy(df_syn).float().to(device)\n",
    "\n",
    "    # y_test.append(label.numpy().list())\n",
    "    # print(label.shape)\n",
    "    # print(images.shape)\n",
    "\n",
    "    output = surface_model(surface_data)\n",
    "\n",
    "    output = sm(output)\n",
    "    # print(output)\n",
    "    \n",
    "    # proxy for uncertainty\n",
    "    # cross entropy penalizes value for not being close to 1 when it's the right category\n",
    "    # saturate leading to overconfidence\n",
    "\n",
    "    max_results = torch.max(output, dim= -1)\n",
    "    predicted = max_results.indices\n",
    "    certainty = max_results.values\n",
    "\n",
    "predicted = predicted.reshape(n_lat, n_lng, n_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab9cf23c-e1e0-448e-a0e9-062ecb39fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('predicted.npy', predicted.cpu())\n",
    "# with open('predicted.npy', 'wb') as f:\n",
    "#     np.save(f, predicted.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d3deb01-0c75-4e46-8acb-78dcbdf67035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 20, 5])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b1da8f0d-d3d2-4bfd-ab92-e0fb060fea5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1760d8ec520>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(predicted[:, :, 0].cpu())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84f41e-6256-4ef3-9664-a4ff26c867b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e80c1-699d-4e37-9153-57629f56614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_2d = torch.sum(predicted, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48bfd7-9ea2-484e-b418-f5db6f7493de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predicted_2d.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72896a9a-2a6c-4e8d-a78b-474415d64182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib.colors import to_hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b09b6a-96e3-4b97-81dd-2101e9710289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmplot as gp\n",
    "\n",
    "mean_lat = df2.latitude.unique().mean()\n",
    "mean_lng = df2.longitude.unique().mean()\n",
    "\n",
    "grid_lng_2d, grid_lat_2d = np.meshgrid(lng_range, lat_range)\n",
    "grid_lng_2d = grid_lng_2d.flatten()\n",
    "grid_lat_2d = grid_lat_2d.flatten()\n",
    "\n",
    "grid_lat_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f4bfd-1b83-4774-8fa0-83f9dc978cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean = cm.get_cmap('inferno',predicted_2d.max())\n",
    "\n",
    "predicted_2d = predicted_2d.flatten().cpu()\n",
    "color_ice = ocean(predicted_2d)\n",
    "len(color_ice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb1487-24e0-446b-b27d-538c47df6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_ice_hex = [None] * len(color_ice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11ca0e-2622-4ecd-b16c-fe88d3e76d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, color in enumerate(color_ice):\n",
    "    color_ice_hex[i] = to_hex(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4699c-01e9-44cb-bfb8-550c21f75ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap = gp.GoogleMapPlotter(mean_lat, mean_lng, 8, apikey = \"AIzaSyBUVgOpJ5OP6L3Rsmtbzy1cTCegpyPAvF4\")\n",
    "# use \"maps javascript api\" credential\n",
    " \n",
    "gmap.scatter(grid_lat_2d, grid_lng_2d, s = 1000, c=color_ice_hex, marker=False) #, color='#3B0B39', size=40, marker=False)\n",
    "# gmap.heatmap(df2.latitude.tolist(), df2.longitude.tolist())  \n",
    "\n",
    "# gmap.scatter(grid_lat_2d[0:2], grid_lng_2d[0:2], s = 500, c=ocean[predicted_2d[0:2]])\n",
    "\n",
    "# Pass the absolute path\n",
    "now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "gmap.draw( \"map_ice{}.html\".format(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1866904e-fdc6-4889-9fb9-4af653fc9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lng_2d.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bdff7-6416-4068-b906-d31f96171af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_2d.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576ea58-15e5-4c76-84a5-b42c4417e5d2",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f050eb-fa12-404a-8d44-bdb3a113e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(testloader))\n",
    "\n",
    "batch['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0ea32-d485-40ab-a987-8a2b125e6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_surfaces = batch['surface_data'].to('cpu')\n",
    "test_images = batch['image'].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c0f33-7b1f-4dcf-a42b-9f546a6e8935",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trainloader))\n",
    "batch['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455979e9-a7f0-4ad3-afd4-c01e4d8dfe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_surfaces = batch['surface_data'].to('cpu')\n",
    "train_images = batch['image'].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e5115-c73b-4e0d-aa42-bc844a104626",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.to('cpu')\n",
    "net_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465cc90-ce11-4650-83c2-3a35701bb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6e263-097d-4b37-b5e1-48d6f194baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_gen = shap.DeepExplainer(gen_model, train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62be1c9-df0d-49a7-bbe4-ba332ecca1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451a262-d427-4593-a542-a4a6d1f5f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9e132-52a6-4609-a0ac-2d28425a2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_fewer = test_images[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd490f-577b-46c2-a217-ba5a3be5754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_fewer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cb467-65ae-447f-8b21-56454d68e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer_gen.shap_values(test_images_fewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892e974-cca1-4cac-b5c4-488f00c3f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n",
    "test_numpy = np.swapaxes(np.swapaxes(test_images_fewer.numpy(), 1, -1), 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a3a54-0c13-4251-a62c-8d20a6d750d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature attributions\n",
    "shap.image_plot(shap_numpy, -test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef10d6-8b40-4218-8b67-3fe9104b4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shap_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bd513-f8bb-4f66-9992-91a8b6e73c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shap_numpy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4da1e-ba9c-44e4-b0da-7eefb889e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shap_numpy[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26caed-21c6-4a48-9c25-ea9e84b6e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shap_numpy[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5792f-bc24-4814-931d-328021fc0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shap_numpy[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be82fb5-c28f-426f-bc2f-d2deb5dae73d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
