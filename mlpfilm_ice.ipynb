{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f98096-bf83-41be-a7d5-72f7794cb779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2675d831-08ab-43f7-b625-cfab6c6128f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_width = 32\n",
    "hidden_nblocks = 4\n",
    "train_max_epoch = 50\n",
    "max_iterations = 10\n",
    "\n",
    "chip_size = 32\n",
    "\n",
    "data_root = \"geomorph_data\"\n",
    "n_channels = len(os.listdir(data_root))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "L2_param = 1e-5\n",
    "\n",
    "label_name = \"visible_ice\"\n",
    "num_classes = 5\n",
    "output_size = 5\n",
    "\n",
    "sm = nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddd8266e-da2e-4871-a8f4-7d37702f30bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0144, -0.9494, -0.7305, -0.9166,  0.2680],\n",
      "        [-1.7730,  0.1862, -0.8611,  2.0290,  1.0670],\n",
      "        [-0.1772,  0.6253, -0.1234, -0.3409, -0.6544]], requires_grad=True)\n",
      "tensor([4, 3, 0])\n",
      "tensor(1.0780, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "print(input)\n",
    "print(target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b59b6ad-d7be-476f-b4bd-c5d6ce57e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6591, 0.4579, 2.2992],\n",
      "        [1.2257, 0.2111, 0.8404]])\n",
      "tensor([[0.1434, 0.1173, 0.7393],\n",
      "        [0.4895, 0.1775, 0.3330]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=-1)\n",
    "# m = nn.Sigmoid()\n",
    "input = torch.randn(2,3)\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629fd05b-9fa8-4502-b8cb-e0892a432591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "        def __init__(self, input_size, output_size = 1, hidden_width = 20, hidden_nblocks = 2):\n",
    "            super(mlp, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.hidden_width = hidden_width\n",
    "            self.hidden_nblocks = hidden_nblocks\n",
    "            \n",
    "            self.fc1 = nn.Linear(self.input_size, self.hidden_width)\n",
    "            self.fc2 = nn.Linear(self.hidden_width,self.hidden_width)\n",
    "            self.fc3 = nn.Linear(self.hidden_width, self.output_size)\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.end= torch.nn.Softmax(dim = -1) ## sigmoid for multi-label, softmax for multi-class (mutually exclusive)\n",
    "            \n",
    "            self.dropout = nn.Dropout(0.25)\n",
    "            \n",
    "        def forward(self, x, film_params):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            \n",
    "            \n",
    "            for i in range(self.hidden_nblocks):\n",
    "                out = self.fc2(out)\n",
    "                \n",
    "                # ------- film layer -----------\n",
    "                start = i * hidden_width * 2\n",
    "                mid = start + hidden_width\n",
    "                end = mid + hidden_width\n",
    "                \n",
    "                gamma = film_params[:, start : mid]\n",
    "                beta = film_params[:, mid : end]\n",
    "                \n",
    "#                 print(out.shape)\n",
    "#                 print(gamma.shape)\n",
    "#                 print(beta.shape)\n",
    "                \n",
    "                out = out * gamma\n",
    "                out += beta\n",
    "                # ------- film layer -----------\n",
    "                # out = self.dropout(out)\n",
    "                out = self.relu(out)\n",
    "            \n",
    "            out = self.fc3(out)\n",
    "            # out = self.end(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905bba3-9233-41cb-8ed6-ffa9087f4e8f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93742035-18c5-42bc-b9e8-86134e61c23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2837\n",
      "566\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TOPSOIL</td>\n",
       "      <td>Organics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>-1.024190</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pure ice</td>\n",
       "      <td>ICE</td>\n",
       "      <td>Ice</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>-0.835900</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>SW-SM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>-0.553466</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GW-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>0.387982</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.16105</td>\n",
       "      <td>-133.08880</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GP-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.437795</td>\n",
       "      <td>1.842620</td>\n",
       "      <td>-0.741756</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude                  time   borehole  depth  frozen  \\\n",
       "0  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.15       0   \n",
       "1  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.85       1   \n",
       "2  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   1.90       1   \n",
       "3  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   5.40       1   \n",
       "4  69.16105 -133.08880  2012-03-21T00:00:00Z  0170-1-12   1.20       1   \n",
       "\n",
       "  cryostructures     visible_ice ASTM_2488    materials  ...  top_of_interval  \\\n",
       "0            NaN             NaN   TOPSOIL     Organics  ...              0.0   \n",
       "1            NaN        Pure ice       ICE          Ice  ...              0.3   \n",
       "2             Nf  No visible ice     SW-SM  Coarse till  ...              1.4   \n",
       "3             Nf  No visible ice     GW-GM  Coarse till  ...              2.4   \n",
       "4             Nf  No visible ice     GP-GM  Coarse till  ...              0.0   \n",
       "\n",
       "   bottom_of_interval  month  year  month_cyclic  lat_norm  lng_norm  \\\n",
       "0                 0.3      3  2012             3  1.439946  1.851455   \n",
       "1                 1.4      3  2012             3  1.439946  1.851455   \n",
       "2                 2.4      3  2012             3  1.439946  1.851455   \n",
       "3                 8.4      3  2012             3  1.439946  1.851455   \n",
       "4                 2.4      3  2012             3  1.437795  1.842620   \n",
       "\n",
       "   depth_norm  year_norm  month_cyclic_norm  \n",
       "0   -1.024190  -1.164992          -1.225295  \n",
       "1   -0.835900  -1.164992          -1.225295  \n",
       "2   -0.553466  -1.164992          -1.225295  \n",
       "3    0.387982  -1.164992          -1.225295  \n",
       "4   -0.741756  -1.164992          -1.225295  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "def normalize(values):\n",
    "    # zero mean, unit variance\n",
    "    value_mean = values.mean()\n",
    "    value_std = values.std()\n",
    "    return (values-values_mean)/values_std\n",
    "\n",
    "def normalize_maxmin(values):\n",
    "    # range from 0 to 1\n",
    "    (values-values.min())/(values.max()-values.min())\n",
    "\n",
    "\n",
    "def get_scaler(data):\n",
    "    scaler = StandardScaler()\n",
    "    print(data)\n",
    "    scaler.fit(data)\n",
    "    return scaler\n",
    "    \n",
    "def preprocess_df(df):\n",
    "    # convert timecodes to year and month columns\n",
    "    datetimes = pd.to_datetime(df['time'])\n",
    "    df['month'] = datetimes.dt.month\n",
    "    df['year'] = datetimes.dt.year\n",
    "\n",
    "    df['month_cyclic'] = 7 - abs(df['month'] - 7)\n",
    "    \n",
    "    data = df[['latitude', 'longitude', 'depth', 'year', 'month_cyclic']]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    df[['lat_norm', 'lng_norm', 'depth_norm', 'year_norm', 'month_cyclic_norm']] = scaler.transform(df[['latitude', 'longitude', 'depth', 'year', 'month_cyclic']])\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "\n",
    "#     df['lat_norm'] = normalize(df['latitude'])\n",
    "#     df['lng_norm'] = normalize(df['longitude'])\n",
    "#     df['depth_norm'],  = normalize(df['depth'])\n",
    "#     df['year_norm'] = normalize(df['year'])\n",
    "#     df['month_cyclic_norm'] = normalize(df['month_cyclic'])\n",
    "\n",
    "df = pd.read_csv('data_stephen_fix_header.csv', header=[0])\n",
    "scaler = preprocess_df(df)\n",
    "    \n",
    "print(df.shape[0])\n",
    "print(df['borehole'].nunique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf25c9ad-dfd3-4503-ba42-c3ad78c5370a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.44755\n",
      "68.36933\n",
      "-132.89346\n",
      "-133.82843\n"
     ]
    }
   ],
   "source": [
    "print(df.latitude.max())\n",
    "print(df.latitude.min())\n",
    "print(df.longitude.max())\n",
    "print(df.longitude.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10b03030-dd00-4063-948c-24493aafdd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>68.38262</td>\n",
       "      <td>-133.71211</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH15</td>\n",
       "      <td>5.45</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf/Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.499167</td>\n",
       "      <td>-0.938559</td>\n",
       "      <td>0.401431</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>68.38262</td>\n",
       "      <td>-133.71211</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH15</td>\n",
       "      <td>8.65</td>\n",
       "      <td>1</td>\n",
       "      <td>Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>9.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.499167</td>\n",
       "      <td>-0.938559</td>\n",
       "      <td>1.262184</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>ORGANICS</td>\n",
       "      <td>Organics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494489</td>\n",
       "      <td>-0.927672</td>\n",
       "      <td>-1.051089</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494489</td>\n",
       "      <td>-0.927672</td>\n",
       "      <td>-0.970393</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1</td>\n",
       "      <td>Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494489</td>\n",
       "      <td>-0.927672</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude  longitude                  time           borehole  depth  \\\n",
       "2832  68.38262 -133.71211  2013-04-27T00:00:00Z  W14103137-S6-BH15   5.45   \n",
       "2833  68.38262 -133.71211  2013-04-27T00:00:00Z  W14103137-S6-BH15   8.65   \n",
       "2834  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   0.05   \n",
       "2835  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   0.35   \n",
       "2836  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   4.85   \n",
       "\n",
       "      frozen cryostructures     visible_ice ASTM_2488 materials  ...  \\\n",
       "2832       1         Nf/Nbn  No visible ice       NaN      Till  ...   \n",
       "2833       1            Nbn  No visible ice       NaN      Till  ...   \n",
       "2834       0            NaN  No visible ice  ORGANICS  Organics  ...   \n",
       "2835       0            NaN  No visible ice       NaN      Till  ...   \n",
       "2836       1            Nbn  No visible ice       NaN      Till  ...   \n",
       "\n",
       "      top_of_interval  bottom_of_interval  month  year  month_cyclic  \\\n",
       "2832              2.7                 8.2      4  2013             4   \n",
       "2833              8.2                 9.1      4  2013             4   \n",
       "2834              0.0                 0.1      4  2013             4   \n",
       "2835              0.1                 0.6      4  2013             4   \n",
       "2836              0.6                 9.1      4  2013             4   \n",
       "\n",
       "      lat_norm  lng_norm  depth_norm  year_norm  month_cyclic_norm  \n",
       "2832 -1.499167 -0.938559    0.401431  -0.319053           0.736422  \n",
       "2833 -1.499167 -0.938559    1.262184  -0.319053           0.736422  \n",
       "2834 -1.494489 -0.927672   -1.051089  -0.319053           0.736422  \n",
       "2835 -1.494489 -0.927672   -0.970393  -0.319053           0.736422  \n",
       "2836 -1.494489 -0.927672    0.240040  -0.319053           0.736422  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['visible_ice'].replace(['None'], 'No visible ice', regex=True, inplace=True)\n",
    "\n",
    "ordered_ice = ['No visible ice', 'Low', \"Medium to high\", 'High', 'Pure ice']\n",
    "df['visible_ice'] = pd.Series(pd.Categorical(df['visible_ice'], categories=ordered_ice, ordered=True))\n",
    "\n",
    "df2 = df.dropna(subset=['visible_ice'])\n",
    "\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e66d4b-f80d-4295-8f97-38f6603c4ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check None values have been replaced\n",
    "len(df2[df2['visible_ice'] == 'None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00107137-4ac0-49a9-bae8-7e5aa1883ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pure ice', 'No visible ice', 'High', 'Medium to high', 'Low']\n",
      "Categories (5, object): ['No visible ice' < 'Low' < 'Medium to high' < 'High' < 'Pure ice']\n",
      "[4 0 3 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-b83c4bbc5a30>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['visible_ice_code'] =  df2['visible_ice'].cat.codes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1             Pure ice\n",
       "2       No visible ice\n",
       "3       No visible ice\n",
       "4       No visible ice\n",
       "5       No visible ice\n",
       "             ...      \n",
       "2832    No visible ice\n",
       "2833    No visible ice\n",
       "2834    No visible ice\n",
       "2835    No visible ice\n",
       "2836    No visible ice\n",
       "Name: visible_ice, Length: 2752, dtype: category\n",
       "Categories (5, object): ['No visible ice' < 'Low' < 'Medium to high' < 'High' < 'Pure ice']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['visible_ice_code'] =  df2['visible_ice'].cat.codes\n",
    "print(df2['visible_ice'].unique())\n",
    "print(df2['visible_ice_code'].unique())\n",
    "df2['visible_ice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c3d6e0-5ed8-4d93-ad53-e6a84b8a007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visible_ice = pd.get_dummies(df2.visible_ice)\n",
    "# bin_visible_ice = (~visible_ice['No visible ice'].astype('bool')).astype('int')\n",
    "# bin_visible_ice.value_counts()\n",
    "\n",
    "# df2['visible_ice'] = bin_visible_ice\n",
    "\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1837f5df-ee84-4d0d-9942-14b948a37934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.dropna(subset=['materials'])\n",
    "df3['materials'].replace(['ICE'], 'Ice', regex=True, inplace=True)\n",
    "df3['materials'].replace(['ice'], 'Ice', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dba8d5fb-1a64-4fea-a69a-20b744e097a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_materials = pd.get_dummies(df3.materials)\n",
    "df3['material_ice'] = dm_materials['Ice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75c3c456-b700-46cd-9a6d-52872ba14aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Geo90Dataset(Dataset):\n",
    "    def __init__(self, data_root, df, base_lat, base_lng, chip_size=32):\n",
    "        \n",
    "        self.base_lat = base_lat\n",
    "        self.base_lng = base_lng\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        self.chip_size = chip_size\n",
    "        \n",
    "        self.trans = transforms.ToTensor()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.preloaded = torch.ones(self.n_channels, 6000, 6000)\n",
    "        \n",
    "        for i, file in enumerate(os.listdir(data_root)):\n",
    "            # name = file.split('_')[0]\n",
    "            # print(name)\n",
    "            self.preloaded[i] = self.trans(Image.open(data_root + os.path.sep + file))\n",
    "        \n",
    "        \n",
    "        print('Dataset initialized')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        bh_id = row.at['borehole']\n",
    "        lat = row.at['latitude']\n",
    "        lng = row.at['longitude']\n",
    "        \n",
    "\n",
    "        pixel_len = 5/6000\n",
    "        \n",
    "\n",
    "        lat_index_start = np.round((self.base_lat - lat) / pixel_len - self.chip_size/2).astype(int)\n",
    "        lat_index_end = lat_index_start + self.chip_size\n",
    "        \n",
    "        lng_index_start = np.round((lng - self.base_lng) / pixel_len - self.chip_size/2).astype(int)\n",
    "        lng_index_end = lng_index_start + self.chip_size\n",
    "        \n",
    "        image = self.preloaded[:, lat_index_start:lat_index_end,lng_index_start:lng_index_end]\n",
    "        \n",
    "        \n",
    "        # surface = torch.tensor(row.filter(['depth'])).float()\n",
    "        surface = torch.tensor(row.filter(['depth_norm', 'month_cyclic_norm', 'lat_norm', 'lng_norm', 'year_norm'])).float()\n",
    "        \n",
    "        frozen = torch.tensor(row.at['frozen']).float()\n",
    "        \n",
    "        # visible_ice = torch.tensor(row.at['visible_ice']).float()\n",
    "        visible_ice = torch.tensor(row.at['visible_ice_code']).long()\n",
    "        \n",
    "        # material_ice = torch.tensor(row.at['material_ice']).float()\n",
    "        \n",
    "        return {'image': image, 'surface_data': surface, 'frozen': frozen,  'visible_ice': visible_ice} #'material_ice': material_ice}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b0df9-d51d-4bb3-9658-7a49e95bc6a6",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f46b0-7537-4ceb-8f96-263c007e97fd",
   "metadata": {},
   "source": [
    "## FiLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "214c4911-bcfb-43a2-b41b-b01949690b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_film_params = hidden_width * hidden_nblocks * 2\n",
    "\n",
    "# generator = resnet18(n_channels, n_film_params)\n",
    "\n",
    "def train_model(trainloader, testloader, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    # loss: binary cross entropy\n",
    "\n",
    "    generator = models.resnet18()\n",
    "    generator.fc = nn.Linear(512, n_film_params)\n",
    "    generator.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    gen_model = generator\n",
    "\n",
    "    # print(gen_model)\n",
    "\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    net_model = mlp(input_size[0],output_size, hidden_width, hidden_nblocks).to(device)\n",
    "    \n",
    "    gen_optimizer = torch.optim.Adam(gen_model.parameters(), weight_decay = L2_param)\n",
    "    net_optimizer = torch.optim.Adam(net_model.parameters(), weight_decay = L2_param)\n",
    "    \n",
    "    gen_model.to(device)\n",
    "    net_model.to(device)\n",
    "\n",
    "    # --------- check back propagation ----------- -\n",
    "    # net_model.fc1.weight.register_hook(lambda x: print('grad accumulated in mlp fc1'))\n",
    "    # gen_first_layer = gen_model.encoder.blocks[0].blocks[0].blocks[0].conv\n",
    "    # gen_first_layer.weight.register_hook(lambda x: print('grad accumulated in resnet first layer'))\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        # ------------ train -----------------\n",
    "        gen_model.train()\n",
    "        net_model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            \n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "\n",
    "            gen_optimizer.zero_grad()\n",
    "            net_optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            gen_optimizer.step()\n",
    "            net_optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        gen_model.eval()\n",
    "        net_model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # TODO: exammine film_params gradients / readup pytorch\n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "\n",
    "        torch.save(gen_model.state_dict(), os.path.join('mlp-resnet-models/', 'gen-epoch-{}.pt'.format(epoch+1)))\n",
    "        torch.save(net_model.state_dict(), os.path.join('mlp-resnet-models/', 'net-epoch-{}.pt'.format(epoch+1)))\n",
    "\n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "## Test model\n",
    "\n",
    "def test_model(epoch_loss, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    generator = models.resnet18()\n",
    "    generator.fc = nn.Linear(512, n_film_params)\n",
    "    generator.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    gen_model = generator\n",
    "\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    net_model = mlp(input_size[0],1, hidden_width, hidden_nblocks)\n",
    "\n",
    "    gen_model.load_state_dict(torch.load('mlp-resnet-models/gen-epoch-{}.pt'.format(ind+1)))\n",
    "    net_model.load_state_dict(torch.load('mlp-resnet-models/net-epoch-{}.pt'.format(ind+1)))\n",
    "    \n",
    "    gen_model.to(device)\n",
    "    net_model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    gen_model.eval()\n",
    "    net_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "\n",
    "            predicted = torch.round(predicted)\n",
    "            # print(predicted.shape)\n",
    "            lb = labels.tolist()\n",
    "            pr = predicted.tolist()\n",
    "            y_test.extend(lb)\n",
    "            y_pred.extend(pr)\n",
    "    \n",
    "    arr_accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "    return arr_accuracy, scores\n",
    "\n",
    "\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "#     print(classification_report(y_test,y_pred))\n",
    "#     print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "## Pure MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8b5b0-1039-4c76-93e4-0f90b60d87be",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43c02928-f9c5-4b2a-8ef7-d8a2de4a4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_pure(nn.Module):\n",
    "        def __init__(self, input_size, output_size):\n",
    "            super(mlp_pure, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.hidden_size = hidden_width\n",
    "            self.hidden_nblocks = hidden_nblocks\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size,self.hidden_size)\n",
    "            self.fc3 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.end = torch.nn.Softmax(dim=-1) ## sigmoid for multi-label, softmax for multi-class (mutually exclusive)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            # print(out.shape)\n",
    "            \n",
    "            for i in range(self.hidden_nblocks):\n",
    "                out = self.fc2(out)\n",
    "                out = self.relu(out)\n",
    "            \n",
    "            out = self.fc3(out)\n",
    "            # out = self.end(out)\n",
    "            return out\n",
    "\n",
    "def train_mlp(trainloader, testloader, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    surface_model = mlp_pure(input_size[0],output_size)\n",
    "    \n",
    "    surface_model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(surface_model.parameters(), weight_decay = L2_param)\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        surface_model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            predicted = surface_model(surface_data)\n",
    "            \n",
    "            # print(predicted.shape)\n",
    "            \n",
    "            # squeeze: return tensor with all dimensions of size 1 removed\n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        surface_model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            predicted = surface_model(surface_data)\n",
    "            \n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "\n",
    "        torch.save(surface_model.state_dict(), os.path.join('mlp-models/', 'epoch-{}.pt'.format(epoch+1)))\n",
    "    \n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "        \n",
    "    return epoch_loss\n",
    "        \n",
    "def test_mlp(epoch_loss, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    \n",
    "    surface_model = mlp_pure(input_size[0],output_size)\n",
    "    surface_model.load_state_dict(torch.load('mlp-models/epoch-{}.pt'.format(ind+1)))\n",
    "    \n",
    "    surface_model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    surface_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        y_cert = []\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            output = surface_model(surface_data)\n",
    "            \n",
    "            output = sm(output)\n",
    "            # print(output)\n",
    "            \n",
    "            \n",
    "            max_results = torch.max(output, dim= -1)\n",
    "            predicted = max_results.indices\n",
    "            certainty = max_results.values\n",
    "            #predicted = torch.round(output)\n",
    "            # print(predicted.shape)\n",
    "            \n",
    "            y_test.extend(labels.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            y_cert.extend(certainty.tolist())\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "#     print(y_test)\n",
    "#     print(y_pred)\n",
    "    with open(\"mlp-certainty/iteration_{}.txt\".format(it), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(y_cert, fp)\n",
    "    #with open(\"test.txt\", \"rb\") as fp:   # Unpickling\n",
    "        #b = pickle.load(fp)\n",
    "    arr_accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "    return arr_accuracy, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cfb6b-c5ca-4fdd-81e9-571685a86522",
   "metadata": {},
   "source": [
    "## Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51bacf58-7320-49da-8f08-aa180ed67446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized\n"
     ]
    }
   ],
   "source": [
    "base_lat = 70\n",
    "base_lng = -135\n",
    "\n",
    "loaded_dataset = Geo90Dataset(data_root, df2, base_lat, base_lng, chip_size)\n",
    "\n",
    "# image = full_dataset[0]['image']\n",
    "# n_channels = list(image.shape)[0]\n",
    "\n",
    "# for data in full_dataset:\n",
    "#     image = data['image']\n",
    "#     for i in range(n_channels):\n",
    "#         channel = image[i]\n",
    "#         ind = (channel == -9999)\n",
    "# #         mean_val = torch.mean(channel[~ind])\n",
    "#         channel[ind] = 0\n",
    "#         data['image'][i] = channel\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44629f3-5888-4eb0-8f54-cfd63f455c54",
   "metadata": {},
   "source": [
    "### Discard samples with invalid values in image chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc861435-97ab-4095-9760-4b797477daa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752\n",
      "2717\n"
     ]
    }
   ],
   "source": [
    "valid_ind = []\n",
    "for i, data in enumerate(loaded_dataset):\n",
    "    image = data['image']\n",
    "    ind  = (image == -9999)\n",
    "    if ~torch.any(ind):\n",
    "        valid_ind.append(i)\n",
    "\n",
    "full_dataset = torch.utils.data.Subset(loaded_dataset, valid_ind)\n",
    "\n",
    "\n",
    "print(len(loaded_dataset))\n",
    "print(len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba299154-57ac-4e30-bf2e-74e41e3fc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# batchsize can cause error when last leftover batchsize is 1, batchnorm cannot function on 1 sample data\n",
    "batchsize = 20\n",
    "while(train_size % batchsize == 1):\n",
    "    batchsize+=1\n",
    "print(batchsize)\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(full_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb6971-4f9e-409e-b8ee-9ebe27a3cfae",
   "metadata": {},
   "source": [
    "## Scale of image chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e37318c4-ff02-48e0-aa8c-c5a708eb0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = full_dataset[0]['image']\n",
    "# n_samples = len(train_data)\n",
    "# n_channels = list(image.shape)[0]\n",
    "\n",
    "# scalers = []\n",
    "# for i in range(n_channels):\n",
    "#     print(i)\n",
    "#     scaler = StandardScaler()\n",
    "#     X = torch.empty((n_samples, chip_size, chip_size))\n",
    "    \n",
    "#     for j, data in enumerate(train_data):\n",
    "#         #print(data['image'][i].shape)\n",
    "#         # print(X[j].shape)\n",
    "#         X[j] = data['image'][i]\n",
    "#     X = torch.reshape(X, (-1,1))\n",
    "#     #print(X)\n",
    "#     # break;\n",
    "#     scaler.fit(X)\n",
    "#     scalers.append(scaler)\n",
    "    \n",
    "#     def scale_data(subset):\n",
    "#         for data in subset:\n",
    "#             X = data['image'][i]\n",
    "#             X_flat = torch.reshape(X, (-1,1))\n",
    "            \n",
    "#             X_trans = scaler.transform(X_flat)\n",
    "#             data['image'][i] = torch.reshape(torch.Tensor(X_trans), (chip_size, chip_size))\n",
    "    \n",
    "#     scale_data(train_data)\n",
    "#     scale_data(test_data)\n",
    "#     print(\"Channel {} scaled.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f7a6d03-a8b0-4c0f-b562-415f30deb0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: running loss: 1.47335, validation loss: 1.36664\n",
      "epoch  2: running loss: 1.38815, validation loss: 1.35103\n",
      "epoch  3: running loss: 1.37903, validation loss: 1.33427\n",
      "epoch  4: running loss: 1.37241, validation loss: 1.33023\n",
      "epoch  5: running loss: 1.36518, validation loss: 1.34114\n",
      "epoch  6: running loss: 1.35300, validation loss: 1.33881\n",
      "epoch  7: running loss: 1.34913, validation loss: 1.32167\n",
      "epoch  8: running loss: 1.33633, validation loss: 1.31028\n",
      "epoch  9: running loss: 1.32868, validation loss: 1.29821\n",
      "epoch 10: running loss: 1.32486, validation loss: 1.33771\n",
      "epoch 11: running loss: 1.30310, validation loss: 1.32336\n",
      "epoch 12: running loss: 1.29723, validation loss: 1.31441\n",
      "epoch 13: running loss: 1.28795, validation loss: 1.30517\n",
      "epoch 14: running loss: 1.28120, validation loss: 1.30562\n",
      "epoch 15: running loss: 1.27372, validation loss: 1.30168\n",
      "epoch 16: running loss: 1.27064, validation loss: 1.31804\n",
      "epoch 17: running loss: 1.26231, validation loss: 1.30066\n",
      "epoch 18: running loss: 1.26195, validation loss: 1.31795\n",
      "epoch 19: running loss: 1.25812, validation loss: 1.26974\n",
      "epoch 20: running loss: 1.25163, validation loss: 1.29979\n",
      "epoch 21: running loss: 1.24277, validation loss: 1.27024\n",
      "epoch 22: running loss: 1.24909, validation loss: 1.29232\n",
      "epoch 23: running loss: 1.24344, validation loss: 1.27988\n",
      "epoch 24: running loss: 1.23925, validation loss: 1.28671\n",
      "epoch 25: running loss: 1.23753, validation loss: 1.28445\n",
      "epoch 26: running loss: 1.23461, validation loss: 1.28699\n",
      "epoch 27: running loss: 1.22995, validation loss: 1.27512\n",
      "epoch 28: running loss: 1.22433, validation loss: 1.29243\n",
      "epoch 29: running loss: 1.22957, validation loss: 1.26601\n",
      "epoch 30: running loss: 1.22342, validation loss: 1.26117\n",
      "epoch 31: running loss: 1.22459, validation loss: 1.26942\n",
      "epoch 32: running loss: 1.22522, validation loss: 1.26428\n",
      "epoch 33: running loss: 1.21579, validation loss: 1.26820\n",
      "epoch 34: running loss: 1.22105, validation loss: 1.30311\n",
      "epoch 35: running loss: 1.21468, validation loss: 1.27072\n",
      "epoch 36: running loss: 1.21499, validation loss: 1.26965\n",
      "epoch 37: running loss: 1.21686, validation loss: 1.26405\n",
      "epoch 38: running loss: 1.20764, validation loss: 1.26354\n",
      "epoch 39: running loss: 1.20983, validation loss: 1.27173\n",
      "epoch 40: running loss: 1.20673, validation loss: 1.30360\n",
      "epoch 41: running loss: 1.21488, validation loss: 1.26886\n",
      "epoch 42: running loss: 1.20704, validation loss: 1.29007\n",
      "epoch 43: running loss: 1.20783, validation loss: 1.26300\n",
      "epoch 44: running loss: 1.20412, validation loss: 1.28479\n",
      "epoch 45: running loss: 1.20611, validation loss: 1.25042\n",
      "epoch 46: running loss: 1.20149, validation loss: 1.24634\n",
      "epoch 47: running loss: 1.20044, validation loss: 1.26983\n",
      "epoch 48: running loss: 1.19794, validation loss: 1.28688\n",
      "epoch 49: running loss: 1.19922, validation loss: 1.26935\n",
      "epoch 50: running loss: 1.19242, validation loss: 1.27832\n",
      "Finished Training\n",
      "epoch 46 model selected\n",
      "iteration 1 elapsed time: 91.24360704421997, accuracy : 0.44485294117647056\n",
      "epoch  1: running loss: 1.49189, validation loss: 1.35657\n",
      "epoch  2: running loss: 1.39634, validation loss: 1.34609\n",
      "epoch  3: running loss: 1.38298, validation loss: 1.33782\n",
      "epoch  4: running loss: 1.37136, validation loss: 1.34800\n",
      "epoch  5: running loss: 1.35682, validation loss: 1.33671\n",
      "epoch  6: running loss: 1.34584, validation loss: 1.33110\n",
      "epoch  7: running loss: 1.33892, validation loss: 1.32487\n",
      "epoch  8: running loss: 1.32033, validation loss: 1.31548\n",
      "epoch  9: running loss: 1.31446, validation loss: 1.31723\n",
      "epoch 10: running loss: 1.30353, validation loss: 1.30421\n",
      "epoch 11: running loss: 1.29430, validation loss: 1.30685\n",
      "epoch 12: running loss: 1.29135, validation loss: 1.30219\n",
      "epoch 13: running loss: 1.28068, validation loss: 1.28396\n",
      "epoch 14: running loss: 1.28019, validation loss: 1.31025\n",
      "epoch 15: running loss: 1.27115, validation loss: 1.29343\n",
      "epoch 16: running loss: 1.26689, validation loss: 1.28051\n",
      "epoch 17: running loss: 1.26486, validation loss: 1.27459\n",
      "epoch 18: running loss: 1.25985, validation loss: 1.27382\n",
      "epoch 19: running loss: 1.26007, validation loss: 1.28102\n",
      "epoch 20: running loss: 1.24999, validation loss: 1.30087\n",
      "epoch 21: running loss: 1.25300, validation loss: 1.30895\n",
      "epoch 22: running loss: 1.24709, validation loss: 1.28382\n",
      "epoch 23: running loss: 1.24467, validation loss: 1.30009\n",
      "epoch 24: running loss: 1.23731, validation loss: 1.27227\n",
      "epoch 25: running loss: 1.23698, validation loss: 1.27669\n",
      "epoch 26: running loss: 1.23557, validation loss: 1.29220\n",
      "epoch 27: running loss: 1.23306, validation loss: 1.29321\n",
      "epoch 28: running loss: 1.23066, validation loss: 1.27743\n",
      "epoch 29: running loss: 1.22691, validation loss: 1.24813\n",
      "epoch 30: running loss: 1.22249, validation loss: 1.28005\n",
      "epoch 31: running loss: 1.21999, validation loss: 1.25930\n",
      "epoch 32: running loss: 1.21925, validation loss: 1.26290\n",
      "epoch 33: running loss: 1.21543, validation loss: 1.28168\n",
      "epoch 34: running loss: 1.21361, validation loss: 1.27356\n",
      "epoch 35: running loss: 1.21588, validation loss: 1.26761\n",
      "epoch 36: running loss: 1.20724, validation loss: 1.25683\n",
      "epoch 37: running loss: 1.21155, validation loss: 1.25923\n",
      "epoch 38: running loss: 1.20433, validation loss: 1.24802\n",
      "epoch 39: running loss: 1.20681, validation loss: 1.27353\n",
      "epoch 40: running loss: 1.20457, validation loss: 1.26399\n",
      "epoch 41: running loss: 1.19856, validation loss: 1.27139\n",
      "epoch 42: running loss: 1.19749, validation loss: 1.26037\n",
      "epoch 43: running loss: 1.19828, validation loss: 1.25783\n",
      "epoch 44: running loss: 1.19192, validation loss: 1.26356\n",
      "epoch 45: running loss: 1.19627, validation loss: 1.27660\n",
      "epoch 46: running loss: 1.19320, validation loss: 1.25280\n",
      "epoch 47: running loss: 1.19020, validation loss: 1.25286\n",
      "epoch 48: running loss: 1.19244, validation loss: 1.28161\n",
      "epoch 49: running loss: 1.18323, validation loss: 1.29145\n",
      "epoch 50: running loss: 1.18660, validation loss: 1.25011\n",
      "Finished Training\n",
      "epoch 38 model selected\n",
      "iteration 2 elapsed time: 91.76295757293701, accuracy : 0.4485294117647059\n",
      "epoch  1: running loss: 1.47368, validation loss: 1.35738\n",
      "epoch  2: running loss: 1.39064, validation loss: 1.34028\n",
      "epoch  3: running loss: 1.38632, validation loss: 1.34511\n",
      "epoch  4: running loss: 1.37521, validation loss: 1.32696\n",
      "epoch  5: running loss: 1.35832, validation loss: 1.32229\n",
      "epoch  6: running loss: 1.34329, validation loss: 1.32067\n",
      "epoch  7: running loss: 1.32569, validation loss: 1.30577\n",
      "epoch  8: running loss: 1.31504, validation loss: 1.29958\n",
      "epoch  9: running loss: 1.30571, validation loss: 1.30557\n",
      "epoch 10: running loss: 1.30315, validation loss: 1.29781\n",
      "epoch 11: running loss: 1.29570, validation loss: 1.33945\n",
      "epoch 12: running loss: 1.29450, validation loss: 1.30277\n",
      "epoch 13: running loss: 1.28323, validation loss: 1.31897\n",
      "epoch 14: running loss: 1.28616, validation loss: 1.30406\n",
      "epoch 15: running loss: 1.27984, validation loss: 1.30295\n",
      "epoch 16: running loss: 1.27579, validation loss: 1.29341\n",
      "epoch 17: running loss: 1.27364, validation loss: 1.28896\n",
      "epoch 18: running loss: 1.26682, validation loss: 1.28450\n",
      "epoch 19: running loss: 1.26524, validation loss: 1.28929\n",
      "epoch 20: running loss: 1.25744, validation loss: 1.27659\n",
      "epoch 21: running loss: 1.25420, validation loss: 1.26952\n",
      "epoch 22: running loss: 1.25379, validation loss: 1.27673\n",
      "epoch 23: running loss: 1.24873, validation loss: 1.27611\n",
      "epoch 24: running loss: 1.24749, validation loss: 1.27240\n",
      "epoch 25: running loss: 1.24209, validation loss: 1.27287\n",
      "epoch 26: running loss: 1.24052, validation loss: 1.26848\n",
      "epoch 27: running loss: 1.23537, validation loss: 1.24839\n",
      "epoch 28: running loss: 1.23496, validation loss: 1.25191\n",
      "epoch 29: running loss: 1.23391, validation loss: 1.27642\n",
      "epoch 30: running loss: 1.23021, validation loss: 1.27889\n",
      "epoch 31: running loss: 1.22283, validation loss: 1.27228\n",
      "epoch 32: running loss: 1.22130, validation loss: 1.27152\n",
      "epoch 33: running loss: 1.22245, validation loss: 1.29073\n",
      "epoch 34: running loss: 1.21660, validation loss: 1.25029\n",
      "epoch 35: running loss: 1.21780, validation loss: 1.25414\n",
      "epoch 36: running loss: 1.21091, validation loss: 1.24102\n",
      "epoch 37: running loss: 1.21343, validation loss: 1.27725\n",
      "epoch 38: running loss: 1.21129, validation loss: 1.29345\n",
      "epoch 39: running loss: 1.21313, validation loss: 1.29840\n",
      "epoch 40: running loss: 1.20683, validation loss: 1.24383\n",
      "epoch 41: running loss: 1.20553, validation loss: 1.24139\n",
      "epoch 42: running loss: 1.20412, validation loss: 1.24793\n",
      "epoch 43: running loss: 1.20154, validation loss: 1.30122\n",
      "epoch 44: running loss: 1.20415, validation loss: 1.27488\n",
      "epoch 45: running loss: 1.21511, validation loss: 1.26910\n",
      "epoch 46: running loss: 1.19820, validation loss: 1.28442\n",
      "epoch 47: running loss: 1.20431, validation loss: 1.25659\n",
      "epoch 48: running loss: 1.19720, validation loss: 1.27772\n",
      "epoch 49: running loss: 1.19463, validation loss: 1.25758\n",
      "epoch 50: running loss: 1.20367, validation loss: 1.24448\n",
      "Finished Training\n",
      "epoch 36 model selected\n",
      "iteration 3 elapsed time: 91.90275120735168, accuracy : 0.4540441176470588\n",
      "epoch  1: running loss: 1.52446, validation loss: 1.35343\n",
      "epoch  2: running loss: 1.38178, validation loss: 1.33475\n",
      "epoch  3: running loss: 1.36676, validation loss: 1.32835\n",
      "epoch  4: running loss: 1.34915, validation loss: 1.32492\n",
      "epoch  5: running loss: 1.33873, validation loss: 1.34292\n",
      "epoch  6: running loss: 1.33110, validation loss: 1.32022\n",
      "epoch  7: running loss: 1.32783, validation loss: 1.33447\n",
      "epoch  8: running loss: 1.32213, validation loss: 1.31310\n",
      "epoch  9: running loss: 1.31918, validation loss: 1.30820\n",
      "epoch 10: running loss: 1.31447, validation loss: 1.30942\n",
      "epoch 11: running loss: 1.31051, validation loss: 1.28508\n",
      "epoch 12: running loss: 1.30174, validation loss: 1.30960\n",
      "epoch 13: running loss: 1.29913, validation loss: 1.28958\n",
      "epoch 14: running loss: 1.29092, validation loss: 1.29161\n",
      "epoch 15: running loss: 1.28693, validation loss: 1.27991\n",
      "epoch 16: running loss: 1.28261, validation loss: 1.29605\n",
      "epoch 17: running loss: 1.27491, validation loss: 1.27708\n",
      "epoch 18: running loss: 1.26962, validation loss: 1.26832\n",
      "epoch 19: running loss: 1.26548, validation loss: 1.28104\n",
      "epoch 20: running loss: 1.26024, validation loss: 1.27543\n",
      "epoch 21: running loss: 1.25618, validation loss: 1.27414\n",
      "epoch 22: running loss: 1.25482, validation loss: 1.25938\n",
      "epoch 23: running loss: 1.25818, validation loss: 1.27315\n",
      "epoch 24: running loss: 1.24147, validation loss: 1.26250\n",
      "epoch 25: running loss: 1.23590, validation loss: 1.26057\n",
      "epoch 26: running loss: 1.23670, validation loss: 1.28422\n",
      "epoch 27: running loss: 1.23222, validation loss: 1.24879\n",
      "epoch 28: running loss: 1.23085, validation loss: 1.24142\n",
      "epoch 29: running loss: 1.22484, validation loss: 1.26016\n",
      "epoch 30: running loss: 1.21899, validation loss: 1.25588\n",
      "epoch 31: running loss: 1.22271, validation loss: 1.27827\n",
      "epoch 32: running loss: 1.21898, validation loss: 1.25758\n",
      "epoch 33: running loss: 1.21371, validation loss: 1.27791\n",
      "epoch 34: running loss: 1.21250, validation loss: 1.26403\n",
      "epoch 35: running loss: 1.21016, validation loss: 1.25566\n",
      "epoch 36: running loss: 1.20873, validation loss: 1.23402\n",
      "epoch 37: running loss: 1.20870, validation loss: 1.25685\n",
      "epoch 38: running loss: 1.20420, validation loss: 1.23546\n",
      "epoch 39: running loss: 1.20527, validation loss: 1.24919\n",
      "epoch 40: running loss: 1.20179, validation loss: 1.25642\n",
      "epoch 41: running loss: 1.19914, validation loss: 1.25439\n",
      "epoch 42: running loss: 1.19779, validation loss: 1.24439\n",
      "epoch 43: running loss: 1.20095, validation loss: 1.24785\n",
      "epoch 44: running loss: 1.19675, validation loss: 1.24127\n",
      "epoch 45: running loss: 1.19227, validation loss: 1.25589\n",
      "epoch 46: running loss: 1.19733, validation loss: 1.24938\n",
      "epoch 47: running loss: 1.19820, validation loss: 1.24301\n",
      "epoch 48: running loss: 1.19196, validation loss: 1.23041\n",
      "epoch 49: running loss: 1.19127, validation loss: 1.24750\n",
      "epoch 50: running loss: 1.18723, validation loss: 1.24347\n",
      "Finished Training\n",
      "epoch 48 model selected\n",
      "iteration 4 elapsed time: 90.87845039367676, accuracy : 0.4375\n",
      "epoch  1: running loss: 1.48352, validation loss: 1.34599\n",
      "epoch  2: running loss: 1.39292, validation loss: 1.34510\n",
      "epoch  3: running loss: 1.37921, validation loss: 1.34215\n",
      "epoch  4: running loss: 1.36689, validation loss: 1.34303\n",
      "epoch  5: running loss: 1.35146, validation loss: 1.33012\n",
      "epoch  6: running loss: 1.33986, validation loss: 1.31587\n",
      "epoch  7: running loss: 1.32648, validation loss: 1.29577\n",
      "epoch  8: running loss: 1.31732, validation loss: 1.29045\n",
      "epoch  9: running loss: 1.31201, validation loss: 1.28929\n",
      "epoch 10: running loss: 1.30269, validation loss: 1.27579\n",
      "epoch 11: running loss: 1.29627, validation loss: 1.32637\n",
      "epoch 12: running loss: 1.30166, validation loss: 1.29151\n",
      "epoch 13: running loss: 1.28518, validation loss: 1.32563\n",
      "epoch 14: running loss: 1.28195, validation loss: 1.28053\n",
      "epoch 15: running loss: 1.27664, validation loss: 1.32775\n",
      "epoch 16: running loss: 1.26892, validation loss: 1.28435\n",
      "epoch 17: running loss: 1.27057, validation loss: 1.28480\n",
      "epoch 18: running loss: 1.26161, validation loss: 1.28713\n",
      "epoch 19: running loss: 1.26586, validation loss: 1.29591\n",
      "epoch 20: running loss: 1.25856, validation loss: 1.31108\n",
      "epoch 21: running loss: 1.26225, validation loss: 1.28784\n",
      "epoch 22: running loss: 1.25242, validation loss: 1.31321\n",
      "epoch 23: running loss: 1.24508, validation loss: 1.31126\n",
      "epoch 24: running loss: 1.25029, validation loss: 1.29714\n",
      "epoch 25: running loss: 1.24321, validation loss: 1.29417\n",
      "epoch 26: running loss: 1.24618, validation loss: 1.29896\n",
      "epoch 27: running loss: 1.23646, validation loss: 1.33079\n",
      "epoch 28: running loss: 1.24388, validation loss: 1.31642\n",
      "epoch 29: running loss: 1.23408, validation loss: 1.29772\n",
      "epoch 30: running loss: 1.23417, validation loss: 1.29689\n",
      "epoch 31: running loss: 1.23203, validation loss: 1.28642\n",
      "epoch 32: running loss: 1.22991, validation loss: 1.30627\n",
      "epoch 33: running loss: 1.22933, validation loss: 1.30176\n",
      "epoch 34: running loss: 1.22470, validation loss: 1.27983\n",
      "epoch 35: running loss: 1.22502, validation loss: 1.33089\n",
      "epoch 36: running loss: 1.22228, validation loss: 1.28479\n",
      "epoch 37: running loss: 1.22036, validation loss: 1.29836\n",
      "epoch 38: running loss: 1.22306, validation loss: 1.28351\n",
      "epoch 39: running loss: 1.21898, validation loss: 1.29396\n",
      "epoch 40: running loss: 1.21672, validation loss: 1.28641\n",
      "epoch 41: running loss: 1.21921, validation loss: 1.27109\n",
      "epoch 42: running loss: 1.21049, validation loss: 1.26670\n",
      "epoch 43: running loss: 1.21246, validation loss: 1.27144\n",
      "epoch 44: running loss: 1.21720, validation loss: 1.26402\n",
      "epoch 45: running loss: 1.21395, validation loss: 1.26199\n",
      "epoch 46: running loss: 1.20938, validation loss: 1.27182\n",
      "epoch 47: running loss: 1.20167, validation loss: 1.27095\n",
      "epoch 48: running loss: 1.20488, validation loss: 1.29468\n",
      "epoch 49: running loss: 1.19984, validation loss: 1.32489\n",
      "epoch 50: running loss: 1.20929, validation loss: 1.28960\n",
      "Finished Training\n",
      "epoch 45 model selected\n",
      "iteration 5 elapsed time: 91.05543041229248, accuracy : 0.4227941176470588\n",
      "epoch  1: running loss: 1.48592, validation loss: 1.36327\n",
      "epoch  2: running loss: 1.40038, validation loss: 1.35365\n",
      "epoch  3: running loss: 1.39303, validation loss: 1.35632\n",
      "epoch  4: running loss: 1.38185, validation loss: 1.34078\n",
      "epoch  5: running loss: 1.37173, validation loss: 1.33401\n",
      "epoch  6: running loss: 1.35677, validation loss: 1.32398\n",
      "epoch  7: running loss: 1.34046, validation loss: 1.31180\n",
      "epoch  8: running loss: 1.32505, validation loss: 1.29456\n",
      "epoch  9: running loss: 1.31632, validation loss: 1.28886\n",
      "epoch 10: running loss: 1.30184, validation loss: 1.30969\n",
      "epoch 11: running loss: 1.29167, validation loss: 1.27946\n",
      "epoch 12: running loss: 1.28105, validation loss: 1.30823\n",
      "epoch 13: running loss: 1.27457, validation loss: 1.26619\n",
      "epoch 14: running loss: 1.26193, validation loss: 1.29537\n",
      "epoch 15: running loss: 1.25994, validation loss: 1.27686\n",
      "epoch 16: running loss: 1.25070, validation loss: 1.25938\n",
      "epoch 17: running loss: 1.24915, validation loss: 1.27617\n",
      "epoch 18: running loss: 1.24524, validation loss: 1.26488\n",
      "epoch 19: running loss: 1.23413, validation loss: 1.25674\n",
      "epoch 20: running loss: 1.23296, validation loss: 1.26591\n",
      "epoch 21: running loss: 1.23309, validation loss: 1.27496\n",
      "epoch 22: running loss: 1.22454, validation loss: 1.23480\n",
      "epoch 23: running loss: 1.22266, validation loss: 1.27021\n",
      "epoch 24: running loss: 1.22047, validation loss: 1.25960\n",
      "epoch 25: running loss: 1.21733, validation loss: 1.25583\n",
      "epoch 26: running loss: 1.21207, validation loss: 1.26998\n",
      "epoch 27: running loss: 1.21169, validation loss: 1.25847\n",
      "epoch 28: running loss: 1.20997, validation loss: 1.26343\n",
      "epoch 29: running loss: 1.20866, validation loss: 1.26555\n",
      "epoch 30: running loss: 1.20712, validation loss: 1.26081\n",
      "epoch 31: running loss: 1.20769, validation loss: 1.26983\n",
      "epoch 32: running loss: 1.20212, validation loss: 1.28603\n",
      "epoch 33: running loss: 1.20320, validation loss: 1.28933\n",
      "epoch 34: running loss: 1.19903, validation loss: 1.26160\n",
      "epoch 35: running loss: 1.19994, validation loss: 1.26719\n",
      "epoch 36: running loss: 1.19392, validation loss: 1.27775\n",
      "epoch 37: running loss: 1.19419, validation loss: 1.25184\n",
      "epoch 38: running loss: 1.19394, validation loss: 1.25294\n",
      "epoch 39: running loss: 1.19087, validation loss: 1.27596\n",
      "epoch 40: running loss: 1.19118, validation loss: 1.28186\n",
      "epoch 41: running loss: 1.19427, validation loss: 1.28460\n",
      "epoch 42: running loss: 1.18937, validation loss: 1.24866\n",
      "epoch 43: running loss: 1.19026, validation loss: 1.28965\n",
      "epoch 44: running loss: 1.18712, validation loss: 1.26540\n",
      "epoch 45: running loss: 1.18484, validation loss: 1.23452\n",
      "epoch 46: running loss: 1.18456, validation loss: 1.27717\n",
      "epoch 47: running loss: 1.18504, validation loss: 1.24486\n",
      "epoch 48: running loss: 1.18156, validation loss: 1.26806\n",
      "epoch 49: running loss: 1.18119, validation loss: 1.26535\n",
      "epoch 50: running loss: 1.18474, validation loss: 1.25431\n",
      "Finished Training\n",
      "epoch 45 model selected\n",
      "iteration 6 elapsed time: 90.81267166137695, accuracy : 0.44485294117647056\n",
      "epoch  1: running loss: 1.49176, validation loss: 1.33293\n",
      "epoch  2: running loss: 1.38580, validation loss: 1.33069\n",
      "epoch  3: running loss: 1.36538, validation loss: 1.31068\n",
      "epoch  4: running loss: 1.34995, validation loss: 1.33175\n",
      "epoch  5: running loss: 1.34139, validation loss: 1.30816\n",
      "epoch  6: running loss: 1.32797, validation loss: 1.30450\n",
      "epoch  7: running loss: 1.31983, validation loss: 1.30681\n",
      "epoch  8: running loss: 1.30788, validation loss: 1.30372\n",
      "epoch  9: running loss: 1.29769, validation loss: 1.30393\n",
      "epoch 10: running loss: 1.29100, validation loss: 1.28181\n",
      "epoch 11: running loss: 1.28182, validation loss: 1.26849\n",
      "epoch 12: running loss: 1.27169, validation loss: 1.26529\n",
      "epoch 13: running loss: 1.26926, validation loss: 1.28555\n",
      "epoch 14: running loss: 1.26280, validation loss: 1.26325\n",
      "epoch 15: running loss: 1.25402, validation loss: 1.27638\n",
      "epoch 16: running loss: 1.24602, validation loss: 1.29091\n",
      "epoch 17: running loss: 1.24515, validation loss: 1.26387\n",
      "epoch 18: running loss: 1.23749, validation loss: 1.25200\n",
      "epoch 19: running loss: 1.23366, validation loss: 1.24764\n",
      "epoch 20: running loss: 1.23151, validation loss: 1.28561\n",
      "epoch 21: running loss: 1.22550, validation loss: 1.28400\n",
      "epoch 22: running loss: 1.22424, validation loss: 1.28288\n",
      "epoch 23: running loss: 1.21740, validation loss: 1.28489\n",
      "epoch 24: running loss: 1.21998, validation loss: 1.26061\n",
      "epoch 25: running loss: 1.21511, validation loss: 1.24168\n",
      "epoch 26: running loss: 1.21261, validation loss: 1.26517\n",
      "epoch 27: running loss: 1.20917, validation loss: 1.26384\n",
      "epoch 28: running loss: 1.20759, validation loss: 1.25452\n",
      "epoch 29: running loss: 1.20409, validation loss: 1.29056\n",
      "epoch 30: running loss: 1.20823, validation loss: 1.24019\n",
      "epoch 31: running loss: 1.20372, validation loss: 1.25220\n",
      "epoch 32: running loss: 1.20495, validation loss: 1.26332\n",
      "epoch 33: running loss: 1.19882, validation loss: 1.25304\n",
      "epoch 34: running loss: 1.19556, validation loss: 1.24884\n",
      "epoch 35: running loss: 1.19572, validation loss: 1.26411\n",
      "epoch 36: running loss: 1.19584, validation loss: 1.26542\n",
      "epoch 37: running loss: 1.19247, validation loss: 1.25967\n",
      "epoch 38: running loss: 1.18794, validation loss: 1.25711\n",
      "epoch 39: running loss: 1.18599, validation loss: 1.25268\n",
      "epoch 40: running loss: 1.19715, validation loss: 1.24882\n",
      "epoch 41: running loss: 1.18796, validation loss: 1.27407\n",
      "epoch 42: running loss: 1.19016, validation loss: 1.26502\n",
      "epoch 43: running loss: 1.17998, validation loss: 1.24179\n",
      "epoch 44: running loss: 1.18096, validation loss: 1.24437\n",
      "epoch 45: running loss: 1.18242, validation loss: 1.23832\n",
      "epoch 46: running loss: 1.17956, validation loss: 1.25992\n",
      "epoch 47: running loss: 1.18098, validation loss: 1.25786\n",
      "epoch 48: running loss: 1.17532, validation loss: 1.25426\n",
      "epoch 49: running loss: 1.17716, validation loss: 1.25301\n",
      "epoch 50: running loss: 1.17140, validation loss: 1.26427\n",
      "Finished Training\n",
      "epoch 45 model selected\n",
      "iteration 7 elapsed time: 91.34771537780762, accuracy : 0.43933823529411764\n",
      "epoch  1: running loss: 1.44210, validation loss: 1.34225\n",
      "epoch  2: running loss: 1.38817, validation loss: 1.34788\n",
      "epoch  3: running loss: 1.37469, validation loss: 1.32350\n",
      "epoch  4: running loss: 1.35883, validation loss: 1.32442\n",
      "epoch  5: running loss: 1.35056, validation loss: 1.32867\n",
      "epoch  6: running loss: 1.34042, validation loss: 1.30506\n",
      "epoch  7: running loss: 1.33263, validation loss: 1.30687\n",
      "epoch  8: running loss: 1.32338, validation loss: 1.30188\n",
      "epoch  9: running loss: 1.31779, validation loss: 1.30538\n",
      "epoch 10: running loss: 1.31444, validation loss: 1.29768\n",
      "epoch 11: running loss: 1.30260, validation loss: 1.29001\n",
      "epoch 12: running loss: 1.30334, validation loss: 1.30221\n",
      "epoch 13: running loss: 1.29382, validation loss: 1.28819\n",
      "epoch 14: running loss: 1.28944, validation loss: 1.28200\n",
      "epoch 15: running loss: 1.28541, validation loss: 1.26889\n",
      "epoch 16: running loss: 1.28532, validation loss: 1.27976\n",
      "epoch 17: running loss: 1.27670, validation loss: 1.27450\n",
      "epoch 18: running loss: 1.27121, validation loss: 1.27640\n",
      "epoch 19: running loss: 1.27351, validation loss: 1.27942\n",
      "epoch 20: running loss: 1.26388, validation loss: 1.29012\n",
      "epoch 21: running loss: 1.26083, validation loss: 1.26522\n",
      "epoch 22: running loss: 1.26128, validation loss: 1.27825\n",
      "epoch 23: running loss: 1.25592, validation loss: 1.26123\n",
      "epoch 24: running loss: 1.24790, validation loss: 1.25731\n",
      "epoch 25: running loss: 1.25039, validation loss: 1.27099\n",
      "epoch 26: running loss: 1.24765, validation loss: 1.31106\n",
      "epoch 27: running loss: 1.24358, validation loss: 1.29853\n",
      "epoch 28: running loss: 1.23919, validation loss: 1.32332\n",
      "epoch 29: running loss: 1.23685, validation loss: 1.26737\n",
      "epoch 30: running loss: 1.23959, validation loss: 1.27678\n",
      "epoch 31: running loss: 1.23293, validation loss: 1.27573\n",
      "epoch 32: running loss: 1.23430, validation loss: 1.26174\n",
      "epoch 33: running loss: 1.22964, validation loss: 1.26932\n",
      "epoch 34: running loss: 1.22545, validation loss: 1.31179\n",
      "epoch 35: running loss: 1.22999, validation loss: 1.26450\n",
      "epoch 36: running loss: 1.22672, validation loss: 1.27428\n",
      "epoch 37: running loss: 1.22461, validation loss: 1.27420\n",
      "epoch 38: running loss: 1.22398, validation loss: 1.27248\n",
      "epoch 39: running loss: 1.22135, validation loss: 1.25185\n",
      "epoch 40: running loss: 1.22199, validation loss: 1.28546\n",
      "epoch 41: running loss: 1.21936, validation loss: 1.26903\n",
      "epoch 42: running loss: 1.21659, validation loss: 1.27294\n",
      "epoch 43: running loss: 1.21680, validation loss: 1.26712\n",
      "epoch 44: running loss: 1.20838, validation loss: 1.27086\n",
      "epoch 45: running loss: 1.21112, validation loss: 1.26576\n",
      "epoch 46: running loss: 1.21051, validation loss: 1.25891\n",
      "epoch 47: running loss: 1.20725, validation loss: 1.26413\n",
      "epoch 48: running loss: 1.20689, validation loss: 1.26507\n",
      "epoch 49: running loss: 1.20491, validation loss: 1.25515\n",
      "epoch 50: running loss: 1.20736, validation loss: 1.28605\n",
      "Finished Training\n",
      "epoch 39 model selected\n",
      "iteration 8 elapsed time: 90.03309655189514, accuracy : 0.42463235294117646\n",
      "epoch  1: running loss: 1.48990, validation loss: 1.34851\n",
      "epoch  2: running loss: 1.38707, validation loss: 1.33165\n",
      "epoch  3: running loss: 1.36671, validation loss: 1.32339\n",
      "epoch  4: running loss: 1.34845, validation loss: 1.32230\n",
      "epoch  5: running loss: 1.33526, validation loss: 1.31841\n",
      "epoch  6: running loss: 1.32095, validation loss: 1.30686\n",
      "epoch  7: running loss: 1.30502, validation loss: 1.28301\n",
      "epoch  8: running loss: 1.29446, validation loss: 1.28150\n",
      "epoch  9: running loss: 1.28272, validation loss: 1.29037\n",
      "epoch 10: running loss: 1.27685, validation loss: 1.29895\n",
      "epoch 11: running loss: 1.27023, validation loss: 1.29722\n",
      "epoch 12: running loss: 1.27068, validation loss: 1.27663\n",
      "epoch 13: running loss: 1.25759, validation loss: 1.29167\n",
      "epoch 14: running loss: 1.26083, validation loss: 1.25974\n",
      "epoch 15: running loss: 1.25081, validation loss: 1.26720\n",
      "epoch 16: running loss: 1.24925, validation loss: 1.27777\n",
      "epoch 17: running loss: 1.24799, validation loss: 1.25505\n",
      "epoch 18: running loss: 1.24224, validation loss: 1.24725\n",
      "epoch 19: running loss: 1.24354, validation loss: 1.25693\n",
      "epoch 20: running loss: 1.23905, validation loss: 1.27193\n",
      "epoch 21: running loss: 1.23479, validation loss: 1.28119\n",
      "epoch 22: running loss: 1.23016, validation loss: 1.26473\n",
      "epoch 23: running loss: 1.22985, validation loss: 1.26296\n",
      "epoch 24: running loss: 1.23081, validation loss: 1.25227\n",
      "epoch 25: running loss: 1.22047, validation loss: 1.24183\n",
      "epoch 26: running loss: 1.23095, validation loss: 1.24141\n",
      "epoch 27: running loss: 1.21999, validation loss: 1.27108\n",
      "epoch 28: running loss: 1.22584, validation loss: 1.28030\n",
      "epoch 29: running loss: 1.21551, validation loss: 1.24761\n",
      "epoch 30: running loss: 1.22034, validation loss: 1.22872\n",
      "epoch 31: running loss: 1.21616, validation loss: 1.25969\n",
      "epoch 32: running loss: 1.20827, validation loss: 1.24593\n",
      "epoch 33: running loss: 1.21626, validation loss: 1.26848\n",
      "epoch 34: running loss: 1.20771, validation loss: 1.28346\n",
      "epoch 35: running loss: 1.20296, validation loss: 1.25116\n",
      "epoch 36: running loss: 1.20570, validation loss: 1.26370\n",
      "epoch 37: running loss: 1.20454, validation loss: 1.25331\n",
      "epoch 38: running loss: 1.20473, validation loss: 1.25357\n",
      "epoch 39: running loss: 1.20255, validation loss: 1.26630\n",
      "epoch 40: running loss: 1.20092, validation loss: 1.25047\n",
      "epoch 41: running loss: 1.20139, validation loss: 1.23362\n",
      "epoch 42: running loss: 1.19633, validation loss: 1.24715\n",
      "epoch 43: running loss: 1.20311, validation loss: 1.28387\n",
      "epoch 44: running loss: 1.19580, validation loss: 1.29823\n",
      "epoch 45: running loss: 1.19240, validation loss: 1.25181\n",
      "epoch 46: running loss: 1.19059, validation loss: 1.25461\n",
      "epoch 47: running loss: 1.19348, validation loss: 1.24866\n",
      "epoch 48: running loss: 1.19343, validation loss: 1.27380\n",
      "epoch 49: running loss: 1.18826, validation loss: 1.23889\n",
      "epoch 50: running loss: 1.18661, validation loss: 1.27313\n",
      "Finished Training\n",
      "epoch 30 model selected\n",
      "iteration 9 elapsed time: 90.55422472953796, accuracy : 0.4375\n",
      "epoch  1: running loss: 1.46720, validation loss: 1.34338\n",
      "epoch  2: running loss: 1.39475, validation loss: 1.34248\n",
      "epoch  3: running loss: 1.38084, validation loss: 1.34469\n",
      "epoch  4: running loss: 1.36768, validation loss: 1.32039\n",
      "epoch  5: running loss: 1.35046, validation loss: 1.31158\n",
      "epoch  6: running loss: 1.33711, validation loss: 1.32720\n",
      "epoch  7: running loss: 1.32948, validation loss: 1.30883\n",
      "epoch  8: running loss: 1.32224, validation loss: 1.29275\n",
      "epoch  9: running loss: 1.31717, validation loss: 1.33432\n",
      "epoch 10: running loss: 1.30788, validation loss: 1.30979\n",
      "epoch 11: running loss: 1.29716, validation loss: 1.30643\n",
      "epoch 12: running loss: 1.29127, validation loss: 1.29286\n",
      "epoch 13: running loss: 1.28456, validation loss: 1.29554\n",
      "epoch 14: running loss: 1.27169, validation loss: 1.28713\n",
      "epoch 15: running loss: 1.26482, validation loss: 1.28751\n",
      "epoch 16: running loss: 1.26294, validation loss: 1.28772\n",
      "epoch 17: running loss: 1.25425, validation loss: 1.28563\n",
      "epoch 18: running loss: 1.24920, validation loss: 1.26869\n",
      "epoch 19: running loss: 1.24555, validation loss: 1.26904\n",
      "epoch 20: running loss: 1.24071, validation loss: 1.27033\n",
      "epoch 21: running loss: 1.23795, validation loss: 1.29001\n",
      "epoch 22: running loss: 1.23301, validation loss: 1.27779\n",
      "epoch 23: running loss: 1.23351, validation loss: 1.27792\n",
      "epoch 24: running loss: 1.22851, validation loss: 1.27988\n",
      "epoch 25: running loss: 1.22681, validation loss: 1.28243\n",
      "epoch 26: running loss: 1.22253, validation loss: 1.28740\n",
      "epoch 27: running loss: 1.21683, validation loss: 1.26240\n",
      "epoch 28: running loss: 1.21720, validation loss: 1.29949\n",
      "epoch 29: running loss: 1.21770, validation loss: 1.26593\n",
      "epoch 30: running loss: 1.21447, validation loss: 1.26154\n",
      "epoch 31: running loss: 1.20907, validation loss: 1.27642\n",
      "epoch 32: running loss: 1.21331, validation loss: 1.27902\n",
      "epoch 33: running loss: 1.21729, validation loss: 1.25576\n",
      "epoch 34: running loss: 1.20934, validation loss: 1.25981\n",
      "epoch 35: running loss: 1.20141, validation loss: 1.27302\n",
      "epoch 36: running loss: 1.20133, validation loss: 1.29694\n",
      "epoch 37: running loss: 1.20767, validation loss: 1.25599\n",
      "epoch 38: running loss: 1.19894, validation loss: 1.26761\n",
      "epoch 39: running loss: 1.20458, validation loss: 1.25884\n",
      "epoch 40: running loss: 1.20365, validation loss: 1.32160\n",
      "epoch 41: running loss: 1.20166, validation loss: 1.25288\n",
      "epoch 42: running loss: 1.19426, validation loss: 1.27353\n",
      "epoch 43: running loss: 1.19173, validation loss: 1.28044\n",
      "epoch 44: running loss: 1.19637, validation loss: 1.28278\n",
      "epoch 45: running loss: 1.19381, validation loss: 1.25492\n",
      "epoch 46: running loss: 1.19039, validation loss: 1.26709\n",
      "epoch 47: running loss: 1.18705, validation loss: 1.27833\n",
      "epoch 48: running loss: 1.19032, validation loss: 1.25612\n",
      "epoch 49: running loss: 1.18576, validation loss: 1.27382\n",
      "epoch 50: running loss: 1.18965, validation loss: 1.26666\n",
      "Finished Training\n",
      "epoch 41 model selected\n",
      "iteration 10 elapsed time: 90.8821451663971, accuracy : 0.42463235294117646\n"
     ]
    }
   ],
   "source": [
    "results = np.zeros([max_iterations, num_classes*4 + 1])\n",
    "# trainloader, testloader = prepare_dataloader(full_dataset)\n",
    "\n",
    "for it in range(max_iterations):\n",
    "    start = time.time()\n",
    "    \n",
    "    # mlp\n",
    "    epoch_loss_mlp = train_mlp(trainloader,testloader, print_epochs = True, loss_fn = nn.CrossEntropyLoss())\n",
    "    acc, scores = test_mlp(epoch_loss_mlp, print_model_epoch = True)\n",
    "    \n",
    "    #     # ------- mlp-resnet film \n",
    "#     epoch_loss = train_model(trainloader, testloader, print_epochs=True, loss_fn = nn.CrossEntropyLoss())\n",
    "#     acc, scores = test_model(epoch_loss, print_model_epoch = True)\n",
    "    \n",
    "    # scores = precision, recall, fscore, support\n",
    "    results[it, 0] = acc\n",
    "    \n",
    "    for j, score in enumerate(scores):\n",
    "        start_ind = 1 + j*num_classes\n",
    "        results[it, start_ind: start_ind + num_classes] = score\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print('iteration {} elapsed time: {}, accuracy : {}'.format(it+1, end-start, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c9a2433-e48a-459f-bc98-a1d16f330f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.563565</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.557598</td>\n",
       "      <td>203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.398419</td>\n",
       "      <td>0.351316</td>\n",
       "      <td>0.370706</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.358434</td>\n",
       "      <td>0.497872</td>\n",
       "      <td>0.411894</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070265</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.047561</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall        f1  support\n",
       "0   0.563565  0.558621  0.557598    203.0\n",
       "1   0.398419  0.351316  0.370706    152.0\n",
       "2   0.358434  0.497872  0.411894    141.0\n",
       "3   0.000000  0.000000  0.000000     15.0\n",
       "4   0.070265  0.036364  0.047561     33.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033424</td>\n",
       "      <td>0.062163</td>\n",
       "      <td>0.027205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019743</td>\n",
       "      <td>0.052401</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011057</td>\n",
       "      <td>0.108989</td>\n",
       "      <td>0.043609</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093961</td>\n",
       "      <td>0.050343</td>\n",
       "      <td>0.064931</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall        f1  support\n",
       "0   0.033424  0.062163  0.027205      0.0\n",
       "1   0.019743  0.052401  0.027321      0.0\n",
       "2   0.011057  0.108989  0.043609      0.0\n",
       "3   0.000000  0.000000  0.000000      0.0\n",
       "4   0.093961  0.050343  0.064931      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy mean: 0.43786764705882353, std: 0.010261239512465341\n"
     ]
    }
   ],
   "source": [
    "def display_table(scores):\n",
    "    df = np.reshape(scores, [num_classes,4], order ='F')\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    # df.style.set_table_attributes(\"style='display:inline'\").set_caption(mode)\n",
    "    \n",
    "    df.columns = ['precision', 'recall', 'f1', 'support']\n",
    "    # df.index = ['unfrozen', 'frozen']\n",
    "    # df.index = ['Visible ice', 'No visible ice']\n",
    "    \n",
    "    display(df)\n",
    "    \n",
    "def display_results(results):\n",
    "    mean = np.mean(results, axis=0)\n",
    "    std = np.std(results, axis=0)\n",
    "    \n",
    "    print(\"mean\")\n",
    "    display_table(mean[1:])\n",
    "    \n",
    "    print(\"std\")\n",
    "    display_table(std[1:])\n",
    "    \n",
    "    print(\"Accuracy mean: {}, std: {}\".format(mean[0], std[0]))\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360fa50-304f-4e3b-b33a-51b9f820a580",
   "metadata": {},
   "source": [
    "## Display Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b51afa27-1fb7-4bf4-9da2-82b90a263657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.1\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(df.depth.max())\n",
    "print(df.depth.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8c1174-a942-4f78-8a9f-4d1bb0f8b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_max = 69.5\n",
    "latitude_min = 68.3\n",
    "longitude_max = -132.8\n",
    "longitude_min = -133.9\n",
    "\n",
    "n_lat = 40\n",
    "n_lng = 20\n",
    "n_depth = 10\n",
    "\n",
    "lng_range = np.linspace(longitude_min, longitude_max, n_lng)\n",
    "lat_range = np.linspace(latitude_min, latitude_max, n_lat)\n",
    "depth_range = np.arange(0,n_depth)\n",
    "\n",
    "grid_lng, grid_lat, grid_depth = np.meshgrid(lng_range, lat_range, depth_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946bd4b9-c52f-4c0c-aec0-db5dd6176c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn = pd.DataFrame(columns=['latitude', 'longitude', 'depth', 'year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d4dce3-206c-4862-b6b9-18bf06583869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn['latitude'] = grid_lat.flatten()\n",
    "df_syn['longitude'] = grid_lng.flatten()\n",
    "df_syn['depth'] = grid_depth.flatten()\n",
    "df_syn['year'] = 2013\n",
    "df_syn['month'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa90869-47cc-481f-b33e-e28f1bcb17ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>69.5</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>69.5</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>6</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>69.5</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>7</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>69.5</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>8</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>69.5</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>9</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude  longitude  depth  year  month\n",
       "7995      69.5     -132.8      5  2013      3\n",
       "7996      69.5     -132.8      6  2013      3\n",
       "7997      69.5     -132.8      7  2013      3\n",
       "7998      69.5     -132.8      8  2013      3\n",
       "7999      69.5     -132.8      9  2013      3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_syn.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca2e1789-890d-45cd-86a4-9cc1b2124194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  68.77996824, -133.50176278,    3.95760839, 2013.37715897,\n",
       "          3.62460345])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceb5256c-18c4-429c-b4d5-947a845e9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn = scaler.transform(df_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "213f1a29-7853-43c9-af0b-b3fccfe40351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.81088706, -1.77691568, -1.06453786, -0.31905326, -1.22529482],\n",
       "       [-1.81088706, -1.77691568, -0.79555272, -0.31905326, -1.22529482],\n",
       "       [-1.81088706, -1.77691568, -0.52656758, -0.31905326, -1.22529482],\n",
       "       ...,\n",
       "       [ 2.71663018,  3.13123246,  0.81835815, -0.31905326, -1.22529482],\n",
       "       [ 2.71663018,  3.13123246,  1.08734329, -0.31905326, -1.22529482],\n",
       "       [ 2.71663018,  3.13123246,  1.35632844, -0.31905326, -1.22529482]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "272aaf1a-afbb-4a68-b560-b80f30a2d280",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a9017e484cce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfull_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'surface_data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'full_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "full_dataset[0]['surface_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "56c8a0d6-6de8-4fce-b995-099c3d73f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 model selected\n"
     ]
    }
   ],
   "source": [
    "# ------ select model ---------\n",
    "ind = 40\n",
    "\n",
    "input_size = list(full_dataset[0]['surface_data'].size())\n",
    "\n",
    "surface_model = mlp_pure(input_size[0],output_size)\n",
    "\n",
    "surface_model.load_state_dict(torch.load('mlp-models/epoch-{}.pt'.format(ind+1)))\n",
    "\n",
    "surface_model.to(device)\n",
    "\n",
    "# surface_model = surface_model.float()\n",
    "\n",
    "print(\"epoch {} model selected\".format(ind+1))\n",
    "\n",
    "# evaluate model on synthetic set\n",
    "surface_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_cert = []\n",
    "    \n",
    "    surface_data= torch.from_numpy(df_syn).float().to(device)\n",
    "\n",
    "    # y_test.append(label.numpy().list())\n",
    "    # print(label.shape)\n",
    "    # print(images.shape)\n",
    "\n",
    "    output = surface_model(surface_data)\n",
    "\n",
    "    output = sm(output)\n",
    "    # print(output)\n",
    "    \n",
    "    # proxy for uncertainty\n",
    "    # cross entropy penalizes value for not being close to 1 when it's the right category\n",
    "    # saturate leading to overconfidence\n",
    "\n",
    "    max_results = torch.max(output, dim= -1)\n",
    "    predicted = max_results.indices\n",
    "    certainty = max_results.values\n",
    "\n",
    "predicted = predicted.reshape(n_lng, n_lat, n_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1b5a81c7-c31d-4bcf-b11a-09021028a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reshaped = predicted.reshape(n_lng, n_lat, n_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0d3deb01-0c75-4e46-8acb-78dcbdf67035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 40, 10])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "385e80c1-699d-4e37-9153-57629f56614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_2d = torch.sum(predicted, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8c706888-8c64-4d5a-a245-321b32e43f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmplot as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1891b4a1-4d1f-4c86-88d8-c082d7adcd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lat = df2.latitude.unique().mean()\n",
    "mean_lng = df2.longitude.unique().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f0235fa8-e777-4b64-89f9-0680ea0a4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lng_2d, grid_lat_2d = np.meshgrid(lng_range, lat_range)\n",
    "grid_lng_2d = grid_lng_2d.flatten()\n",
    "grid_lat_2d = grid_lat_2d.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0c1c2afb-96c0-4e63-b87d-36aadac6060c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lat_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ec7f4bfd-1b83-4774-8fa0-83f9dc978cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_2d = predicted_2d.flatten().cpu()\n",
    "color_ice = ocean(predicted_2d)\n",
    "len(color_ice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "26bb1487-24e0-446b-b27d-538c47df6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_ice_hex = [None] * len(color_ice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bb11ca0e-2622-4ecd-b16c-fe88d3e76d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, color in enumerate(color_ice):\n",
    "    color_ice_hex[i] = to_hex(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "6ce4699c-01e9-44cb-bfb8-550c21f75ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap = gp.GoogleMapPlotter(mean_lat, mean_lng, 8, apikey = \"AIzaSyAabDb0aXNQT-YabM-lFULtYeotn_I2eqA\")\n",
    "\n",
    " \n",
    "gmap.scatter(grid_lat_2d, grid_lng_2d, s = 1000, c=color_ice_hex, marker=False) #, color='#3B0B39', size=40, marker=False)\n",
    "# gmap.heatmap(df2.latitude.tolist(), df2.longitude.tolist())  \n",
    "\n",
    "# gmap.scatter(grid_lat_2d[0:2], grid_lng_2d[0:2], s = 500, c=ocean[predicted_2d[0:2]])\n",
    "\n",
    "# Pass the absolute path\n",
    "gmap.draw( \"map_ice2.html\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1866904e-fdc6-4889-9fb9-4af653fc9818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lng_2d.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "185bdff7-6416-4068-b906-d31f96171af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_2d.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "54bb51c1-4fe5-43d6-b040-7f5a6da6b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib.colors import to_hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e483b7f0-cded-4428-ada9-0c0c1d02f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean = cm.get_cmap('ocean',predicted_2d.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4fc2cdde-13d9-4f7c-a10a-e30ce4a871e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.5, 0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(ocean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6a7af814-7566-41a6-8be9-a9a025861230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.903225806451613, 0.9516129032258065, 0.967741935483871, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(ocean(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a696f-0942-40e2-a053-aec3ad27eac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
