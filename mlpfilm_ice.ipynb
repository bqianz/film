{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f98096-bf83-41be-a7d5-72f7794cb779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2675d831-08ab-43f7-b625-cfab6c6128f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_width = 32\n",
    "hidden_nblocks = 4\n",
    "train_max_epoch = 50\n",
    "max_iterations = 10\n",
    "\n",
    "chip_size = 32\n",
    "\n",
    "data_root = \"geomorph_data\"\n",
    "n_channels = len(os.listdir(data_root))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "L2_param = 1e-5\n",
    "\n",
    "label_name = \"visible_ice\"\n",
    "num_classes = 5\n",
    "output_size = 5\n",
    "\n",
    "sm = nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd8266e-da2e-4871-a8f4-7d37702f30bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2824, -1.2291,  0.8613, -1.1938,  0.3424],\n",
      "        [-1.4388,  0.2917, -0.2714,  0.9226,  0.1243],\n",
      "        [ 0.1513,  0.4008, -0.9835,  0.6059, -0.3950]], requires_grad=True)\n",
      "tensor([2, 0, 4])\n",
      "tensor(2.0711, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "print(input)\n",
    "print(target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b59b6ad-d7be-476f-b4bd-c5d6ce57e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2012, -0.2795,  1.1126],\n",
      "        [-0.8434,  0.6605, -0.7067]])\n",
      "tensor([[0.1771, 0.1638, 0.6591],\n",
      "        [0.1505, 0.6770, 0.1725]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=-1)\n",
    "# m = nn.Sigmoid()\n",
    "input = torch.randn(2,3)\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629fd05b-9fa8-4502-b8cb-e0892a432591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "        def __init__(self, input_size, output_size = 1, hidden_width = 20, hidden_nblocks = 2):\n",
    "            super(mlp, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.hidden_width = hidden_width\n",
    "            self.hidden_nblocks = hidden_nblocks\n",
    "            \n",
    "            self.fc1 = nn.Linear(self.input_size, self.hidden_width)\n",
    "            self.fc2 = nn.Linear(self.hidden_width,self.hidden_width)\n",
    "            self.fc3 = nn.Linear(self.hidden_width, self.output_size)\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.end= torch.nn.Softmax(dim = -1) ## sigmoid for multi-label, softmax for multi-class (mutually exclusive)\n",
    "            \n",
    "            self.dropout = nn.Dropout(0.25)\n",
    "            \n",
    "        def forward(self, x, film_params):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            \n",
    "            \n",
    "            for i in range(self.hidden_nblocks):\n",
    "                out = self.fc2(out)\n",
    "                \n",
    "                # ------- film layer -----------\n",
    "                start = i * hidden_width * 2\n",
    "                mid = start + hidden_width\n",
    "                end = mid + hidden_width\n",
    "                \n",
    "                gamma = film_params[:, start : mid]\n",
    "                beta = film_params[:, mid : end]\n",
    "                \n",
    "#                 print(out.shape)\n",
    "#                 print(gamma.shape)\n",
    "#                 print(beta.shape)\n",
    "                \n",
    "                out = out * gamma\n",
    "                out += beta\n",
    "                # ------- film layer -----------\n",
    "                # out = self.dropout(out)\n",
    "                out = self.relu(out)\n",
    "            \n",
    "            out = self.fc3(out)\n",
    "            # out = self.end(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905bba3-9233-41cb-8ed6-ffa9087f4e8f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93742035-18c5-42bc-b9e8-86134e61c23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2837\n",
      "566\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TOPSOIL</td>\n",
       "      <td>Organics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>-1.024190</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pure ice</td>\n",
       "      <td>ICE</td>\n",
       "      <td>Ice</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>-0.835900</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>SW-SM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>-0.553466</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GW-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439946</td>\n",
       "      <td>1.851455</td>\n",
       "      <td>0.387982</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.16105</td>\n",
       "      <td>-133.08880</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GP-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.437795</td>\n",
       "      <td>1.842620</td>\n",
       "      <td>-0.741756</td>\n",
       "      <td>-1.164992</td>\n",
       "      <td>-1.225295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude                  time   borehole  depth  frozen  \\\n",
       "0  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.15       0   \n",
       "1  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.85       1   \n",
       "2  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   1.90       1   \n",
       "3  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   5.40       1   \n",
       "4  69.16105 -133.08880  2012-03-21T00:00:00Z  0170-1-12   1.20       1   \n",
       "\n",
       "  cryostructures     visible_ice ASTM_2488    materials  ...  top_of_interval  \\\n",
       "0            NaN             NaN   TOPSOIL     Organics  ...              0.0   \n",
       "1            NaN        Pure ice       ICE          Ice  ...              0.3   \n",
       "2             Nf  No visible ice     SW-SM  Coarse till  ...              1.4   \n",
       "3             Nf  No visible ice     GW-GM  Coarse till  ...              2.4   \n",
       "4             Nf  No visible ice     GP-GM  Coarse till  ...              0.0   \n",
       "\n",
       "   bottom_of_interval  month  year  month_cyclic  lat_norm  lng_norm  \\\n",
       "0                 0.3      3  2012             3  1.439946  1.851455   \n",
       "1                 1.4      3  2012             3  1.439946  1.851455   \n",
       "2                 2.4      3  2012             3  1.439946  1.851455   \n",
       "3                 8.4      3  2012             3  1.439946  1.851455   \n",
       "4                 2.4      3  2012             3  1.437795  1.842620   \n",
       "\n",
       "   depth_norm  year_norm  month_cyclic_norm  \n",
       "0   -1.024190  -1.164992          -1.225295  \n",
       "1   -0.835900  -1.164992          -1.225295  \n",
       "2   -0.553466  -1.164992          -1.225295  \n",
       "3    0.387982  -1.164992          -1.225295  \n",
       "4   -0.741756  -1.164992          -1.225295  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "def normalize(values):\n",
    "    # zero mean, unit variance\n",
    "    value_mean = values.mean()\n",
    "    value_std = values.std()\n",
    "    return (values-values_mean)/values_std\n",
    "\n",
    "def normalize_maxmin(values):\n",
    "    # range from 0 to 1\n",
    "    (values-values.min())/(values.max()-values.min())\n",
    "\n",
    "\n",
    "def get_scaler(data):\n",
    "    scaler = StandardScaler()\n",
    "    print(data)\n",
    "    scaler.fit(data)\n",
    "    return scaler\n",
    "    \n",
    "def preprocess_df(df):\n",
    "    # convert timecodes to year and month columns\n",
    "    datetimes = pd.to_datetime(df['time'])\n",
    "    df['month'] = datetimes.dt.month\n",
    "    df['year'] = datetimes.dt.year\n",
    "\n",
    "    df['month_cyclic'] = 7 - abs(df['month'] - 7)\n",
    "    \n",
    "    data = df[['latitude', 'longitude', 'depth', 'year', 'month_cyclic']]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    df[['lat_norm', 'lng_norm', 'depth_norm', 'year_norm', 'month_cyclic_norm']] = scaler.transform(df[['latitude', 'longitude', 'depth', 'year', 'month_cyclic']])\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "\n",
    "#     df['lat_norm'] = normalize(df['latitude'])\n",
    "#     df['lng_norm'] = normalize(df['longitude'])\n",
    "#     df['depth_norm'],  = normalize(df['depth'])\n",
    "#     df['year_norm'] = normalize(df['year'])\n",
    "#     df['month_cyclic_norm'] = normalize(df['month_cyclic'])\n",
    "\n",
    "df = pd.read_csv('data_stephen_fix_header.csv', header=[0])\n",
    "scaler = preprocess_df(df)\n",
    "    \n",
    "print(df.shape[0])\n",
    "print(df['borehole'].nunique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf25c9ad-dfd3-4503-ba42-c3ad78c5370a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.44755\n",
      "68.36933\n",
      "-132.89346\n",
      "-133.82843\n"
     ]
    }
   ],
   "source": [
    "print(df.latitude.max())\n",
    "print(df.latitude.min())\n",
    "print(df.longitude.max())\n",
    "print(df.longitude.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b03030-dd00-4063-948c-24493aafdd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>68.38262</td>\n",
       "      <td>-133.71211</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH15</td>\n",
       "      <td>5.45</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf/Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.499167</td>\n",
       "      <td>-0.938559</td>\n",
       "      <td>0.401431</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>68.38262</td>\n",
       "      <td>-133.71211</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH15</td>\n",
       "      <td>8.65</td>\n",
       "      <td>1</td>\n",
       "      <td>Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>9.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.499167</td>\n",
       "      <td>-0.938559</td>\n",
       "      <td>1.262184</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>ORGANICS</td>\n",
       "      <td>Organics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494489</td>\n",
       "      <td>-0.927672</td>\n",
       "      <td>-1.051089</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494489</td>\n",
       "      <td>-0.927672</td>\n",
       "      <td>-0.970393</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1</td>\n",
       "      <td>Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494489</td>\n",
       "      <td>-0.927672</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>0.736422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude  longitude                  time           borehole  depth  \\\n",
       "2832  68.38262 -133.71211  2013-04-27T00:00:00Z  W14103137-S6-BH15   5.45   \n",
       "2833  68.38262 -133.71211  2013-04-27T00:00:00Z  W14103137-S6-BH15   8.65   \n",
       "2834  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   0.05   \n",
       "2835  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   0.35   \n",
       "2836  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   4.85   \n",
       "\n",
       "      frozen cryostructures     visible_ice ASTM_2488 materials  ...  \\\n",
       "2832       1         Nf/Nbn  No visible ice       NaN      Till  ...   \n",
       "2833       1            Nbn  No visible ice       NaN      Till  ...   \n",
       "2834       0            NaN  No visible ice  ORGANICS  Organics  ...   \n",
       "2835       0            NaN  No visible ice       NaN      Till  ...   \n",
       "2836       1            Nbn  No visible ice       NaN      Till  ...   \n",
       "\n",
       "      top_of_interval  bottom_of_interval  month  year  month_cyclic  \\\n",
       "2832              2.7                 8.2      4  2013             4   \n",
       "2833              8.2                 9.1      4  2013             4   \n",
       "2834              0.0                 0.1      4  2013             4   \n",
       "2835              0.1                 0.6      4  2013             4   \n",
       "2836              0.6                 9.1      4  2013             4   \n",
       "\n",
       "      lat_norm  lng_norm  depth_norm  year_norm  month_cyclic_norm  \n",
       "2832 -1.499167 -0.938559    0.401431  -0.319053           0.736422  \n",
       "2833 -1.499167 -0.938559    1.262184  -0.319053           0.736422  \n",
       "2834 -1.494489 -0.927672   -1.051089  -0.319053           0.736422  \n",
       "2835 -1.494489 -0.927672   -0.970393  -0.319053           0.736422  \n",
       "2836 -1.494489 -0.927672    0.240040  -0.319053           0.736422  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['visible_ice'].replace(['None'], 'No visible ice', regex=True, inplace=True)\n",
    "\n",
    "ordered_ice = ['No visible ice', 'Low', \"Medium to high\", 'High', 'Pure ice']\n",
    "df['visible_ice'] = pd.Series(pd.Categorical(df['visible_ice'], categories=ordered_ice, ordered=True))\n",
    "\n",
    "df2 = df.dropna(subset=['visible_ice'])\n",
    "\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e66d4b-f80d-4295-8f97-38f6603c4ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check None values have been replaced\n",
    "len(df2[df2['visible_ice'] == 'None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00107137-4ac0-49a9-bae8-7e5aa1883ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pure ice', 'No visible ice', 'High', 'Medium to high', 'Low']\n",
      "Categories (5, object): ['No visible ice' < 'Low' < 'Medium to high' < 'High' < 'Pure ice']\n",
      "[4 0 3 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-b83c4bbc5a30>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['visible_ice_code'] =  df2['visible_ice'].cat.codes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1             Pure ice\n",
       "2       No visible ice\n",
       "3       No visible ice\n",
       "4       No visible ice\n",
       "5       No visible ice\n",
       "             ...      \n",
       "2832    No visible ice\n",
       "2833    No visible ice\n",
       "2834    No visible ice\n",
       "2835    No visible ice\n",
       "2836    No visible ice\n",
       "Name: visible_ice, Length: 2752, dtype: category\n",
       "Categories (5, object): ['No visible ice' < 'Low' < 'Medium to high' < 'High' < 'Pure ice']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['visible_ice_code'] =  df2['visible_ice'].cat.codes\n",
    "print(df2['visible_ice'].unique())\n",
    "print(df2['visible_ice_code'].unique())\n",
    "df2['visible_ice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21c3d6e0-5ed8-4d93-ad53-e6a84b8a007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visible_ice = pd.get_dummies(df2.visible_ice)\n",
    "# bin_visible_ice = (~visible_ice['No visible ice'].astype('bool')).astype('int')\n",
    "# bin_visible_ice.value_counts()\n",
    "\n",
    "# df2['visible_ice'] = bin_visible_ice\n",
    "\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1837f5df-ee84-4d0d-9942-14b948a37934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.dropna(subset=['materials'])\n",
    "df3['materials'].replace(['ICE'], 'Ice', regex=True, inplace=True)\n",
    "df3['materials'].replace(['ice'], 'Ice', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dba8d5fb-1a64-4fea-a69a-20b744e097a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_materials = pd.get_dummies(df3.materials)\n",
    "df3['material_ice'] = dm_materials['Ice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75c3c456-b700-46cd-9a6d-52872ba14aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Geo90Dataset(Dataset):\n",
    "    def __init__(self, data_root, df, base_lat, base_lng, chip_size=32):\n",
    "        \n",
    "        self.base_lat = base_lat\n",
    "        self.base_lng = base_lng\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        self.chip_size = chip_size\n",
    "        \n",
    "        self.trans = transforms.ToTensor()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.preloaded = torch.ones(self.n_channels, 6000, 6000)\n",
    "        \n",
    "        for i, file in enumerate(os.listdir(data_root)):\n",
    "            # name = file.split('_')[0]\n",
    "            # print(name)\n",
    "            self.preloaded[i] = self.trans(Image.open(data_root + os.path.sep + file))\n",
    "        \n",
    "        \n",
    "        print('Dataset initialized')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        bh_id = row.at['borehole']\n",
    "        lat = row.at['latitude']\n",
    "        lng = row.at['longitude']\n",
    "        \n",
    "\n",
    "        pixel_len = 5/6000\n",
    "        \n",
    "\n",
    "        lat_index_start = np.round((self.base_lat - lat) / pixel_len - self.chip_size/2).astype(int)\n",
    "        lat_index_end = lat_index_start + self.chip_size\n",
    "        \n",
    "        lng_index_start = np.round((lng - self.base_lng) / pixel_len - self.chip_size/2).astype(int)\n",
    "        lng_index_end = lng_index_start + self.chip_size\n",
    "        \n",
    "        image = self.preloaded[:, lat_index_start:lat_index_end,lng_index_start:lng_index_end]\n",
    "        \n",
    "        \n",
    "        # surface = torch.tensor(row.filter(['depth'])).float()\n",
    "        surface = torch.tensor(row.filter(['depth_norm', 'month_cyclic_norm', 'lat_norm', 'lng_norm', 'year_norm'])).float()\n",
    "        \n",
    "        frozen = torch.tensor(row.at['frozen']).float()\n",
    "        \n",
    "        # visible_ice = torch.tensor(row.at['visible_ice']).float()\n",
    "        visible_ice = torch.tensor(row.at['visible_ice_code']).long()\n",
    "        \n",
    "        # material_ice = torch.tensor(row.at['material_ice']).float()\n",
    "        \n",
    "        return {'image': image, 'surface_data': surface, 'frozen': frozen,  'visible_ice': visible_ice} #'material_ice': material_ice}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b0df9-d51d-4bb3-9658-7a49e95bc6a6",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f46b0-7537-4ceb-8f96-263c007e97fd",
   "metadata": {},
   "source": [
    "## FiLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "214c4911-bcfb-43a2-b41b-b01949690b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_film_params = hidden_width * hidden_nblocks * 2\n",
    "\n",
    "# generator = resnet18(n_channels, n_film_params)\n",
    "\n",
    "def train_model(trainloader, testloader, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    # loss: binary cross entropy\n",
    "\n",
    "    generator = models.resnet18()\n",
    "    generator.fc = nn.Linear(512, n_film_params)\n",
    "    generator.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    gen_model = generator\n",
    "\n",
    "    # print(gen_model)\n",
    "\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    net_model = mlp(input_size[0],output_size, hidden_width, hidden_nblocks).to(device)\n",
    "    \n",
    "    gen_optimizer = torch.optim.Adam(gen_model.parameters(), weight_decay = L2_param)\n",
    "    net_optimizer = torch.optim.Adam(net_model.parameters(), weight_decay = L2_param)\n",
    "    \n",
    "    gen_model.to(device)\n",
    "    net_model.to(device)\n",
    "\n",
    "    # --------- check back propagation ----------- -\n",
    "    # net_model.fc1.weight.register_hook(lambda x: print('grad accumulated in mlp fc1'))\n",
    "    # gen_first_layer = gen_model.encoder.blocks[0].blocks[0].blocks[0].conv\n",
    "    # gen_first_layer.weight.register_hook(lambda x: print('grad accumulated in resnet first layer'))\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        # ------------ train -----------------\n",
    "        gen_model.train()\n",
    "        net_model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            \n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "\n",
    "            gen_optimizer.zero_grad()\n",
    "            net_optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            gen_optimizer.step()\n",
    "            net_optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        gen_model.eval()\n",
    "        net_model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # TODO: exammine film_params gradients / readup pytorch\n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "\n",
    "        torch.save(gen_model.state_dict(), os.path.join('mlp-resnet-models/', 'gen-epoch-{}.pt'.format(epoch+1)))\n",
    "        torch.save(net_model.state_dict(), os.path.join('mlp-resnet-models/', 'net-epoch-{}.pt'.format(epoch+1)))\n",
    "\n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "## Test model\n",
    "\n",
    "def test_model(epoch_loss, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    generator = models.resnet18()\n",
    "    generator.fc = nn.Linear(512, n_film_params)\n",
    "    generator.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    gen_model = generator\n",
    "\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    net_model = mlp(input_size[0],1, hidden_width, hidden_nblocks)\n",
    "\n",
    "    gen_model.load_state_dict(torch.load('mlp-resnet-models/gen-epoch-{}.pt'.format(ind+1)))\n",
    "    net_model.load_state_dict(torch.load('mlp-resnet-models/net-epoch-{}.pt'.format(ind+1)))\n",
    "    \n",
    "    gen_model.to(device)\n",
    "    net_model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    gen_model.eval()\n",
    "    net_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "\n",
    "            predicted = torch.round(predicted)\n",
    "            # print(predicted.shape)\n",
    "            lb = labels.tolist()\n",
    "            pr = predicted.tolist()\n",
    "            y_test.extend(lb)\n",
    "            y_pred.extend(pr)\n",
    "    \n",
    "    arr_accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "    return arr_accuracy, scores, gen_model, net_model\n",
    "\n",
    "\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "#     print(classification_report(y_test,y_pred))\n",
    "#     print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "## Pure MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8b5b0-1039-4c76-93e4-0f90b60d87be",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c02928-f9c5-4b2a-8ef7-d8a2de4a4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_pure(nn.Module):\n",
    "        def __init__(self, input_size, output_size):\n",
    "            super(mlp_pure, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.hidden_size = hidden_width\n",
    "            self.hidden_nblocks = hidden_nblocks\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size,self.hidden_size)\n",
    "            self.fc3 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.end = torch.nn.Softmax(dim=-1) ## sigmoid for multi-label, softmax for multi-class (mutually exclusive)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            # print(out.shape)\n",
    "            \n",
    "            for i in range(self.hidden_nblocks):\n",
    "                out = self.fc2(out)\n",
    "                out = self.relu(out)\n",
    "            \n",
    "            out = self.fc3(out)\n",
    "            # out = self.end(out)\n",
    "            return out\n",
    "\n",
    "def train_mlp(trainloader, testloader, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    surface_model = mlp_pure(input_size[0],output_size)\n",
    "    \n",
    "    surface_model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(surface_model.parameters(), weight_decay = L2_param)\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        surface_model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            predicted = surface_model(surface_data)\n",
    "            \n",
    "            # print(predicted.shape)\n",
    "            \n",
    "            # squeeze: return tensor with all dimensions of size 1 removed\n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        surface_model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            predicted = surface_model(surface_data)\n",
    "            \n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "\n",
    "        torch.save(surface_model.state_dict(), os.path.join('mlp-models/', 'epoch-{}.pt'.format(epoch+1)))\n",
    "    \n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "        \n",
    "    return epoch_loss\n",
    "        \n",
    "def test_mlp(epoch_loss, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    \n",
    "    surface_model = mlp_pure(input_size[0],output_size)\n",
    "    surface_model.load_state_dict(torch.load('mlp-models/epoch-{}.pt'.format(ind+1)))\n",
    "    \n",
    "    surface_model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    surface_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        y_cert = []\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            output = surface_model(surface_data)\n",
    "            \n",
    "            output = sm(output)\n",
    "            # print(output)\n",
    "            \n",
    "            \n",
    "            max_results = torch.max(output, dim= -1)\n",
    "            predicted = max_results.indices\n",
    "            certainty = max_results.values\n",
    "            #predicted = torch.round(output)\n",
    "            # print(predicted.shape)\n",
    "            \n",
    "            y_test.extend(labels.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            y_cert.extend(certainty.tolist())\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "#     print(y_test)\n",
    "#     print(y_pred)\n",
    "    with open(\"mlp-certainty/iteration_{}.txt\".format(it), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(y_cert, fp)\n",
    "    #with open(\"test.txt\", \"rb\") as fp:   # Unpickling\n",
    "        #b = pickle.load(fp)\n",
    "    arr_accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "    return arr_accuracy, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cfb6b-c5ca-4fdd-81e9-571685a86522",
   "metadata": {},
   "source": [
    "## Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51bacf58-7320-49da-8f08-aa180ed67446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized\n"
     ]
    }
   ],
   "source": [
    "base_lat = 70\n",
    "base_lng = -135\n",
    "\n",
    "loaded_dataset = Geo90Dataset(data_root, df2, base_lat, base_lng, chip_size)\n",
    "\n",
    "# image = full_dataset[0]['image']\n",
    "# n_channels = list(image.shape)[0]\n",
    "\n",
    "# for data in full_dataset:\n",
    "#     image = data['image']\n",
    "#     for i in range(n_channels):\n",
    "#         channel = image[i]\n",
    "#         ind = (channel == -9999)\n",
    "# #         mean_val = torch.mean(channel[~ind])\n",
    "#         channel[ind] = 0\n",
    "#         data['image'][i] = channel\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44629f3-5888-4eb0-8f54-cfd63f455c54",
   "metadata": {},
   "source": [
    "### Discard samples with invalid values in image chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc861435-97ab-4095-9760-4b797477daa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752\n",
      "2717\n"
     ]
    }
   ],
   "source": [
    "valid_ind = []\n",
    "for i, data in enumerate(loaded_dataset):\n",
    "    image = data['image']\n",
    "    ind  = (image == -9999)\n",
    "    if ~torch.any(ind):\n",
    "        valid_ind.append(i)\n",
    "\n",
    "full_dataset = torch.utils.data.Subset(loaded_dataset, valid_ind)\n",
    "\n",
    "\n",
    "print(len(loaded_dataset))\n",
    "print(len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba299154-57ac-4e30-bf2e-74e41e3fc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# batchsize can cause error when last leftover batchsize is 1, batchnorm cannot function on 1 sample data\n",
    "batchsize = 20\n",
    "while(train_size % batchsize == 1):\n",
    "    batchsize+=1\n",
    "print(batchsize)\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(full_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb6971-4f9e-409e-b8ee-9ebe27a3cfae",
   "metadata": {},
   "source": [
    "## Scale of image chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e37318c4-ff02-48e0-aa8c-c5a708eb0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = full_dataset[0]['image']\n",
    "# n_samples = len(train_data)\n",
    "# n_channels = list(image.shape)[0]\n",
    "\n",
    "# scalers = []\n",
    "# for i in range(n_channels):\n",
    "#     print(i)\n",
    "#     scaler = StandardScaler()\n",
    "#     X = torch.empty((n_samples, chip_size, chip_size))\n",
    "    \n",
    "#     for j, data in enumerate(train_data):\n",
    "#         #print(data['image'][i].shape)\n",
    "#         # print(X[j].shape)\n",
    "#         X[j] = data['image'][i]\n",
    "#     X = torch.reshape(X, (-1,1))\n",
    "#     #print(X)\n",
    "#     # break;\n",
    "#     scaler.fit(X)\n",
    "#     scalers.append(scaler)\n",
    "    \n",
    "#     def scale_data(subset):\n",
    "#         for data in subset:\n",
    "#             X = data['image'][i]\n",
    "#             X_flat = torch.reshape(X, (-1,1))\n",
    "            \n",
    "#             X_trans = scaler.transform(X_flat)\n",
    "#             data['image'][i] = torch.reshape(torch.Tensor(X_trans), (chip_size, chip_size))\n",
    "    \n",
    "#     scale_data(train_data)\n",
    "#     scale_data(test_data)\n",
    "#     print(\"Channel {} scaled.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5f7a6d03-a8b0-4c0f-b562-415f30deb0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: running loss: 1.42660, validation loss: 1.36362\n",
      "epoch  2: running loss: 1.37564, validation loss: 1.32361\n",
      "epoch  3: running loss: 1.35967, validation loss: 1.36168\n",
      "epoch  4: running loss: 1.35044, validation loss: 1.31392\n",
      "epoch  5: running loss: 1.34590, validation loss: 1.34843\n",
      "epoch  6: running loss: 1.32139, validation loss: 1.38295\n",
      "epoch  7: running loss: 1.33040, validation loss: 1.35228\n",
      "epoch  8: running loss: 1.31607, validation loss: 1.30063\n",
      "epoch  9: running loss: 1.32525, validation loss: 1.32615\n",
      "epoch 10: running loss: 1.30063, validation loss: 1.37722\n",
      "epoch 11: running loss: 1.29910, validation loss: 1.33399\n",
      "epoch 12: running loss: 1.27183, validation loss: 1.35280\n",
      "epoch 13: running loss: 1.26689, validation loss: 1.39113\n",
      "epoch 14: running loss: 1.26882, validation loss: 1.32190\n",
      "epoch 15: running loss: 1.25196, validation loss: 1.32884\n",
      "epoch 16: running loss: 1.24816, validation loss: 1.35672\n",
      "epoch 17: running loss: 1.23621, validation loss: 1.33628\n",
      "epoch 18: running loss: 1.23398, validation loss: 1.35638\n",
      "epoch 19: running loss: 1.23201, validation loss: 1.34914\n",
      "epoch 20: running loss: 1.22229, validation loss: 1.38147\n",
      "epoch 21: running loss: 1.22719, validation loss: 1.38282\n",
      "epoch 22: running loss: 1.21019, validation loss: 1.36306\n",
      "epoch 23: running loss: 1.19435, validation loss: 1.38711\n",
      "epoch 24: running loss: 1.20071, validation loss: 1.38526\n",
      "epoch 25: running loss: 1.18543, validation loss: 1.39242\n",
      "epoch 26: running loss: 1.17510, validation loss: 1.34894\n",
      "epoch 27: running loss: 1.17836, validation loss: 1.34251\n",
      "epoch 28: running loss: 1.16222, validation loss: 1.46711\n",
      "epoch 29: running loss: 1.16659, validation loss: 1.41626\n",
      "epoch 30: running loss: 1.15446, validation loss: 1.34851\n",
      "epoch 31: running loss: 1.14723, validation loss: 1.39781\n",
      "epoch 32: running loss: 1.14337, validation loss: 1.43332\n",
      "epoch 33: running loss: 1.13803, validation loss: 1.42522\n",
      "epoch 34: running loss: 1.13744, validation loss: 1.37659\n",
      "epoch 35: running loss: 1.12080, validation loss: 1.37800\n",
      "epoch 36: running loss: 1.11752, validation loss: 1.47231\n",
      "epoch 37: running loss: 1.10913, validation loss: 1.43552\n",
      "epoch 38: running loss: 1.10033, validation loss: 1.42054\n",
      "epoch 39: running loss: 1.11112, validation loss: 1.46370\n",
      "epoch 40: running loss: 1.09869, validation loss: 1.43511\n",
      "epoch 41: running loss: 1.08262, validation loss: 1.40774\n",
      "epoch 42: running loss: 1.07843, validation loss: 1.41104\n",
      "epoch 43: running loss: 1.07045, validation loss: 1.46295\n",
      "epoch 44: running loss: 1.07921, validation loss: 1.45606\n",
      "epoch 45: running loss: 1.06089, validation loss: 1.50478\n",
      "epoch 46: running loss: 1.07448, validation loss: 1.46071\n",
      "epoch 47: running loss: 1.06225, validation loss: 1.51586\n",
      "epoch 48: running loss: 1.05671, validation loss: 1.52732\n",
      "epoch 49: running loss: 1.08549, validation loss: 1.46880\n",
      "epoch 50: running loss: 1.06801, validation loss: 1.48243\n",
      "Finished Training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for mlp:\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([5, 32]) from checkpoint, the shape in current model is torch.Size([1, 32]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-db348c2a00a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#     # ------- mlp-resnet film\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_model_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# scores = precision, recall, fscore, support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-db5a6461a75c>\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(epoch_loss, print_model_epoch)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mgen_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mlp-resnet-models/gen-epoch-{}.pt'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mnet_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mlp-resnet-models/net-epoch-{}.pt'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mgen_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mouju\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for mlp:\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([5, 32]) from checkpoint, the shape in current model is torch.Size([1, 32]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([1])."
     ]
    }
   ],
   "source": [
    "max_iterations = 1\n",
    "\n",
    "results = np.zeros([max_iterations, num_classes*4 + 1])\n",
    "# trainloader, testloader = prepare_dataloader(full_dataset)\n",
    "\n",
    "for it in range(max_iterations):\n",
    "    start = time.time()\n",
    "    \n",
    "    # mlp\n",
    "#     epoch_loss_mlp = train_mlp(trainloader,testloader, print_epochs = True, loss_fn = nn.CrossEntropyLoss())\n",
    "#     acc, scores = test_mlp(epoch_loss_mlp, print_model_epoch = True)\n",
    "    \n",
    "    #     # ------- mlp-resnet film \n",
    "    epoch_loss = train_model(trainloader, testloader, print_epochs=True, loss_fn = nn.CrossEntropyLoss())\n",
    "    acc, scores, gen_model, test_model = test_model(epoch_loss, print_model_epoch = True)\n",
    "    \n",
    "    # scores = precision, recall, fscore, support\n",
    "    results[it, 0] = acc\n",
    "    \n",
    "    for j, score in enumerate(scores):\n",
    "        start_ind = 1 + j*num_classes\n",
    "        results[it, start_ind: start_ind + num_classes] = score\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print('iteration {} elapsed time: {}, accuracy : {}'.format(it+1, end-start, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c9a2433-e48a-459f-bc98-a1d16f330f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.576118</td>\n",
       "      <td>0.605911</td>\n",
       "      <td>0.589764</td>\n",
       "      <td>203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.395131</td>\n",
       "      <td>0.381579</td>\n",
       "      <td>0.387015</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.394743</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.428023</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall        f1  support\n",
       "0   0.576118  0.605911  0.589764    203.0\n",
       "1   0.395131  0.381579  0.387015    152.0\n",
       "2   0.394743  0.468085  0.428023    141.0\n",
       "3   0.000000  0.000000  0.000000     15.0\n",
       "4   0.108696  0.075758  0.089286     33.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034993</td>\n",
       "      <td>0.009852</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017082</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.005390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall        f1  support\n",
       "0   0.034993  0.009852  0.013727      0.0\n",
       "1   0.017082  0.026316  0.005390      0.0\n",
       "2   0.014062  0.007092  0.005310      0.0\n",
       "3   0.000000  0.000000  0.000000      0.0\n",
       "4   0.108696  0.075758  0.089286      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy mean: 0.4586397058823529, std: 0.00643382352941177\n"
     ]
    }
   ],
   "source": [
    "def display_table(scores):\n",
    "    df = np.reshape(scores, [num_classes,4], order ='F')\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    # df.style.set_table_attributes(\"style='display:inline'\").set_caption(mode)\n",
    "    \n",
    "    df.columns = ['precision', 'recall', 'f1', 'support']\n",
    "    # df.index = ['unfrozen', 'frozen']\n",
    "    # df.index = ['Visible ice', 'No visible ice']\n",
    "    \n",
    "    display(df)\n",
    "    \n",
    "def display_results(results):\n",
    "    mean = np.mean(results, axis=0)\n",
    "    std = np.std(results, axis=0)\n",
    "    \n",
    "    print(\"mean\")\n",
    "    display_table(mean[1:])\n",
    "    \n",
    "    print(\"std\")\n",
    "    display_table(std[1:])\n",
    "    \n",
    "    print(\"Accuracy mean: {}, std: {}\".format(mean[0], std[0]))\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360fa50-304f-4e3b-b33a-51b9f820a580",
   "metadata": {},
   "source": [
    "## Display Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b51afa27-1fb7-4bf4-9da2-82b90a263657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.1\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(df.depth.max())\n",
    "print(df.depth.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b8c1174-a942-4f78-8a9f-4d1bb0f8b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_max = 69.5\n",
    "latitude_min = 68.3\n",
    "longitude_max = -132.8\n",
    "longitude_min = -133.9\n",
    "\n",
    "n_lat = 40\n",
    "n_lng = 20\n",
    "\n",
    "lng_range = np.linspace(longitude_min, longitude_max, n_lng)\n",
    "lat_range = np.linspace(latitude_min, latitude_max, n_lat)\n",
    "depth_range = [1.25, 3.75, 6.25, 8.75, 15]\n",
    "\n",
    "n_depth = len(depth_range)\n",
    "\n",
    "grid_lng, grid_lat, grid_depth = np.meshgrid(lng_range, lat_range, depth_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "946bd4b9-c52f-4c0c-aec0-db5dd6176c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn = pd.DataFrame(columns=['latitude', 'longitude', 'depth', 'year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9d4dce3-206c-4862-b6b9-18bf06583869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn['latitude'] = grid_lat.flatten()\n",
    "df_syn['longitude'] = grid_lng.flatten()\n",
    "df_syn['depth'] = grid_depth.flatten()\n",
    "df_syn['year'] = 2013\n",
    "df_syn['month'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5aa90869-47cc-481f-b33e-e28f1bcb17ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>69.5</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>69.5</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>69.5</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>6.25</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>69.5</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>8.75</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>69.5</td>\n",
       "      <td>-132.8</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude  longitude  depth  year  month\n",
       "3995      69.5     -132.8   1.25  2013      3\n",
       "3996      69.5     -132.8   3.75  2013      3\n",
       "3997      69.5     -132.8   6.25  2013      3\n",
       "3998      69.5     -132.8   8.75  2013      3\n",
       "3999      69.5     -132.8  15.00  2013      3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_syn.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca2e1789-890d-45cd-86a4-9cc1b2124194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  68.77996824, -133.50176278,    3.95760839, 2013.37715897,\n",
       "          3.62460345])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ceb5256c-18c4-429c-b4d5-947a845e9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn = scaler.transform(df_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "213f1a29-7853-43c9-af0b-b3fccfe40351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.81088706, -1.77691568, -0.72830643, -0.31905326, -1.22529482],\n",
       "       [-1.81088706, -1.77691568, -0.05584357, -0.31905326, -1.22529482],\n",
       "       [-1.81088706, -1.77691568,  0.61661929, -0.31905326, -1.22529482],\n",
       "       ...,\n",
       "       [ 2.71663018,  3.13123246,  0.61661929, -0.31905326, -1.22529482],\n",
       "       [ 2.71663018,  3.13123246,  1.28908215, -0.31905326, -1.22529482],\n",
       "       [ 2.71663018,  3.13123246,  2.9702393 , -0.31905326, -1.22529482]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "272aaf1a-afbb-4a68-b560-b80f30a2d280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8359, -1.2253,  1.4399,  1.8515, -1.1650])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0]['surface_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56c8a0d6-6de8-4fce-b995-099c3d73f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 model selected\n"
     ]
    }
   ],
   "source": [
    "# ------ select model ---------\n",
    "ind = 40\n",
    "\n",
    "input_size = list(full_dataset[0]['surface_data'].size())\n",
    "\n",
    "surface_model = mlp_pure(input_size[0],output_size)\n",
    "\n",
    "surface_model.load_state_dict(torch.load('mlp-models/epoch-{}.pt'.format(ind+1)))\n",
    "\n",
    "surface_model.to(device)\n",
    "\n",
    "# surface_model = surface_model.float()\n",
    "\n",
    "print(\"epoch {} model selected\".format(ind+1))\n",
    "\n",
    "# evaluate model on synthetic set\n",
    "surface_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_cert = []\n",
    "    \n",
    "    surface_data= torch.from_numpy(df_syn).float().to(device)\n",
    "\n",
    "    # y_test.append(label.numpy().list())\n",
    "    # print(label.shape)\n",
    "    # print(images.shape)\n",
    "\n",
    "    output = surface_model(surface_data)\n",
    "\n",
    "    output = sm(output)\n",
    "    # print(output)\n",
    "    \n",
    "    # proxy for uncertainty\n",
    "    # cross entropy penalizes value for not being close to 1 when it's the right category\n",
    "    # saturate leading to overconfidence\n",
    "\n",
    "    max_results = torch.max(output, dim= -1)\n",
    "    predicted = max_results.indices\n",
    "    certainty = max_results.values\n",
    "\n",
    "predicted = predicted.reshape(n_lat, n_lng, n_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d3deb01-0c75-4e46-8acb-78dcbdf67035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 20, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "385e80c1-699d-4e37-9153-57629f56614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_2d = torch.sum(predicted, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50203665-7da9-4778-8a83-cc0013f9dd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91a1bd18-65b3-423c-bd4e-00b0180a19f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(predicted[:,:,3].cpu(), vmin=0, vmax=4)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks([0, 1, 2,3, 4])\n",
    "# cbar.set_ticklabels([\"A\", \"B\", \"C\", \"D\"])\n",
    "# plt.colorbar().ax.set_yticklabels(['0', '', '1', '', '2','', '3','', '4'])  # vertically oriented colorbar\n",
    "\n",
    "plt.title(\"Depth 7.5 - 10 m\")\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "plt.tick_params(\n",
    "    axis='y',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    left=False,      # ticks along the bottom edge are off\n",
    "    right=False,         # ticks along the top edge are off\n",
    "    labelleft=False) # labels along the bottom edge are off\n",
    "\n",
    "# frame = plt.gca()\n",
    "# frame.axes.get_xaxis().set_visible(False)\n",
    "# frame.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c706888-8c64-4d5a-a245-321b32e43f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmplot as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1891b4a1-4d1f-4c86-88d8-c082d7adcd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lat = df2.latitude.unique().mean()\n",
    "mean_lng = df2.longitude.unique().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0235fa8-e777-4b64-89f9-0680ea0a4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lng_2d, grid_lat_2d = np.meshgrid(lng_range, lat_range)\n",
    "grid_lng_2d = grid_lng_2d.flatten()\n",
    "grid_lat_2d = grid_lat_2d.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c1c2afb-96c0-4e63-b87d-36aadac6060c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lat_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54bb51c1-4fe5-43d6-b040-7f5a6da6b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib.colors import to_hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e483b7f0-cded-4428-ada9-0c0c1d02f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean = cm.get_cmap('ocean',predicted_2d.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec7f4bfd-1b83-4774-8fa0-83f9dc978cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_2d = predicted[:,:,0].flatten().cpu()\n",
    "color_ice = ocean(predicted_2d)\n",
    "len(color_ice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26bb1487-24e0-446b-b27d-538c47df6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_ice_hex = [None] * len(color_ice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb11ca0e-2622-4ecd-b16c-fe88d3e76d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, color in enumerate(color_ice):\n",
    "    color_ice_hex[i] = to_hex(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ce4699c-01e9-44cb-bfb8-550c21f75ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap = gp.GoogleMapPlotter(mean_lat, mean_lng, 8, apikey = \"AIzaSyBSu4YFhdYhQKw02J-AUP62mCODh735OLQ\")\n",
    "\n",
    " \n",
    "gmap.scatter(grid_lat_2d, grid_lng_2d, s = 1000, c=color_ice_hex, marker=False) #, color='#3B0B39', size=40, marker=False)\n",
    "# gmap.heatmap(df2.latitude.tolist(), df2.longitude.tolist())  \n",
    "\n",
    "# gmap.scatter(grid_lat_2d[0:2], grid_lng_2d[0:2], s = 500, c=ocean[predicted_2d[0:2]])\n",
    "\n",
    "# Pass the absolute path\n",
    "gmap.draw( \"map_ice_depth0.html\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1866904e-fdc6-4889-9fb9-4af653fc9818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lng_2d.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "185bdff7-6416-4068-b906-d31f96171af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_2d.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4fc2cdde-13d9-4f7c-a10a-e30ce4a871e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.5, 0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(ocean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6a7af814-7566-41a6-8be9-a9a025861230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.903225806451613, 0.9516129032258065, 0.967741935483871, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(ocean(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c15a696f-0942-40e2-a053-aec3ad27eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap = gp.GoogleMapPlotter(mean_lat, mean_lng, 8, apikey = \"AIzaSyBSu4YFhdYhQKw02J-AUP62mCODh735OLQ\")\n",
    "gmap.heatmap(df2.latitude.tolist(), df2.longitude.tolist())  \n",
    "# Pass the absolute path\n",
    "gmap.draw( \"heatmap.html\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e0d3a-b002-4a65-961d-931f008eaf7c",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89b136d1-e49e-46dc-8e2e-1643cb006715",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c055ee46-2310-4744-b6a2-988734ba970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_surfaces = batch['surface_data'].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04ae0b0a-a92e-48cb-9616-acc61ccfc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "835bda05-90ef-48b1-8fda-68f6f46142b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_surfaces = batch['surface_data'].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d9c705d-867f-45d8-b4ae-aca9dedebb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp_pure(\n",
       "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (end): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surface_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "933e8571-4c86-4aba-9960-21099dc9fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "efd727b6-37da-4677-9631-c1e0a124bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = shap.DeepExplainer(surface_model, train_surfaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5590e874-99f6-43c3-a3ef-fbab49defc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    }
   ],
   "source": [
    "shap_values = e.shap_values(test_surfaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "576ada57-5588-4bc6-9f05-4d771d7dcd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_v = np.array(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d8865f06-ca2e-46a6-b6f1-2c600b1d02ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20, 5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94090a-97aa-48fb-a4ce-2528c27fee3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
