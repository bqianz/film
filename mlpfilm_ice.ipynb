{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c6a9f38-9ff3-4628-97f9-39fa73883c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import time\n",
    "\n",
    "hidden_width = 32\n",
    "hidden_nblocks = 4\n",
    "train_max_epoch = 50\n",
    "\n",
    "chip_size = 32\n",
    "\n",
    "data_root = \"geomorph_data\"\n",
    "n_channels = len(os.listdir(data_root))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "L2_param = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629fd05b-9fa8-4502-b8cb-e0892a432591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "        def __init__(self, input_size, output_size = 1, hidden_width = 20, hidden_nblocks = 2):\n",
    "            super(mlp, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.hidden_width = hidden_width\n",
    "            self.hidden_nblocks = hidden_nblocks\n",
    "            \n",
    "            self.fc1 = nn.Linear(self.input_size, self.hidden_width)\n",
    "            self.fc2 = nn.Linear(self.hidden_width,self.hidden_width)\n",
    "            self.fc3 = nn.Linear(self.hidden_width, self.output_size)\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.sigmoid = torch.nn.Sigmoid() ## sigmoid for multi-label, softmax for multi-class (mutually exclusive)\n",
    "            \n",
    "            self.dropout = nn.Dropout(0.25)\n",
    "            \n",
    "        def forward(self, x, film_params):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            \n",
    "            \n",
    "            for i in range(self.hidden_nblocks):\n",
    "                out = self.fc2(out)\n",
    "                \n",
    "                # ------- film layer -----------\n",
    "                start = i * hidden_width * 2\n",
    "                mid = start + hidden_width\n",
    "                end = mid + hidden_width\n",
    "                \n",
    "                gamma = film_params[:, start : mid]\n",
    "                beta = film_params[:, mid : end]\n",
    "                \n",
    "#                 print(out.shape)\n",
    "#                 print(gamma.shape)\n",
    "#                 print(beta.shape)\n",
    "                \n",
    "                out = out * gamma\n",
    "                out += beta\n",
    "                # ------- film layer -----------\n",
    "                # out = self.dropout(out)\n",
    "                out = self.relu(out)\n",
    "            \n",
    "            out = self.fc3(out)\n",
    "            out = self.sigmoid(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905bba3-9233-41cb-8ed6-ffa9087f4e8f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93742035-18c5-42bc-b9e8-86134e61c23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2837\n",
      "566\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TOPSOIL</td>\n",
       "      <td>Organics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>-1.024010</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pure ice</td>\n",
       "      <td>ICE</td>\n",
       "      <td>Ice</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>-0.835753</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>SW-SM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>-0.553369</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GW-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>0.387914</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.16105</td>\n",
       "      <td>-133.08880</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>GP-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.437542</td>\n",
       "      <td>1.842296</td>\n",
       "      <td>-0.741625</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude                  time   borehole  depth  frozen  \\\n",
       "0  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.15       0   \n",
       "1  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.85       1   \n",
       "2  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   1.90       1   \n",
       "3  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   5.40       1   \n",
       "4  69.16105 -133.08880  2012-03-21T00:00:00Z  0170-1-12   1.20       1   \n",
       "\n",
       "  cryostructures     visible_ice ASTM_2488    materials  ...  top_of_interval  \\\n",
       "0            NaN             NaN   TOPSOIL     Organics  ...              0.0   \n",
       "1            NaN        Pure ice       ICE          Ice  ...              0.3   \n",
       "2             Nf  No visible ice     SW-SM  Coarse till  ...              1.4   \n",
       "3             Nf  No visible ice     GW-GM  Coarse till  ...              2.4   \n",
       "4             Nf  No visible ice     GP-GM  Coarse till  ...              0.0   \n",
       "\n",
       "   bottom_of_interval  month  year  month_cyclic  lat_norm  lng_norm  \\\n",
       "0                 0.3      3  2012             3  1.439692  1.851129   \n",
       "1                 1.4      3  2012             3  1.439692  1.851129   \n",
       "2                 2.4      3  2012             3  1.439692  1.851129   \n",
       "3                 8.4      3  2012             3  1.439692  1.851129   \n",
       "4                 2.4      3  2012             3  1.437542  1.842296   \n",
       "\n",
       "   depth_norm  year_norm  month_cyclic_norm  \n",
       "0   -1.024010  -1.164786          -1.225079  \n",
       "1   -0.835753  -1.164786          -1.225079  \n",
       "2   -0.553369  -1.164786          -1.225079  \n",
       "3    0.387914  -1.164786          -1.225079  \n",
       "4   -0.741625  -1.164786          -1.225079  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "def normalize(values):\n",
    "    # zero mean, unit variance\n",
    "    return (values-values.mean())/values.std()\n",
    "\n",
    "def normalize_maxmin(values):\n",
    "    # range from 0 to 1\n",
    "    (values-values.min())/(values.max()-values.min())\n",
    "\n",
    "def preprocess_df(df):\n",
    "    # convert timecodes to year and month columns\n",
    "    datetimes = pd.to_datetime(df['time'])\n",
    "    df['month'] = datetimes.dt.month\n",
    "    df['year'] = datetimes.dt.year\n",
    "\n",
    "    df['month_cyclic'] = 7 - abs(df['month'] - 7)\n",
    "\n",
    "    df['lat_norm'] = normalize(df['latitude'])\n",
    "    df['lng_norm'] = normalize(df['longitude'])\n",
    "    df['depth_norm'] = normalize(df['depth'])\n",
    "    df['year_norm'] = normalize(df['year'])\n",
    "    df['month_cyclic_norm'] = normalize(df['month_cyclic'])\n",
    "\n",
    "df = pd.read_csv('data_stephen_fix_header.csv', header=[0])\n",
    "preprocess_df(df)\n",
    "    \n",
    "print(df.shape[0])\n",
    "print(df['borehole'].nunique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32725fd1-d4ef-41e8-8fc9-6cb0af2751ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mouju\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\series.py:4509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>68.38262</td>\n",
       "      <td>-133.71211</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH15</td>\n",
       "      <td>5.45</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf/Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.498903</td>\n",
       "      <td>-0.938394</td>\n",
       "      <td>0.401360</td>\n",
       "      <td>-0.318997</td>\n",
       "      <td>0.736292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>68.38262</td>\n",
       "      <td>-133.71211</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH15</td>\n",
       "      <td>8.65</td>\n",
       "      <td>1</td>\n",
       "      <td>Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>9.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.498903</td>\n",
       "      <td>-0.938394</td>\n",
       "      <td>1.261961</td>\n",
       "      <td>-0.318997</td>\n",
       "      <td>0.736292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>ORGANICS</td>\n",
       "      <td>Organics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494226</td>\n",
       "      <td>-0.927509</td>\n",
       "      <td>-1.050903</td>\n",
       "      <td>-0.318997</td>\n",
       "      <td>0.736292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494226</td>\n",
       "      <td>-0.927509</td>\n",
       "      <td>-0.970222</td>\n",
       "      <td>-0.318997</td>\n",
       "      <td>0.736292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>68.38386</td>\n",
       "      <td>-133.70967</td>\n",
       "      <td>2013-04-27T00:00:00Z</td>\n",
       "      <td>W14103137-S6-BH16</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1</td>\n",
       "      <td>Nbn</td>\n",
       "      <td>No visible ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.494226</td>\n",
       "      <td>-0.927509</td>\n",
       "      <td>0.239998</td>\n",
       "      <td>-0.318997</td>\n",
       "      <td>0.736292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude  longitude                  time           borehole  depth  \\\n",
       "2832  68.38262 -133.71211  2013-04-27T00:00:00Z  W14103137-S6-BH15   5.45   \n",
       "2833  68.38262 -133.71211  2013-04-27T00:00:00Z  W14103137-S6-BH15   8.65   \n",
       "2834  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   0.05   \n",
       "2835  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   0.35   \n",
       "2836  68.38386 -133.70967  2013-04-27T00:00:00Z  W14103137-S6-BH16   4.85   \n",
       "\n",
       "      frozen cryostructures     visible_ice ASTM_2488 materials  ...  \\\n",
       "2832       1         Nf/Nbn  No visible ice       NaN      Till  ...   \n",
       "2833       1            Nbn  No visible ice       NaN      Till  ...   \n",
       "2834       0            NaN  No visible ice  ORGANICS  Organics  ...   \n",
       "2835       0            NaN  No visible ice       NaN      Till  ...   \n",
       "2836       1            Nbn  No visible ice       NaN      Till  ...   \n",
       "\n",
       "      top_of_interval  bottom_of_interval  month  year  month_cyclic  \\\n",
       "2832              2.7                 8.2      4  2013             4   \n",
       "2833              8.2                 9.1      4  2013             4   \n",
       "2834              0.0                 0.1      4  2013             4   \n",
       "2835              0.1                 0.6      4  2013             4   \n",
       "2836              0.6                 9.1      4  2013             4   \n",
       "\n",
       "      lat_norm  lng_norm  depth_norm  year_norm  month_cyclic_norm  \n",
       "2832 -1.498903 -0.938394    0.401360  -0.318997           0.736292  \n",
       "2833 -1.498903 -0.938394    1.261961  -0.318997           0.736292  \n",
       "2834 -1.494226 -0.927509   -1.050903  -0.318997           0.736292  \n",
       "2835 -1.494226 -0.927509   -0.970222  -0.318997           0.736292  \n",
       "2836 -1.494226 -0.927509    0.239998  -0.318997           0.736292  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.dropna(subset=['visible_ice'])\n",
    "df2['visible_ice'].replace(['None'], 'No visible ice', regex=True, inplace=True)\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0223c99-9d82-4efb-92d3-f4281a9897c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check None values have been replaced\n",
    "len(df2[df2['visible_ice'] == 'None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e75fb7-eae7-46ae-b952-38c585925846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1796\n",
       "0     956\n",
       "Name: No visible ice, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visible_ice = pd.get_dummies(df2.visible_ice)\n",
    "bin_visible_ice = (~visible_ice['No visible ice'].astype('bool')).astype('int')\n",
    "bin_visible_ice.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c029bb47-2a0f-4d54-be1a-e00630ece3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-697191a84568>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['visible_ice'] = bin_visible_ice\n"
     ]
    }
   ],
   "source": [
    "df2['visible_ice'] = bin_visible_ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "830c0d00-3256-4107-a9d6-0b3026a7f95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>borehole</th>\n",
       "      <th>depth</th>\n",
       "      <th>frozen</th>\n",
       "      <th>cryostructures</th>\n",
       "      <th>visible_ice</th>\n",
       "      <th>ASTM_2488</th>\n",
       "      <th>materials</th>\n",
       "      <th>...</th>\n",
       "      <th>top_of_interval</th>\n",
       "      <th>bottom_of_interval</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_cyclic</th>\n",
       "      <th>lat_norm</th>\n",
       "      <th>lng_norm</th>\n",
       "      <th>depth_norm</th>\n",
       "      <th>year_norm</th>\n",
       "      <th>month_cyclic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ICE</td>\n",
       "      <td>Ice</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>-0.835753</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>0</td>\n",
       "      <td>SW-SM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>-0.553369</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.16162</td>\n",
       "      <td>-133.08682</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-10</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>0</td>\n",
       "      <td>GW-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.439692</td>\n",
       "      <td>1.851129</td>\n",
       "      <td>0.387914</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.16105</td>\n",
       "      <td>-133.08880</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>0</td>\n",
       "      <td>GP-GM</td>\n",
       "      <td>Coarse till</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.437542</td>\n",
       "      <td>1.842296</td>\n",
       "      <td>-0.741625</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69.16105</td>\n",
       "      <td>-133.08880</td>\n",
       "      <td>2012-03-21T00:00:00Z</td>\n",
       "      <td>0170-1-12</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1</td>\n",
       "      <td>Nf</td>\n",
       "      <td>0</td>\n",
       "      <td>SM</td>\n",
       "      <td>Sand</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>1.437542</td>\n",
       "      <td>1.842296</td>\n",
       "      <td>-0.002046</td>\n",
       "      <td>-1.164786</td>\n",
       "      <td>-1.225079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude                  time   borehole  depth  frozen  \\\n",
       "1  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   0.85       1   \n",
       "2  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   1.90       1   \n",
       "3  69.16162 -133.08682  2012-03-21T00:00:00Z  0170-1-10   5.40       1   \n",
       "4  69.16105 -133.08880  2012-03-21T00:00:00Z  0170-1-12   1.20       1   \n",
       "5  69.16105 -133.08880  2012-03-21T00:00:00Z  0170-1-12   3.95       1   \n",
       "\n",
       "  cryostructures  visible_ice ASTM_2488    materials  ...  top_of_interval  \\\n",
       "1            NaN            1       ICE          Ice  ...              0.3   \n",
       "2             Nf            0     SW-SM  Coarse till  ...              1.4   \n",
       "3             Nf            0     GW-GM  Coarse till  ...              2.4   \n",
       "4             Nf            0     GP-GM  Coarse till  ...              0.0   \n",
       "5             Nf            0        SM         Sand  ...              2.4   \n",
       "\n",
       "   bottom_of_interval  month  year  month_cyclic  lat_norm  lng_norm  \\\n",
       "1                 1.4      3  2012             3  1.439692  1.851129   \n",
       "2                 2.4      3  2012             3  1.439692  1.851129   \n",
       "3                 8.4      3  2012             3  1.439692  1.851129   \n",
       "4                 2.4      3  2012             3  1.437542  1.842296   \n",
       "5                 5.5      3  2012             3  1.437542  1.842296   \n",
       "\n",
       "   depth_norm  year_norm  month_cyclic_norm  \n",
       "1   -0.835753  -1.164786          -1.225079  \n",
       "2   -0.553369  -1.164786          -1.225079  \n",
       "3    0.387914  -1.164786          -1.225079  \n",
       "4   -0.741625  -1.164786          -1.225079  \n",
       "5   -0.002046  -1.164786          -1.225079  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1837f5df-ee84-4d0d-9942-14b948a37934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.dropna(subset=['materials'])\n",
    "df3['materials'].replace(['ICE'], 'Ice', regex=True, inplace=True)\n",
    "df3['materials'].replace(['ice'], 'Ice', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba8d5fb-1a64-4fea-a69a-20b744e097a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_materials = pd.get_dummies(df3.materials)\n",
    "df3['material_ice'] = dm_materials['Ice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75c3c456-b700-46cd-9a6d-52872ba14aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Geo90Dataset(Dataset):\n",
    "    def __init__(self, data_root, df, base_lat, base_lng, chip_size=32, label_name = 'frozen'):\n",
    "        \n",
    "        self.base_lat = base_lat\n",
    "        self.base_lng = base_lng\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        self.chip_size = chip_size\n",
    "        self.label_name = label_name\n",
    "        \n",
    "        self.trans = transforms.ToTensor()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.preloaded = torch.ones(self.n_channels, 6000, 6000)\n",
    "        \n",
    "        for i, file in enumerate(os.listdir(data_root)):\n",
    "            # name = file.split('_')[0]\n",
    "            # print(name)\n",
    "            self.preloaded[i] = self.trans(Image.open(data_root + os.path.sep + file))\n",
    "        \n",
    "        print('Dataset initialized')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        bh_id = row.at['borehole']\n",
    "        lat = row.at['latitude']\n",
    "        lng = row.at['longitude']\n",
    "        \n",
    "\n",
    "        pixel_len = 5/6000\n",
    "        \n",
    "\n",
    "        lat_index_start = np.round((self.base_lat - lat) / pixel_len - self.chip_size/2).astype(int)\n",
    "        lat_index_end = lat_index_start + self.chip_size\n",
    "        \n",
    "        lng_index_start = np.round((lng - self.base_lng) / pixel_len - self.chip_size/2).astype(int)\n",
    "        lng_index_end = lng_index_start + self.chip_size\n",
    "        \n",
    "        image = self.preloaded[:, lat_index_start:lat_index_end,lng_index_start:lng_index_end]\n",
    "        \n",
    "        \n",
    "        # surface = torch.tensor(row.filter(['depth'])).float()\n",
    "        surface = torch.tensor(row.filter(['depth_norm', 'month_cyclic_norm', 'lat_norm', 'lng_norm', 'year_norm'])).float()\n",
    "        \n",
    "        frozen = torch.tensor(row.at['frozen']).float()\n",
    "        \n",
    "        visible_ice = torch.tensor(row.at['visible_ice']).float()\n",
    "        \n",
    "        # material_ice = torch.tensor(row.at['material_ice']).float()\n",
    "        \n",
    "        return {'image': image, 'surface_data': surface, 'frozen': frozen,  'visible_ice': visible_ice} #'material_ice': material_ice}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b0df9-d51d-4bb3-9658-7a49e95bc6a6",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f46b0-7537-4ceb-8f96-263c007e97fd",
   "metadata": {},
   "source": [
    "## FiLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "214c4911-bcfb-43a2-b41b-b01949690b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_film_params = hidden_width * hidden_nblocks * 2\n",
    "\n",
    "# generator = resnet18(n_channels, n_film_params)\n",
    "\n",
    "def train_model(trainloader, testloader, label_name, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    # loss: binary cross entropy\n",
    "\n",
    "    generator = models.resnet18()\n",
    "    generator.fc = nn.Linear(512, n_film_params)\n",
    "    generator.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    gen_model = generator\n",
    "\n",
    "    # print(gen_model)\n",
    "\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    net_model = mlp(input_size[0],1, hidden_width, hidden_nblocks).to(device)\n",
    "    \n",
    "    gen_optimizer = torch.optim.Adam(gen_model.parameters(), weight_decay = L2_param)\n",
    "    net_optimizer = torch.optim.Adam(net_model.parameters(), weight_decay = L2_param)\n",
    "    \n",
    "    gen_model.to(device)\n",
    "    net_model.to(device)\n",
    "\n",
    "    # --------- check back propagation ----------- -\n",
    "    # net_model.fc1.weight.register_hook(lambda x: print('grad accumulated in mlp fc1'))\n",
    "    # gen_first_layer = gen_model.encoder.blocks[0].blocks[0].blocks[0].conv\n",
    "    # gen_first_layer.weight.register_hook(lambda x: print('grad accumulated in resnet first layer'))\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        # ------------ train -----------------\n",
    "        gen_model.train()\n",
    "        net_model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            \n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "\n",
    "            gen_optimizer.zero_grad()\n",
    "            net_optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            gen_optimizer.step()\n",
    "            net_optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        gen_model.eval()\n",
    "        net_model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # TODO: exammine film_params gradients / readup pytorch\n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "\n",
    "        torch.save(gen_model.state_dict(), os.path.join('mlp-resnet-models/', 'gen-epoch-{}.pt'.format(epoch+1)))\n",
    "        torch.save(net_model.state_dict(), os.path.join('mlp-resnet-models/', 'net-epoch-{}.pt'.format(epoch+1)))\n",
    "\n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "## Test model\n",
    "\n",
    "def test_model(epoch_loss, label_name, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    generator = models.resnet18()\n",
    "    generator.fc = nn.Linear(512, n_film_params)\n",
    "    generator.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    gen_model = generator\n",
    "\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    net_model = mlp(input_size[0],1, hidden_width, hidden_nblocks)\n",
    "\n",
    "    gen_model.load_state_dict(torch.load('mlp-resnet-models/gen-epoch-{}.pt'.format(ind+1)))\n",
    "    net_model.load_state_dict(torch.load('mlp-resnet-models/net-epoch-{}.pt'.format(ind+1)))\n",
    "    \n",
    "    gen_model.to(device)\n",
    "    net_model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    gen_model.eval()\n",
    "    net_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            images, surface_data, labels = data['image'].to(device), data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            gen_params = gen_model(images)\n",
    "            predicted = net_model(surface_data, gen_params)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "\n",
    "            predicted = torch.round(predicted)\n",
    "            # print(predicted.shape)\n",
    "            lb = labels.tolist()\n",
    "            pr = predicted.tolist()\n",
    "            y_test.extend(lb)\n",
    "            y_pred.extend(pr)\n",
    "    \n",
    "    arr_accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "    return arr_accuracy, scores\n",
    "\n",
    "\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "#     print(classification_report(y_test,y_pred))\n",
    "#     print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "## Pure MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8b5b0-1039-4c76-93e4-0f90b60d87be",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43c02928-f9c5-4b2a-8ef7-d8a2de4a4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_pure(nn.Module):\n",
    "        def __init__(self, input_size, output_size):\n",
    "            super(mlp_pure, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.hidden_size = hidden_width\n",
    "            self.hidden_nblocks = hidden_nblocks\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size,self.hidden_size)\n",
    "            self.fc3 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.sigmoid = torch.nn.Sigmoid() ## sigmoid for multi-label, softmax for multi-class (mutually exclusive)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            # print(out.shape)\n",
    "            \n",
    "            for i in range(self.hidden_nblocks):\n",
    "                out = self.fc2(out)\n",
    "                out = self.relu(out)\n",
    "            \n",
    "            out = self.fc3(out)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "def train_mlp(trainloader, testloader, label_name, print_epochs = False, loss_fn = torch.nn.BCELoss()):\n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    surface_model = mlp_pure(input_size[0],1)\n",
    "    \n",
    "    surface_model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(surface_model.parameters(), weight_decay = L2_param)\n",
    "\n",
    "    epoch_loss = np.zeros([train_max_epoch, 2])\n",
    "    for epoch in range(train_max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        surface_model.train()\n",
    "        running_loss_sum = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # loop over each sample\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            predicted = surface_model(surface_data)\n",
    "\n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_sum += loss.item()\n",
    "\n",
    "        # ----------- get validation loss for current epoch --------------\n",
    "        surface_model.eval()\n",
    "        validation_loss_sum = 0.0\n",
    "        for i, data in enumerate(testloader, 0): # loop over each sample\n",
    "\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            predicted = surface_model(surface_data)\n",
    "\n",
    "            loss = loss_fn(predicted.squeeze(), labels)\n",
    "\n",
    "            validation_loss_sum += loss.item()\n",
    "\n",
    "        # ---------------- print statistics ------------------------\n",
    "\n",
    "        running_loss = running_loss_sum / len(trainloader)\n",
    "        validation_loss = validation_loss_sum / len(testloader)\n",
    "        epoch_loss[epoch, :] =  [running_loss, validation_loss]\n",
    "        \n",
    "        if print_epochs:\n",
    "            print('epoch %2d: running loss: %.5f, validation loss: %.5f' %\n",
    "                          (epoch + 1, running_loss, validation_loss))\n",
    "\n",
    "        torch.save(surface_model.state_dict(), os.path.join('mlp-models/', 'epoch-{}.pt'.format(epoch+1)))\n",
    "    \n",
    "    if print_epochs:\n",
    "        print('Finished Training')\n",
    "        \n",
    "    return epoch_loss\n",
    "        \n",
    "def test_mlp(epoch_loss, label_name, print_model_epoch = False):\n",
    "    \n",
    "    # ------ select model ---------\n",
    "    ind = np.argmin(epoch_loss[:, 1])\n",
    "    \n",
    "    input_size = list(full_dataset[0]['surface_data'].size())\n",
    "    \n",
    "    surface_model = mlp_pure(input_size[0],1)\n",
    "    surface_model.load_state_dict(torch.load('mlp-models/epoch-{}.pt'.format(ind+1)))\n",
    "    \n",
    "    surface_model.to(device)\n",
    "    \n",
    "    if print_model_epoch:\n",
    "        print(\"epoch {} model selected\".format(ind+1))\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    surface_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            surface_data, labels = data['surface_data'].to(device), data[label_name].to(device)\n",
    "\n",
    "            # y_test.append(label.numpy().list())\n",
    "            # print(label.shape)\n",
    "            # print(images.shape)\n",
    "\n",
    "            output = surface_model(surface_data)\n",
    "            predicted = torch.round(output)\n",
    "            # print(predicted.shape)\n",
    "            lb = labels.tolist()\n",
    "            pr = predicted.tolist()\n",
    "            y_test.extend(lb)\n",
    "            y_pred.extend(pr)\n",
    "    \n",
    "    arr_accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "    return arr_accuracy, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cfb6b-c5ca-4fdd-81e9-571685a86522",
   "metadata": {},
   "source": [
    "## Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51bacf58-7320-49da-8f08-aa180ed67446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized\n"
     ]
    }
   ],
   "source": [
    "base_lat = 70\n",
    "base_lng = -135\n",
    "\n",
    "loaded_dataset = Geo90Dataset(data_root, df2, base_lat, base_lng, chip_size)\n",
    "\n",
    "# image = full_dataset[0]['image']\n",
    "# n_channels = list(image.shape)[0]\n",
    "\n",
    "# for data in full_dataset:\n",
    "#     image = data['image']\n",
    "#     for i in range(n_channels):\n",
    "#         channel = image[i]\n",
    "#         ind = (channel == -9999)\n",
    "# #         mean_val = torch.mean(channel[~ind])\n",
    "#         channel[ind] = 0\n",
    "#         data['image'][i] = channel\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44629f3-5888-4eb0-8f54-cfd63f455c54",
   "metadata": {},
   "source": [
    "### Discard samples with invalid values in image chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc861435-97ab-4095-9760-4b797477daa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752\n",
      "2717\n"
     ]
    }
   ],
   "source": [
    "valid_ind = []\n",
    "for i, data in enumerate(loaded_dataset):\n",
    "    image = data['image']\n",
    "    ind  = (image == -9999)\n",
    "    if ~torch.any(ind):\n",
    "        valid_ind.append(i)\n",
    "\n",
    "full_dataset = torch.utils.data.Subset(loaded_dataset, valid_ind)\n",
    "\n",
    "\n",
    "print(len(loaded_dataset))\n",
    "print(len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba299154-57ac-4e30-bf2e-74e41e3fc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# batchsize can cause error when last leftover batchsize is 1, batchnorm cannot function on 1 sample data\n",
    "batchsize = 20\n",
    "while(train_size % batchsize == 1):\n",
    "    batchsize+=1\n",
    "print(batchsize)\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(full_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb6971-4f9e-409e-b8ee-9ebe27a3cfae",
   "metadata": {},
   "source": [
    "## Scale of image chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e37318c4-ff02-48e0-aa8c-c5a708eb0c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[ 0.7546],\n",
      "        [ 0.7777],\n",
      "        [ 0.7408],\n",
      "        ...,\n",
      "        [-0.9796],\n",
      "        [-0.9590],\n",
      "        [-0.8945]])\n"
     ]
    }
   ],
   "source": [
    "image = full_dataset[0]['image']\n",
    "n_samples = len(train_data)\n",
    "n_channels = list(image.shape)[0]\n",
    "\n",
    "scalers = []\n",
    "for i in range(n_channels):\n",
    "    print(i)\n",
    "    scaler = StandardScaler()\n",
    "    X = torch.empty((n_samples, chip_size, chip_size))\n",
    "    \n",
    "    for j, data in enumerate(train_data):\n",
    "        #print(data['image'][i].shape)\n",
    "        # print(X[j].shape)\n",
    "        X[j] = data['image'][i]\n",
    "    X = torch.reshape(X, (-1,1))\n",
    "    print(X)\n",
    "    break;\n",
    "    scaler.fit(X)\n",
    "    scalers.append(scaler)\n",
    "    \n",
    "    def scale_data(subset):\n",
    "        for data in subset:\n",
    "            X = data['image'][i]\n",
    "            X_flat = torch.reshape(X, (-1,1))\n",
    "            \n",
    "            X_trans = scaler.transform(X_flat)\n",
    "            data['image'][i] = torch.reshape(torch.Tensor(X_trans), (chip_size, chip_size))\n",
    "    \n",
    "    scale_data(train_data)\n",
    "    scale_data(test_data)\n",
    "    print(\"Channel {} scaled.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a6d03-a8b0-4c0f-b562-415f30deb0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: running loss: 0.65230, validation loss: 0.65717\n",
      "epoch  2: running loss: 0.63635, validation loss: 0.65024\n",
      "epoch  3: running loss: 0.63188, validation loss: 0.64193\n",
      "epoch  4: running loss: 0.62463, validation loss: 0.64702\n",
      "epoch  5: running loss: 0.60815, validation loss: 0.62442\n",
      "epoch  6: running loss: 0.59748, validation loss: 0.62084\n",
      "epoch  7: running loss: 0.58801, validation loss: 0.61294\n",
      "epoch  8: running loss: 0.58002, validation loss: 0.62611\n",
      "epoch  9: running loss: 0.57523, validation loss: 0.61230\n",
      "epoch 10: running loss: 0.57086, validation loss: 0.61014\n",
      "epoch 11: running loss: 0.56264, validation loss: 0.61533\n",
      "epoch 12: running loss: 0.56602, validation loss: 0.60621\n",
      "epoch 13: running loss: 0.56487, validation loss: 0.59364\n",
      "epoch 14: running loss: 0.56555, validation loss: 0.59972\n",
      "epoch 15: running loss: 0.55935, validation loss: 0.59735\n",
      "epoch 16: running loss: 0.55772, validation loss: 0.61046\n",
      "epoch 17: running loss: 0.55494, validation loss: 0.60492\n",
      "epoch 18: running loss: 0.55383, validation loss: 0.61133\n",
      "epoch 19: running loss: 0.55095, validation loss: 0.62241\n",
      "epoch 20: running loss: 0.54933, validation loss: 0.63908\n",
      "epoch 21: running loss: 0.55180, validation loss: 0.59687\n",
      "epoch 22: running loss: 0.55150, validation loss: 0.60591\n",
      "epoch 23: running loss: 0.55006, validation loss: 0.59084\n",
      "epoch 24: running loss: 0.54861, validation loss: 0.61719\n",
      "epoch 25: running loss: 0.54760, validation loss: 0.60585\n",
      "epoch 26: running loss: 0.54876, validation loss: 0.59599\n",
      "epoch 27: running loss: 0.54957, validation loss: 0.58949\n",
      "epoch 28: running loss: 0.54630, validation loss: 0.58358\n",
      "epoch 29: running loss: 0.54800, validation loss: 0.58373\n",
      "epoch 30: running loss: 0.54323, validation loss: 0.59902\n",
      "epoch 31: running loss: 0.54391, validation loss: 0.59374\n",
      "epoch 32: running loss: 0.54440, validation loss: 0.60010\n",
      "epoch 33: running loss: 0.54253, validation loss: 0.60721\n",
      "epoch 34: running loss: 0.54306, validation loss: 0.59073\n",
      "epoch 35: running loss: 0.54223, validation loss: 0.61623\n",
      "epoch 36: running loss: 0.54629, validation loss: 0.58690\n",
      "epoch 37: running loss: 0.53858, validation loss: 0.58410\n",
      "epoch 38: running loss: 0.53729, validation loss: 0.59243\n",
      "epoch 39: running loss: 0.54147, validation loss: 0.58511\n",
      "epoch 40: running loss: 0.53709, validation loss: 0.57566\n",
      "epoch 41: running loss: 0.53727, validation loss: 0.60566\n",
      "epoch 42: running loss: 0.53690, validation loss: 0.60360\n",
      "epoch 43: running loss: 0.54737, validation loss: 0.58678\n",
      "epoch 44: running loss: 0.53196, validation loss: 0.59113\n",
      "epoch 45: running loss: 0.53993, validation loss: 0.58917\n",
      "epoch 46: running loss: 0.53896, validation loss: 0.59286\n",
      "epoch 47: running loss: 0.53310, validation loss: 0.59098\n",
      "epoch 48: running loss: 0.53306, validation loss: 0.58023\n",
      "epoch 49: running loss: 0.53247, validation loss: 0.59970\n",
      "epoch 50: running loss: 0.53161, validation loss: 0.58283\n",
      "Finished Training\n",
      "epoch 40 model selected\n",
      "iteration 1 elapsed time: 77.53769540786743, accuracy : 0.7077205882352942\n",
      "epoch  1: running loss: 0.65337, validation loss: 0.66475\n",
      "epoch  2: running loss: 0.63226, validation loss: 0.64101\n",
      "epoch  3: running loss: 0.61837, validation loss: 0.64385\n",
      "epoch  4: running loss: 0.60515, validation loss: 0.61955\n",
      "epoch  5: running loss: 0.58958, validation loss: 0.61219\n",
      "epoch  6: running loss: 0.58656, validation loss: 0.61618\n",
      "epoch  7: running loss: 0.58099, validation loss: 0.60779\n",
      "epoch  8: running loss: 0.57556, validation loss: 0.60142\n",
      "epoch  9: running loss: 0.56796, validation loss: 0.63633\n",
      "epoch 10: running loss: 0.57902, validation loss: 0.60796\n",
      "epoch 11: running loss: 0.56522, validation loss: 0.63741\n",
      "epoch 12: running loss: 0.57077, validation loss: 0.60517\n",
      "epoch 13: running loss: 0.56277, validation loss: 0.59722\n",
      "epoch 14: running loss: 0.55695, validation loss: 0.63084\n",
      "epoch 15: running loss: 0.55803, validation loss: 0.60800\n",
      "epoch 16: running loss: 0.56244, validation loss: 0.59149\n",
      "epoch 17: running loss: 0.55702, validation loss: 0.62081\n",
      "epoch 18: running loss: 0.55519, validation loss: 0.61902\n",
      "epoch 19: running loss: 0.55567, validation loss: 0.61686\n",
      "epoch 20: running loss: 0.55488, validation loss: 0.60490\n",
      "epoch 21: running loss: 0.54967, validation loss: 0.61813\n",
      "epoch 22: running loss: 0.55615, validation loss: 0.59222\n",
      "epoch 23: running loss: 0.55284, validation loss: 0.61497\n",
      "epoch 24: running loss: 0.54908, validation loss: 0.58961\n",
      "epoch 25: running loss: 0.54585, validation loss: 0.60511\n",
      "epoch 26: running loss: 0.55638, validation loss: 0.58632\n",
      "epoch 27: running loss: 0.54956, validation loss: 0.59805\n",
      "epoch 28: running loss: 0.54660, validation loss: 0.61024\n",
      "epoch 29: running loss: 0.54820, validation loss: 0.60468\n",
      "epoch 30: running loss: 0.54945, validation loss: 0.58864\n",
      "epoch 31: running loss: 0.54565, validation loss: 0.58902\n",
      "epoch 32: running loss: 0.54443, validation loss: 0.59581\n",
      "epoch 33: running loss: 0.54398, validation loss: 0.59025\n",
      "epoch 34: running loss: 0.54422, validation loss: 0.59351\n",
      "epoch 35: running loss: 0.54365, validation loss: 0.57968\n",
      "epoch 36: running loss: 0.54299, validation loss: 0.59190\n",
      "epoch 37: running loss: 0.54548, validation loss: 0.58440\n",
      "epoch 38: running loss: 0.54107, validation loss: 0.60619\n",
      "epoch 39: running loss: 0.54546, validation loss: 0.59174\n",
      "epoch 40: running loss: 0.53772, validation loss: 0.58944\n",
      "epoch 41: running loss: 0.54117, validation loss: 0.59792\n",
      "epoch 42: running loss: 0.53775, validation loss: 0.58681\n",
      "epoch 43: running loss: 0.54171, validation loss: 0.57432\n",
      "epoch 44: running loss: 0.54014, validation loss: 0.58697\n",
      "epoch 45: running loss: 0.53855, validation loss: 0.60391\n",
      "epoch 46: running loss: 0.53918, validation loss: 0.58604\n",
      "epoch 47: running loss: 0.53493, validation loss: 0.59079\n",
      "epoch 48: running loss: 0.53598, validation loss: 0.58930\n",
      "epoch 49: running loss: 0.53625, validation loss: 0.59069\n",
      "epoch 50: running loss: 0.53456, validation loss: 0.58898\n",
      "Finished Training\n",
      "epoch 43 model selected\n",
      "iteration 2 elapsed time: 74.83065414428711, accuracy : 0.7150735294117647\n",
      "epoch  1: running loss: 0.65743, validation loss: 0.65599\n",
      "epoch  2: running loss: 0.63625, validation loss: 0.65008\n",
      "epoch  3: running loss: 0.61987, validation loss: 0.63972\n",
      "epoch  4: running loss: 0.60232, validation loss: 0.62293\n",
      "epoch  5: running loss: 0.59129, validation loss: 0.62292\n",
      "epoch  6: running loss: 0.58246, validation loss: 0.60564\n",
      "epoch  7: running loss: 0.57678, validation loss: 0.60578\n",
      "epoch  8: running loss: 0.57152, validation loss: 0.60588\n",
      "epoch  9: running loss: 0.56421, validation loss: 0.60791\n",
      "epoch 10: running loss: 0.56171, validation loss: 0.60998\n",
      "epoch 11: running loss: 0.55948, validation loss: 0.60147\n",
      "epoch 12: running loss: 0.55534, validation loss: 0.60152\n",
      "epoch 13: running loss: 0.55208, validation loss: 0.61007\n",
      "epoch 14: running loss: 0.55421, validation loss: 0.60567\n",
      "epoch 15: running loss: 0.54873, validation loss: 0.61137\n",
      "epoch 16: running loss: 0.55227, validation loss: 0.59581\n",
      "epoch 17: running loss: 0.54697, validation loss: 0.60970\n",
      "epoch 18: running loss: 0.54585, validation loss: 0.59499\n",
      "epoch 19: running loss: 0.54271, validation loss: 0.60212\n",
      "epoch 20: running loss: 0.54097, validation loss: 0.59693\n",
      "epoch 21: running loss: 0.54605, validation loss: 0.59272\n",
      "epoch 22: running loss: 0.54129, validation loss: 0.58145\n",
      "epoch 23: running loss: 0.53961, validation loss: 0.58783\n",
      "epoch 24: running loss: 0.54177, validation loss: 0.60456\n",
      "epoch 25: running loss: 0.53513, validation loss: 0.62210\n",
      "epoch 26: running loss: 0.53545, validation loss: 0.58276\n",
      "epoch 27: running loss: 0.53352, validation loss: 0.59958\n",
      "epoch 28: running loss: 0.53664, validation loss: 0.60031\n",
      "epoch 29: running loss: 0.53418, validation loss: 0.58160\n",
      "epoch 30: running loss: 0.53372, validation loss: 0.58269\n",
      "epoch 31: running loss: 0.53128, validation loss: 0.58997\n",
      "epoch 32: running loss: 0.52680, validation loss: 0.59401\n",
      "epoch 33: running loss: 0.52972, validation loss: 0.57298\n",
      "epoch 34: running loss: 0.53599, validation loss: 0.57663\n",
      "epoch 35: running loss: 0.52836, validation loss: 0.59065\n",
      "epoch 36: running loss: 0.52600, validation loss: 0.59270\n",
      "epoch 37: running loss: 0.52851, validation loss: 0.59022\n",
      "epoch 38: running loss: 0.52496, validation loss: 0.58998\n",
      "epoch 39: running loss: 0.52499, validation loss: 0.59566\n",
      "epoch 40: running loss: 0.52282, validation loss: 0.57222\n",
      "epoch 41: running loss: 0.52583, validation loss: 0.58318\n",
      "epoch 42: running loss: 0.51991, validation loss: 0.56801\n",
      "epoch 43: running loss: 0.52358, validation loss: 0.58222\n",
      "epoch 44: running loss: 0.52447, validation loss: 0.56702\n",
      "epoch 45: running loss: 0.52321, validation loss: 0.56381\n",
      "epoch 46: running loss: 0.51922, validation loss: 0.59071\n",
      "epoch 47: running loss: 0.51952, validation loss: 0.56682\n",
      "epoch 48: running loss: 0.51960, validation loss: 0.58572\n",
      "epoch 49: running loss: 0.51801, validation loss: 0.56491\n",
      "epoch 50: running loss: 0.52030, validation loss: 0.57377\n",
      "Finished Training\n",
      "epoch 45 model selected\n",
      "iteration 3 elapsed time: 76.16107368469238, accuracy : 0.7279411764705882\n",
      "epoch  1: running loss: 0.66100, validation loss: 0.65596\n",
      "epoch  2: running loss: 0.62878, validation loss: 0.63796\n",
      "epoch  3: running loss: 0.60917, validation loss: 0.63907\n",
      "epoch  4: running loss: 0.59681, validation loss: 0.61413\n",
      "epoch  5: running loss: 0.58377, validation loss: 0.62785\n",
      "epoch  6: running loss: 0.57680, validation loss: 0.61305\n",
      "epoch  7: running loss: 0.56896, validation loss: 0.60079\n",
      "epoch  8: running loss: 0.56262, validation loss: 0.61641\n",
      "epoch  9: running loss: 0.56355, validation loss: 0.61324\n",
      "epoch 10: running loss: 0.55835, validation loss: 0.60124\n",
      "epoch 11: running loss: 0.55418, validation loss: 0.59401\n",
      "epoch 12: running loss: 0.55156, validation loss: 0.59017\n",
      "epoch 13: running loss: 0.55764, validation loss: 0.60741\n",
      "epoch 14: running loss: 0.55043, validation loss: 0.59490\n",
      "epoch 15: running loss: 0.54886, validation loss: 0.58463\n",
      "epoch 16: running loss: 0.54480, validation loss: 0.59195\n",
      "epoch 17: running loss: 0.54584, validation loss: 0.59098\n",
      "epoch 18: running loss: 0.54325, validation loss: 0.58294\n",
      "epoch 19: running loss: 0.54192, validation loss: 0.59203\n",
      "epoch 20: running loss: 0.54122, validation loss: 0.58471\n",
      "epoch 21: running loss: 0.54251, validation loss: 0.58790\n",
      "epoch 22: running loss: 0.53801, validation loss: 0.58328\n",
      "epoch 23: running loss: 0.53529, validation loss: 0.59221\n",
      "epoch 24: running loss: 0.53820, validation loss: 0.57495\n",
      "epoch 25: running loss: 0.53757, validation loss: 0.57220\n",
      "epoch 26: running loss: 0.53287, validation loss: 0.59810\n",
      "epoch 27: running loss: 0.53560, validation loss: 0.60509\n",
      "epoch 28: running loss: 0.53228, validation loss: 0.59425\n",
      "epoch 29: running loss: 0.53845, validation loss: 0.59732\n",
      "epoch 30: running loss: 0.53505, validation loss: 0.56820\n",
      "epoch 31: running loss: 0.53192, validation loss: 0.57818\n",
      "epoch 32: running loss: 0.52905, validation loss: 0.58653\n",
      "epoch 33: running loss: 0.52864, validation loss: 0.58592\n",
      "epoch 34: running loss: 0.52937, validation loss: 0.56349\n",
      "epoch 35: running loss: 0.52707, validation loss: 0.58042\n",
      "epoch 36: running loss: 0.52791, validation loss: 0.57113\n",
      "epoch 37: running loss: 0.52277, validation loss: 0.57680\n",
      "epoch 38: running loss: 0.52323, validation loss: 0.58600\n",
      "epoch 39: running loss: 0.52681, validation loss: 0.59895\n",
      "epoch 40: running loss: 0.52786, validation loss: 0.57249\n",
      "epoch 41: running loss: 0.52431, validation loss: 0.58827\n",
      "epoch 42: running loss: 0.52107, validation loss: 0.59231\n",
      "epoch 43: running loss: 0.52270, validation loss: 0.59023\n",
      "epoch 44: running loss: 0.52077, validation loss: 0.57846\n",
      "epoch 45: running loss: 0.52127, validation loss: 0.56109\n",
      "epoch 46: running loss: 0.51469, validation loss: 0.56917\n",
      "epoch 47: running loss: 0.52186, validation loss: 0.56733\n",
      "epoch 48: running loss: 0.52019, validation loss: 0.57280\n",
      "epoch 49: running loss: 0.51595, validation loss: 0.56719\n",
      "epoch 50: running loss: 0.51629, validation loss: 0.57005\n",
      "Finished Training\n",
      "epoch 45 model selected\n",
      "iteration 4 elapsed time: 75.83367371559143, accuracy : 0.7242647058823529\n",
      "epoch  1: running loss: 0.64835, validation loss: 0.65797\n",
      "epoch  2: running loss: 0.63674, validation loss: 0.64782\n",
      "epoch  3: running loss: 0.62238, validation loss: 0.64142\n",
      "epoch  4: running loss: 0.60610, validation loss: 0.62882\n",
      "epoch  5: running loss: 0.59235, validation loss: 0.62156\n",
      "epoch  6: running loss: 0.58411, validation loss: 0.61519\n",
      "epoch  7: running loss: 0.57788, validation loss: 0.61927\n",
      "epoch  8: running loss: 0.57618, validation loss: 0.61213\n",
      "epoch  9: running loss: 0.56591, validation loss: 0.59825\n",
      "epoch 10: running loss: 0.56591, validation loss: 0.60213\n",
      "epoch 11: running loss: 0.56172, validation loss: 0.60510\n",
      "epoch 12: running loss: 0.56571, validation loss: 0.60183\n",
      "epoch 13: running loss: 0.56784, validation loss: 0.62167\n",
      "epoch 14: running loss: 0.55673, validation loss: 0.60566\n",
      "epoch 15: running loss: 0.55547, validation loss: 0.62007\n",
      "epoch 16: running loss: 0.55651, validation loss: 0.59951\n",
      "epoch 17: running loss: 0.55469, validation loss: 0.61128\n",
      "epoch 18: running loss: 0.55546, validation loss: 0.63083\n",
      "epoch 19: running loss: 0.55058, validation loss: 0.60739\n",
      "epoch 20: running loss: 0.55376, validation loss: 0.60253\n",
      "epoch 21: running loss: 0.54823, validation loss: 0.59662\n",
      "epoch 22: running loss: 0.55148, validation loss: 0.59626\n",
      "epoch 23: running loss: 0.54732, validation loss: 0.58984\n",
      "epoch 24: running loss: 0.54526, validation loss: 0.60623\n",
      "epoch 25: running loss: 0.54580, validation loss: 0.59790\n",
      "epoch 26: running loss: 0.54252, validation loss: 0.59637\n",
      "epoch 27: running loss: 0.54630, validation loss: 0.58851\n",
      "epoch 28: running loss: 0.54276, validation loss: 0.59978\n",
      "epoch 29: running loss: 0.54404, validation loss: 0.61525\n",
      "epoch 30: running loss: 0.54303, validation loss: 0.61725\n",
      "epoch 31: running loss: 0.54276, validation loss: 0.57575\n",
      "epoch 32: running loss: 0.53862, validation loss: 0.60313\n",
      "epoch 33: running loss: 0.53787, validation loss: 0.58258\n",
      "epoch 34: running loss: 0.53495, validation loss: 0.58839\n",
      "epoch 35: running loss: 0.53345, validation loss: 0.58792\n",
      "epoch 36: running loss: 0.53326, validation loss: 0.57506\n",
      "epoch 37: running loss: 0.53352, validation loss: 0.60353\n",
      "epoch 38: running loss: 0.53483, validation loss: 0.59393\n",
      "epoch 39: running loss: 0.52972, validation loss: 0.58076\n",
      "epoch 40: running loss: 0.52934, validation loss: 0.58544\n",
      "epoch 41: running loss: 0.53020, validation loss: 0.61136\n",
      "epoch 42: running loss: 0.53233, validation loss: 0.58621\n",
      "epoch 43: running loss: 0.52674, validation loss: 0.59407\n",
      "epoch 44: running loss: 0.52674, validation loss: 0.58718\n",
      "epoch 45: running loss: 0.53094, validation loss: 0.57691\n",
      "epoch 46: running loss: 0.52790, validation loss: 0.58152\n",
      "epoch 47: running loss: 0.52915, validation loss: 0.58510\n",
      "epoch 48: running loss: 0.52743, validation loss: 0.58767\n",
      "epoch 49: running loss: 0.52284, validation loss: 0.57121\n",
      "epoch 50: running loss: 0.52527, validation loss: 0.57952\n",
      "Finished Training\n",
      "epoch 49 model selected\n",
      "iteration 5 elapsed time: 75.16332411766052, accuracy : 0.7095588235294118\n",
      "epoch  1: running loss: 0.66406, validation loss: 0.64974\n",
      "epoch  2: running loss: 0.63563, validation loss: 0.65624\n",
      "epoch  3: running loss: 0.62246, validation loss: 0.63802\n",
      "epoch  4: running loss: 0.60562, validation loss: 0.63077\n",
      "epoch  5: running loss: 0.59136, validation loss: 0.61856\n",
      "epoch  6: running loss: 0.58164, validation loss: 0.60503\n",
      "epoch  7: running loss: 0.57775, validation loss: 0.60095\n",
      "epoch  8: running loss: 0.57327, validation loss: 0.60785\n",
      "epoch  9: running loss: 0.56760, validation loss: 0.59785\n",
      "epoch 10: running loss: 0.56455, validation loss: 0.59886\n",
      "epoch 11: running loss: 0.56281, validation loss: 0.60461\n",
      "epoch 12: running loss: 0.56560, validation loss: 0.59618\n",
      "epoch 13: running loss: 0.56070, validation loss: 0.62134\n",
      "epoch 14: running loss: 0.55562, validation loss: 0.60509\n",
      "epoch 15: running loss: 0.55468, validation loss: 0.60915\n",
      "epoch 16: running loss: 0.55176, validation loss: 0.58706\n",
      "epoch 17: running loss: 0.54855, validation loss: 0.58970\n",
      "epoch 18: running loss: 0.55035, validation loss: 0.59324\n",
      "epoch 19: running loss: 0.54737, validation loss: 0.59848\n",
      "epoch 20: running loss: 0.54747, validation loss: 0.60158\n",
      "epoch 21: running loss: 0.54700, validation loss: 0.58651\n",
      "epoch 22: running loss: 0.54632, validation loss: 0.58442\n",
      "epoch 23: running loss: 0.54192, validation loss: 0.61254\n",
      "epoch 24: running loss: 0.53729, validation loss: 0.58327\n",
      "epoch 25: running loss: 0.54359, validation loss: 0.59489\n",
      "epoch 26: running loss: 0.54103, validation loss: 0.59928\n",
      "epoch 27: running loss: 0.54185, validation loss: 0.57331\n",
      "epoch 28: running loss: 0.53652, validation loss: 0.58721\n",
      "epoch 29: running loss: 0.53806, validation loss: 0.58043\n",
      "epoch 30: running loss: 0.53522, validation loss: 0.59064\n",
      "epoch 31: running loss: 0.53462, validation loss: 0.58134\n",
      "epoch 32: running loss: 0.53762, validation loss: 0.56812\n",
      "epoch 33: running loss: 0.53306, validation loss: 0.59058\n",
      "epoch 34: running loss: 0.53267, validation loss: 0.60652\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 10\n",
    "results = np.zeros([max_iterations, 9])\n",
    "# trainloader, testloader = prepare_dataloader(full_dataset)\n",
    "\n",
    "labelName = 'visible_ice'\n",
    "\n",
    "for it in range(max_iterations):\n",
    "    start = time.time()\n",
    "    \n",
    "    # mlp\n",
    "    epoch_loss_mlp = train_mlp(trainloader,testloader, labelName, print_epochs = True)\n",
    "    acc, scores = test_mlp(epoch_loss_mlp, labelName, print_model_epoch = True)\n",
    "    \n",
    "    #     # ------- mlp-resnet film \n",
    "#     epoch_loss = train_model(trainloader, testloader, labelName, print_epochs=True)\n",
    "#     acc, scores = test_model(epoch_loss, labelName, print_model_epoch = True)\n",
    "    \n",
    "    results[it, 0:2] = scores[0]\n",
    "    results[it, 2:4] = scores[1]\n",
    "    results[it, 4:6] = scores[2]\n",
    "    results[it, 6:8] = scores[3]\n",
    "    results[it, 8] = acc \n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print('iteration {} elapsed time: {}, accuracy : {}'.format(it+1, end-start, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a2433-e48a-459f-bc98-a1d16f330f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_table(scores):\n",
    "    df = np.reshape(scores, [2,4], order ='F')\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    # df.style.set_table_attributes(\"style='display:inline'\").set_caption(mode)\n",
    "    \n",
    "    df.columns = ['precision', 'recall', 'f1', 'support']\n",
    "    # df.index = ['unfrozen', 'frozen']\n",
    "    # df.index = ['Visible ice', 'No visible ice']\n",
    "    \n",
    "    display(df)\n",
    "    \n",
    "def display_results(results):\n",
    "    mean = np.mean(results, axis=0)\n",
    "    std = np.std(results, axis=0)\n",
    "    \n",
    "    print(\"mean\")\n",
    "    display_table(mean[0:8])\n",
    "    \n",
    "    print(\"std\")\n",
    "    display_table(std[0:8])\n",
    "    \n",
    "    print(\"Accuracy mean: {}, std: {}\".format(mean[8], std[8]))\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c067723-4947-49a5-915e-c4ac4611cfba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
